{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is a word-level language model that has taken the world by storm. In this notebook, we will focus on what is under the hood of ChatGPT. What is the neural network under the hood that models the sequence of words?\n",
    "\n",
    "ChatGPT is an attention-based architecture -- GPT is short for Generatively Pretrained Transformer. The paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" was a landmark paper in AI that proposed the Transformer architecture. The paper reads like a random machine translation paper. The authors probably didn't fully anticipate the impact that the architecture would have on the whole field of ML. The architecture they proposed in the context of machine translation ended up taking over the rest of AI since it has been published. The architecture was copy-pasted with minor changes into a huge number of applications, including ChatGPT. This neural net architecture handles the heavy load in ChatGPT.\n",
    "\n",
    "We're gonna build *something like* ChatGPT. Of course, we're not going to be able to reproduce ChatGPT. It is a very serious production-grade system trained on a good chunk of the internet, and there is a lot of pre-training and fine-tuning to it. Instead, we're going to build and train a Transformer-based character-level language model, which will be very educational wrt. how the actual ChatGPT works. We also won't train on a good chunk of the internet -- we are going to use a smaller dataset. In particular, our choice will be Andrej's favorite toy dataset, the \"Tiny Shakespeare\" dataset.    \n",
    "\n",
    "Given some context of characters in the past (the prompt), the Transformer NN will look at the context and predict the next token (character) of the sequence. We're going to maximize the likelihood of the dataset under the model parameters. In the actual ChatGPT, the tokens do not correspond to characters, they correspond to word \"chunks\", i.e., sub-word pieces.\n",
    "\n",
    "We will follow the [nanoGPT] repository of Andrej, which is a repo for training Transformers on any given text. There are many ways to train Transformers, this is probably the most simple one. It only contains 2 files of 300 lines each. One of them defines the GPT model (the Transformer) and the other one trains it on a given text dataset. If we train the model on OpenWebText (a fairly large dataset of webpages), then we can reproduce the performance of GPT-2 124M (which is an early version of openAI's GPT from 2017), which proves that the codebase is correctly arranged. We can also load the GPT-2 124M weights that openAI released.\n",
    "\n",
    "Now we're going to basically write the repository from scratch. First, we start with the ``train.py`` file. We will define a Transformer piece by piece, and we're going to train it on the TinyShakespeare dataset. We'll see how we can then generate infinite Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karpathy_nn.makemore.data.load_data import load_shakespeare\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "\n",
    "from typing import Optional\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "text = load_shakespeare()\n",
    "print(f\"Length of dataset in characters: {len(text):,}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 1000 characters of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the unique characters that occur in the entire text. These are all possible characters that the model can see/emit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "num_tokens = len(characters)\n",
    "print(\"\".join(characters))\n",
    "print(num_tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tokenize the input text. We will convert the raw text (a single string) into a sequence of integers according to some vocabulary of possible elements. As we're building a character-level language model, we will be converting the individual characters into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "string_to_integer = {character: integer for integer, character in enumerate(characters)}\n",
    "integer_to_string = {integer: character for integer, character in enumerate(characters)}\n",
    "\n",
    "\n",
    "def encode(string: str) -> list[int]:\n",
    "    return [string_to_integer[character] for character in string]\n",
    "\n",
    "\n",
    "def decode(int_list: list[int]) -> str:\n",
    "    return \"\".join([integer_to_string[integer] for integer in int_list])\n",
    "\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple text encoding/tokenizer. People have come up with many tokenizer methods. E.g., Google uses [SentencePiece](https://github.com/google/sentencepiece), which also encodes text into integers, but in a different schema and using a different vocabulary. It is a **sub-word tokenizer**, i.e., we're not encoding entire words, but we're also not encoding individual characters. This is usually what's used in practice. OpenAI uses [tiktoken](https://github.com/openai/tiktoken), which is a **byte pair encoding tokenizer**. This is what GPT uses. Instead of having just 65 tokens, they have 50257 tokens. When we encode the string \"hii there\", we only get a list of three integers, that are between 0 and 50256. There is a trade-off between the code book size and the tokenized sequence lengths. In summary, people in practice use sub-word tokenizers, but we will keep our tokenizer very simple and we will only build a character-level language model. (Small code book, simple encode and decode, long encoded sequences.)\n",
    "\n",
    "Now we're ready to tokenize the entire TinyShakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "The 1000 characters we looked at earlier will look like this to GPT-2 tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Let's encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(\n",
    "    \"The 1000 characters we looked at earlier will look like this to GPT-2\", data[:1000]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split our dataset into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # First 90% will be trainined data, rest will be val\n",
    "training_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the validation performance, we can get a good idea about overfitting. In particular, we don't want the network to memorize the training data -- we want it to produce training-data-like text, and we want the validation text to have a high probability under our model. Of course, we can also overfit to the validation set itself.\n",
    "\n",
    "When we train the Transformer, we sample little chunks from the training set and train the network to predict the GT next token given the context from the chunks. The maximum length of the chunks is typically called the block size or the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "training_data[: block_size + 1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk actually has multiple examples packed into it, as all characters follow each other. We are going to simultaneously train the Transformer to make predictions at every one of these positions. In a chunk of 9 characters, we have 8 individual examples:\n",
    "- Given {18}, 47 likely comes next.\n",
    "- Given {18, 47}, 56 likely comes next.\n",
    "- Given {18, 47, 56}, 57 likely comes next.\n",
    "- ...\n",
    "- Given {18, 47, 56, 57, 58, 1, 15, 47}, 58 likely comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [18], the target is 47.\n",
      "When input is [18, 47], the target is 56.\n",
      "When input is [18, 47, 56], the target is 57.\n",
      "When input is [18, 47, 56, 57], the target is 58.\n",
      "When input is [18, 47, 56, 57, 58], the target is 1.\n",
      "When input is [18, 47, 56, 57, 58, 1], the target is 15.\n",
      "When input is [18, 47, 56, 57, 58, 1, 15], the target is 47.\n",
      "When input is [18, 47, 56, 57, 58, 1, 15, 47], the target is 58.\n"
     ]
    }
   ],
   "source": [
    "x = training_data[:block_size]\n",
    "y = training_data[1 : block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context.tolist()}, the target is {target}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 8 examples hidden in the chunk of 9 characters that were sampled from the training set. We train on all of these examples, with context length between 1 and 8. We don't only train on all of them for computational reasons (because we have the sequence already), it's also done to make the transformer network be used to seeing contexts from size one to size ``block_size``. That's going to be very useful during inference, because while we're sampling, we can start the sampling generation with as little as one character of context. Then the Transformer will know how to predict the next character even given such a small context. After ``block_size``, we'll have to start truncating, because the Transformer will never receive more than ``block_size`` inputs at once.\n",
    "\n",
    "So far we've only looked at the time dimension. We also have to consider the batch dimension. When we feed the examples to the Transformer, we are actually going to feed *mini-batches* of multiple chunks of text that are all stacked up into a single tensor. This is all just done for efficiency, because GPUs are very good at parallel processing of data. The chunks are processed completely independently -- they don't influence each other, unlike in BatchNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "\t (4, 8)\n",
      "\t tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "\t (4, 8)\n",
      "\t tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "--------\n",
      "When input is [24], the target is 43.\n",
      "When input is [24, 43], the target is 58.\n",
      "When input is [24, 43, 58], the target is 5.\n",
      "When input is [24, 43, 58, 5], the target is 57.\n",
      "When input is [24, 43, 58, 5, 57], the target is 1.\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39.\n",
      "When input is [44], the target is 53.\n",
      "When input is [44, 53], the target is 56.\n",
      "When input is [44, 53, 56], the target is 1.\n",
      "When input is [44, 53, 56, 1], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58], the target is 46.\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52], the target is 58.\n",
      "When input is [52, 58], the target is 1.\n",
      "When input is [52, 58, 1], the target is 58.\n",
      "When input is [52, 58, 1, 58], the target is 46.\n",
      "When input is [52, 58, 1, 58, 46], the target is 39.\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46.\n",
      "When input is [25], the target is 17.\n",
      "When input is [25, 17], the target is 27.\n",
      "When input is [25, 17, 27], the target is 10.\n",
      "When input is [25, 17, 27, 10], the target is 0.\n",
      "When input is [25, 17, 27, 10, 0], the target is 21.\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "# How many independent sequences will we process in parallel?\n",
    "batch_size = 4\n",
    "\n",
    "# What is the maximum context length for predictions?\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> tuple[Tensor, Tensor]:\n",
    "    # Generate a small batch batch of data of inputs x and targets y\n",
    "    data = training_data if split == \"train\" else val_data\n",
    "\n",
    "    # Start index of chunks\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i : i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"Inputs:\")\n",
    "print(\"\\t\", tuple(xb.shape))\n",
    "print(\"\\t\", xb)\n",
    "print(\"Targets:\")\n",
    "print(\"\\t\", tuple(yb.shape))\n",
    "print(\"\\t\", yb)\n",
    "\n",
    "print(\"-\" * 8)\n",
    "\n",
    "for batch in range(batch_size):  # Batch dimension\n",
    "    for t in range(block_size):  # Time dimension\n",
    "        context = xb[batch, : t + 1]\n",
    "        target = yb[batch, t]\n",
    "        print(f\"When input is {context.tolist()}, the target is {target}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is going to simultaneously process all of these examples, and then it will predict (hopefully the correct) characters for *every* position in a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)  # Our input to the transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, num_tokens: int) -> None:\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(num_tokens, num_tokens)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: Optional[Tensor] = None) -> Tensor:\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        # e.g., (4, 8, 65)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # PyTorch wants a (B, C) or a (B, C, T) input!\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.reshape(B * T, C)\n",
    "            targets = targets.reshape(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx)  # (B, T, C), ()\n",
    "            # Focus only on the last time step\n",
    "            # The history is not used at all, only T - 1 -> T\n",
    "            # so this looks silly, but we want to keep the\n",
    "            # structure of the dataset, as the history will\n",
    "            # be used later on.\n",
    "            logits = logits[:, -1, :]  # Becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled inde to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1)  # (B, T + k)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(num_tokens=num_tokens)\n",
    "logits, loss = model(xb, yb)\n",
    "print(yb.shape, logits.shape, loss)\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)  # Batch 1, time 1, zero is a newline\n",
    "idx = model.generate(idx, max_new_tokens=100)[0].tolist()  # (101,)\n",
    "print(decode(idx))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are expecting the loss to be $-\\log(1/65) \\approx 4.174$. As we have a notable discrepancy, this shows that we are guessing wrong (we have a little bit of entropy) initially. Our initialization is not perfect, but it might be just noise coming from the small number of samples. The generated string is garbage, but that's what we expect from an untrained model. Let's train the model.\n",
    "\n",
    "Note: In makemore we only used SGD. AdamW is a much more advanced and popular optimizer.\n",
    "A typical good learning rate for AdamW is 1e-3, but for very small nets (like here),\n",
    "we can get away with higher learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=False)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = model.generate(idx, max_new_tokens=500)[0].tolist()\n",
    "print(decode(idx))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is not Shakespeare, but also not complete gibberish. The model is making some progress.\n",
    "\n",
    "Note that the tokens are not \"talking\" to each other: we're only making our predictions only based on the very last token. Now we want to make them \"talk\" to each other. This is how we arrive at Transformers.\n",
    "\n",
    "We're almost ready to start writing our very first self-attention block for processing the tokens. Before that, we will look at a **mathematical trick used in self-attention**. This is at the heart of an efficient implementation of self-attention. Let's see a toy example first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # Batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like the 8 tokens in a single batch to \"talk to each other\". We would like to couple them. In particular, we want to couple them in a very specific way. The token at the 5th location should not communicate with subsequent tokens, because those are future tokens in the sequence. The token at the 5th location should only communicate with previous tokens. **Information should only flow from previous context to the current time step.** We can't get any information from the future, because we want to *predict* the future.\n",
    "\n",
    "What is the easiest way for tokens to communicate? If I am the 5th token and would like to communicate with my past, the simplest way to do that is to just do an average of all preceding elements. In particular, we want to average the channels for the 1st, 2nd, 3rd, 4th, and 5th time steps. That becomes a feature vector that summarizes me and my history.\n",
    "\n",
    "Of course, doing an average is an extremely weak form of interaction -- this communication is very lossy. We lose all information about the spatial arrangement of all tokens. But that's OK for now. We'll see how we can bring that information back later.\n",
    "\n",
    "What we want to do is as follows.\n",
    "- For every single batch element independently, ...\n",
    "- ..., for every t-th token in the sequence ...,\n",
    "- ..., we want to calculate the average of all the vectors at the current or previous locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x_bow[batch, t] = mean_{i <= t} x[batch, i]\n",
    "# BoW = Bag of Words -- term that people use when you just average up things\n",
    "x_bow = torch.zeros((B, T, C))\n",
    "for batch in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[batch, : t + 1]  # (t, C)\n",
    "        x_bow[batch, t] = x_prev.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bow[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we're basically performing a ``.cumsum``, but with averaging instead of summing. The trick is that we can be very efficient in computing this using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--------\n",
      "b =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--------\n",
      "c =\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a =\")\n",
    "print(a)\n",
    "print(\"-\" * 8)\n",
    "print(\"b =\")\n",
    "print(b)\n",
    "print(\"-\" * 8)\n",
    "print(\"c =\")\n",
    "print(c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a column-wise sum of ``b``. ``torch.tril`` returns the lower-triangular portion of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--------\n",
      "b =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--------\n",
      "c =\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a =\")\n",
    "print(a)\n",
    "print(\"-\" * 8)\n",
    "print(\"b =\")\n",
    "print(b)\n",
    "print(\"-\" * 8)\n",
    "print(\"c =\")\n",
    "print(c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a column-wise ``.cumsum``! If we normalize the rows of ``a``, we get a ``.cumavg``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--------\n",
      "b =\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--------\n",
      "c =\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a =\")\n",
    "print(a)\n",
    "print(\"-\" * 8)\n",
    "print(\"b =\")\n",
    "print(b)\n",
    "print(\"-\" * 8)\n",
    "print(\"c =\")\n",
    "print(c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the column-wise ``.cummean`` of ``a``. Let's vectorize the calculation of ``x_bow``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(dim=1, keepdim=True)\n",
    "x_bow2 = weights @ x  # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "# This is a batched matmul, which implements a weighted cumsum!!!\n",
    "torch.allclose(x_bow, x_bow2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see one more way to calculate ``x_bow``. This is a bit more interesting than the previous approaches. The reason why we'll end up using it in self-attention: the weights begin with 0. We can think of this as an interaction strength or an affinity. It's telling us how much of each token from the past do we want to aggregate and average up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))  # The affinities are just set by us to be zeros.\n",
    "# Quick preview: these affinities between the tokens are not going to be just constant\n",
    "# zeros, they are going to be data-dependent. The tokens will start looking at each\n",
    "# other, and some tokens will find other tokens more or less interesting. Depending on\n",
    "# what their values are, they are going to find each other interesting to different\n",
    "# extents.\n",
    "\n",
    "# Row i of weights is telling us how much each other token (columns) count in the\n",
    "# aggregation of embedding values.\n",
    "# The subsequent lines will stay the same in self-attention.\n",
    "\n",
    "# This line is saying: tokens from the future cannot communicate.\n",
    "# By setting them to -inf we're saying that we won't aggregate anything from\n",
    "# those tokens.\n",
    "weights = weights.masked_fill(tril == 0, -float(\"inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "# Weighted (now uniform) agrgregation of past elements\n",
    "# through (batched) matrix multiplication\n",
    "x_bow3 = weights @ x\n",
    "# We aggregate the token embeddings depending on how interesting they find each other.\n",
    "torch.allclose(x_bow, x_bow3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use this trick to implement the self-attention block. We start with the crux of self-attention. This is probably the most important part of this notebook to understand. We'll implement a small self-attention for a single individual \"head\", as they're called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: self-attention\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 4 x 8 arrangement of tokens\n",
    "# Embedding of each token is 32D\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Previously, we initialized the affinities between all tokens/nodes to be zero\n",
    "# (uniform).\n",
    "# This corresponds to a simple masked average.\n",
    "# Now, we don't want this to be uniform. Different tokens will find different other\n",
    "# tokens more or less interesting, and we want that to be data_dependent.\n",
    "# E.g., if I'm a vowel, then maybe I'm looking for consonants in my past. Maybe I want to\n",
    "# know what those consonants are and I want that information to flow to me. Thus, I want\n",
    "# to gather information from the past, but I want to do it in a data dependent way.\n",
    "# This is the problem that self-attention solves.\n",
    "\n",
    "# Every single token at each position will emit two vectors. It will emit a query, and a\n",
    "# key.\n",
    "# The query vector encodes \"What am I looking for?\".\n",
    "# The key vector encodes \"What do I contain?\".\n",
    "# We get affinities between the tokens in the sequence by doing a dot product between\n",
    "# the keys and the queries. My query . all the other keys = weights. If the key and the\n",
    "# query are aligned, they will interact by a large amount. Then I will get to learn more\n",
    "# about that specific token compared to the other tokens in the sequence. Let's implement\n",
    "# this. Actually, we will implement a single head of self-attention.\n",
    "\n",
    "# Let's see a single head perform self-attention\n",
    "\n",
    "# Hyperparameter of the head -- influences the dimensionality of the vectors\n",
    "# we are going to take the dot products of.\n",
    "head_size = 16\n",
    "\n",
    "# Just matmul with some weights\n",
    "key = nn.Linear(in_features=C, out_features=head_size, bias=False)  # (C, head_size)\n",
    "query = nn.Linear(in_features=C, out_features=head_size, bias=False)  # (C, head_size)\n",
    "\n",
    "# When we push through our mini-batch through key and query, we're processing all tokens\n",
    "# in all positions of the (B, T) arrangement *independently* to produce a key and a\n",
    "# query. No communication is happening. This is embarrassingly parallelizable.\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "\n",
    "# The communication comes now. All the queries will dot product with all the keys.\n",
    "# weights gives us the (T, T) non-symmetric affinities for each batch element.\n",
    "weights = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size @ T) = (B, T, T)\n",
    "print(torch.allclose(weights, weights.transpose(-2, -1)))  # Not symmetric!\n",
    "# Take a fixed batch b, t' from q, take t'' from k, measure their dot product.\n",
    "# Place it at weights[b, t', t''].\n",
    "# Rows of weights correspond to the queries, columns correspond to the keys.\n",
    "# We keep the batch dimension untouched -- the tokens across samples can't communicate.\n",
    "\n",
    "# Up until now we have the raw affinities between each node.\n",
    "# Now we will make sure that we don't look into the future and aggregate information\n",
    "# from subsequent nodes. We're not allowed to communicate with the future.\n",
    "\n",
    "weights = weights.masked_fill(tril == 0, -float(\"inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "# There is one more part to a single self-attention head.\n",
    "# When we do the aggregation, we don't actually aggregate the tokens exactly. We\n",
    "# produce one more value, called the \"value\". :)\n",
    "\n",
    "# Can be of different dimensionality than queries and keys\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "v = value(x)  # The elements that we aggregate instead of the raw x values.\n",
    "# We can think of x as private information to each token. Each token's information is\n",
    "# kept in vector x. \"**For the purposes of this single head**, here's what I'm interested\n",
    "# in, here's what is important about me for this head, and if you find me interesting,\n",
    "# this is what I'll communicate to you.\" This is stored in v. It's just the thing that\n",
    "# gets aggregated for the purposes of this single head between the different nodes.\n",
    "\n",
    "# out = weights @ x\n",
    "out = weights @ v  # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "# The weighted aggregation now is a function in a data-dependent manner of the keys and\n",
    "# queries of these nodes. It answers how much informations a query node should aggregate\n",
    "# from any of the tokens in the past.\n",
    "\n",
    "# This is basically the self-attention mechanism.\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, weights was just a constant -- we applied a uniform aggregation to all tokens\n",
    "in the same batch element. Now, every single node in a single batch element has a\n",
    "different weight wrt. a fixed query node, because everything is data dependent.\n",
    "\n",
    "Example: take the 8th token. It knows what content it has and what position it's in.\n",
    "Based on that, the token creates a query: \"Hey, I'm looking for this kind of stuff! I'm\n",
    "a vowel, I'm in the 8th position, I'm looking for any consonants at position up to four.\n",
    "\". (Just an example.) All nodes emit keys. Maybe a node's key will say \"I'm a consonant!\n",
    "I'm in a position up to four!\". That key, query dot product will have a high value, a\n",
    "high *affinity*. When two nodes have a high affinity, then through the softmax the query\n",
    "node will aggregate a lot of the corresponding key node's information. This is the case\n",
    "e.g. for the query of the 8th node and the key of the 4th node with 22.97% matching\n",
    "probability.\n",
    "\n",
    "A few notes about attention:\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. The nodes of the graph are laid out linearly, with the i-th node having arrows pointing into it from all previous nodes and itself. That's the structure that our graph has in autoregressive scenarios such as language modeling. But in principle, attention can be applied to any arbitrary directed graph and is just a communication mechanism between the nodes.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. The nodes have no clue where they are positioned in the space. This is why we need to positionally encode tokens and give them some information that is anchored to a specific position such that they know where they are. This is different from convolution, because if we run the convolution operation over some input, there is a very specific layout of the information in space, and the convolutional filters are acting in the space. Attention just makes a set of vectors communicate, and if we want them to have a notion of space, we need to add it explicitly. This is what we've done when we calculated the positional encoding and added that information to the vectors.\n",
    "- Each example across the batch dimension is, of course, processed completely independently and never \"talk\" to each other. We have ``batch_size`` separate pools of ``T`` nodes in our graph.\n",
    "- In the case of language modeling, we have a specific structure of a directed graph where the future tokens' informations are not aggregated into the current token's information pool. This is not a general constraint. In many cases we want to have all of the nodes talk to each other fully. E.g., if we're doing sentiment analysis with a Transformer, we might have a number of tokens and we might want all of them to talk to each other fully, because later we're predicting the sentiment of the sentence, having seen the entire sentence. In an \"encoder\" (self-)attention block, we just have to delete the single line that does masking with `tril`, allowing all tokens to communicate with each other. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling (where it's decoding language -- nodes in the future never talk to the current node, as they don't exist yet). Here we have a triangular structure. But both are allowed, and attention doesn't care. Attention supports arbitrary connectivity between nodes.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source x as queries. In principle, attention is much more general than that. In \"cross-attention\" (e.g., in encoder-decoder transformers), the queries still get produced from x, but the keys and values come from some other, external source (e.g., an encoder module/block that encodes some context we'd like to condition on). So the keys and values will come from a whole separate source (nodes on the side), and here we're just producing queries and we're reading off information from the side. Cros-attention is thus used when there is a separate source of nodes we'd like to pull information from into our nodes. The attention above is thus self-attention.\n",
    "- \"Scaled\" attention additionally divides ``weight`` by $1 / \\sqrt{\\text{head\\_size}}$. This makes it so when input Q, K are unit variance, ``weight`` will be unit variance too and Softmax will stay diffuse and not saturate too much. If we don't do it, then we will be aggregating information from 1-2 other nodes. This is not we want, especially at initialization. The scaling is done to control the variance at initialization. It is applied even during training to smoothen the softmax predictions. Illustration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "\n",
    "# Multiplying by this is harmful: * 1 / T**0.5\n",
    "v = torch.randn(B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product of individual elements in the matmul $X \\sim \\mathcal{N}(0, 1)$ and $Y \\sim \\mathcal{N}(0, 1)$ will not be Gaussian distributed, but whatever their distribution is, their variance is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(XY) &= \\mathbb{E}[X^2Y^2] - \\mathbb{E}[XY]^2\\\\\n",
    "&= \\int_{\\mathcal{X} \\times \\mathcal{Y}} p(x, y) x^2y^2 d(x, y) - \\left(\\int_{\\mathcal{X} \\times \\mathcal{Y}} p(x, y) xy d(x, y)\\right)^2\\\\\n",
    "&= \\int_{\\mathcal{X}} p(x) x^2 dx \\int_{\\mathcal{Y}} p(y) y^2 dy - \\left(\\int_{\\mathcal{X}} p(x)xdx \\int_{\\mathcal{Y}} p(y)ydy\\right)^2\\\\\n",
    "&= \\mathbb{E}[X^2]\\mathbb{E}[Y^2] - \\mathbb{E}[X]^2\\mathbb{E}[Y]^2\\\\\n",
    "&= \\left(\\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\right)\\left(\\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2\\right) + \\mathbb{E}[X^2]\\mathbb{E}[Y]^2 + \\mathbb{E}[Y^2]\\mathbb{E}[X]^2 - 2\\mathbb{E}[X]^2\\mathbb{E}[Y]^2\\\\\n",
    "&= \\text{Var}(X)\\text{Var}(Y)\\\\\n",
    "&= 1,\n",
    "\\end{align*}\n",
    "$$\n",
    "as the variables are *independent*. For the same reason, their expectation is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[XY] &= \\int_{\\mathcal{X} \\times \\mathcal{Y}} p(x, y) xy d(x, y)\\\\\n",
    "&= \\int_{\\mathcal{X}}p(x)xdx \\int_{\\mathcal{Y}}p(y)ydy\\\\\n",
    "&= \\mathbb{E}[X]\\mathbb{E}[Y]\\\\\n",
    "&= 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " We also know that the sum of any two independent random variables $Z$ and $W$ has variance\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(Z + W) &= \\int_{\\mathcal{Z} \\times \\mathcal{W}} p(z, w) \\left(z + w - \\mathbb{E}[Z + W]^2\\right)^2dzdw\\\\\n",
    "&= \\int_{\\mathcal{Z}}\\int_{\\mathcal{W}}p(z)p(w) \\left((z - \\mathbb{E}[Z]) + (w - \\mathbb{E}[W])\\right)^2dzdw\\\\\n",
    "&= \\int_{\\mathcal{Z}}\\int_{\\mathcal{W}}p(z)p(w)\\left((z - \\mathbb{E}[Z])^2 + 2(z - \\mathbb{E}[Z])(w - \\mathbb{E}[W]) + (w - \\mathbb{E}[W])^2\\right)dzdw\\\\\n",
    "&= \\int_{\\mathcal{Z}} p(z)(z - \\mathbb{E}[Z])^2dz + \\int_{\\mathcal{W}}p(w)(w - \\mathbb{E}[W])^2 dw + 2\\int_{\\mathcal{Z}}p(z)(z - \\mathbb{E}[Z])dz \\int_{\\mathcal{W}}p(w)(w - \\mathbb{E}[W])dw\\\\\n",
    "&= \\text{Var}(X) + \\text{Var}(Y).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we now consider the dot product between two independent random vectors $X \\sim \\mathcal{N}(0, I_n), Y \\sim \\mathcal{N}(0, I_n)$, we get\n",
    "$$\\text{Var}(X^\\top Y) = \\text{Var}\\left(\\sum_{i = 1}^n X_iY_i\\right) = \\sum_{i = 1}^n \\text{Var}\\left(X_iY_i\\right) = n$$\n",
    "and\n",
    "$$\\mathbb{E}[X^\\top Y] = 0.$$\n",
    "If we consider a matrix multiplication, this dot product will give a single element of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.4690)"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The population variance is exactly 16.\n",
    "weights.var()  # On the order of head_size = 16 (= n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0237e-11) tensor(0.9997) tensor(0.1250) tensor(0.7422)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.0374)"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(weights, dim=1)\n",
    "print(\n",
    "    weights.min(),\n",
    "    weights.max(),\n",
    "    weights.mean(),\n",
    "    torch.logical_or(weights > 0.95, weights < 0.05).float().mean(),\n",
    ")\n",
    "\n",
    "out = weights @ v\n",
    "out.var()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not make the elements of the weights tensor explode and have confidently wrong (we still have randomness) predictions in the softmax, we have to devide the elements of weights by $\\sqrt{\\text{head\\_size}}$. By doing so, the weights tensor will have unit variance again, making it well-behaved under the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0918)\n",
      "tensor(0.0014) tensor(0.8043) tensor(0.1250) tensor(0.2812)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1150)"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "print(weights.var())  # Unit variance is preserved\n",
    "weights = F.softmax(weights, dim=1)\n",
    "print(\n",
    "    weights.min(),\n",
    "    weights.max(),\n",
    "    weights.mean(),\n",
    "    torch.logical_or(weights > 0.95, weights < 0.05).float().mean(),\n",
    ")\n",
    "\n",
    "out = weights @ v\n",
    "out.var()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire network is now implemented in ``gpt.py``.\n",
    "\n",
    "We did a pretty good job of implementing the Transformer architecture, but the Figure in the paper doesn't exactly match up to what we've been doing. What's going on with all the additional parts?\n",
    "\n",
    "What we implemented here is a decoder only Transformer. There's no encoder component, and there is also no cross-attention block. Our block only has a self-attention and a feed-forward. It is missing the third in-between piece. The reason why we only have a decoder only is that we are just generating text and it's unconditioned on anything. We're just blabbering on, according to a given dataset. What makes our network a decoder is that we're using the triangular mask, so it has this autoregressive property where we can just go and sample from it. It can be used for language modeling. We just have a dataset and we want to imitate it, which is why we're using a decoder only transformer. This is exactly as done in GPT.\n",
    "\n",
    "In machine learning, autoregressive refers to a class of models that make predictions based on previous values of the same variable. Autoregressive models are commonly used in time series analysis, where the goal is to make predictions based on patterns observed in past data. For example, a time series model that predicts stock prices might use an autoregressive approach to capture trends and seasonality in the data.\n",
    "\n",
    "The reason that the original paper had an encoder-decoder architecture is because it is a machine translation paper. It is concerned with a different setting. In particular, it expects some tokens that encode, e.g., a French sentence, and then it is expected to decode the translation in English. Example:\n",
    "\n",
    "<------------- ENCODE -------------> | <------------------ DECODE ----------------->\n",
    "\n",
    "Les rseaux de neurones sont gniaux! | \\<START\\>Neural networks are awesome!\\<END\\>\n",
    "\n",
    "\\<START\\> and \\<END\\> are special tokens. The network is expected to read in the French sentence and condition on it, and then we start off the generation with the special \\<START\\> token which we always place in the beginning. Then the network is expected to output the English translation of the French sentence and the special \\<END\\> token to finish the generation. Here, the decoder will work exactly as we implemented it. It is first conditioned on the \\<START\\> token, then the \\<START\\> and the newly generated token (not necessarily \"Neural\"), and so on. But unlike how we implemented transformers, they condition the generation of tokens on some additional information. In their case, this additional information is the French sentence that they should be translating. The encoder reads the French sentence (after tokenizing it) and puts a Transformer on top, but *without* the triangular mask in the self-attention layers. All tokens are allowed to talk to each other as much as they want. They are just encoding the content of the French sentence. Once they've encoded it, they come out on the top in shape (B, T, C). In our decode (which does the language modeling), there is an additional connection to the outputs of the encoder. This connection is the cross-attention between the encoder output representation and the decoder representation. The queries are still generated from x, but the keys and values are coming from the \"side\", i.e., they are generated from the output nodes of the encoder. These keys and values feed into every single block of the decoder. Thus, the decoder is conditioned not just on the past of the current sentence, but also on the fully encoded French prompt.\n",
    "\n",
    "In summary, the original architecture was used for machine translation (encoder-decoder model), and that's why we have those two Transformers and an additional block in the decoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A walkthrough of nanoGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nanoGPT is basically two files of interest, ``train.py`` and ``model.py``. ``train.py`` is the boilerplate code for training the network. It's very similar to our training loop, but it's a lot more complicated:\n",
    "- We are saving and loading checkpoints and pretrained weights\n",
    "- We are decaying the learning rate\n",
    "- We are compiling the model\n",
    "- We are using distributed training across multiple nodes of GPUs\n",
    "- etc.\n",
    "\n",
    "However, the ``model.py`` should look very-very similar to what we've done here. In fact,\n",
    "the models are almost identical. The ``CausalSelfAttention`` block implements masked self-attention.\n",
    "\n",
    "What's different is that in our code, we've separated the multi-head attention into just a single individual head, and we explicitly concatenate these in the multi-head attention module. In nanoGPT, all of that is implemented in a batched manner inside a single causal self-attention block. Thus, we don't just have a B, T, and C dimension, we end up with a fourth dimension that is the heads. The two approaches are mathematically equivalent, nanoGPT is just much more efficient because all the heads are now treated as a batch dimension as well. The MLP uses the gelu non-linearity instead of ReLU. This is because openAI also used it and Andrej wanted to be able to load their checkpoints. The blocks of the Transformer are identical (communicate and compute phase), and GPT is also identical. We have the positional encodings, token encodings, the Blocks, the LayerNorms, etc. There are some more things, e.g., Andrej separates out the weights that should be weight decayed and ones that shouldn't. So a few details are different, but we should understand most things."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would it look like if we wanted to train ChatGPT ourselves? How does it relate to what we've learned today? To train ChatGPT, there are roughly two stages, pre-training and fine-tuning. In the pre-training stage, we are training on a large chunk of the internet and just trying to get a great decoder-only Transformer to babble text. This part is very-very similar to what we've done ourselves, except we've done a baby pre-training step. Our upscaled model has about 10M parameters, so this Transformer is quite tiny compared to today's standards. Our dataset was roughly 1M characters/tokens. OpenAI uses a different vocabulary -- they're not on the character level, they use sub-word chunks and have a vocabulary of 50K elements. Their sequences therefore are a bit more condensed. The Shakespeare dataset would be around 2-300K tokens in the OpenAI vocabulary, so we trained a 1M parameter model on roughly 300K tokens. If we go to the GPT-3 paper ([Language Models are Few-Shot Leaners](https://arxiv.org/abs/2005.14165)) and look at Table 2.1, we see that the biggest Transformer had 175B parameters, they used 96 layers, ``dim_embedding = 12288``, ``num_heads = 96``, (so ``head_size = dim_embedding / num_heads = 128``). Their batch size was 3.2M (!!!), compared to our 64. Their learning rate was 0.6e-4. All models were trained for a total of 300B tokens (!!!). This number is not even large for today's standards, it would be 1 trillion and above.\n",
    "\n",
    "So they're training a significantly larger model on a good chunk of the internet. That's the pretraining stage. But the architecture is nearly identical to what we've implemented, the difference is that they are using much larger hyperparameters. Obviously, it's a massive infrastructure challenge to train this. We're talking 1000s of GPUs having to talk to each other to train models of this size.\n",
    "\n",
    "After we complete the pretraining stage, we don't get something that responds to our questions with answers. We only get a *document completer*. It babbles, but it doesn't babble Shakespeare, it babbles the internet. It will create arbitrary news articles, it'll try to complete documents, etc. This is what it was trained to do: to complete the sequence. If we gave it a question, it would potentially just follow up with more questions. It would do whatever some closed document would do in the training data. We're getting undefined behavior: it could actually also answer our question if it saw that kind of data. It can also ignore our question, it can start a totally different news article, etc. It's totally *underlined* as we say.\n",
    "\n",
    "The second fine-tuning stage is to actually align ChatGPT to be an assistant. The [ChatGPT blog post](https://openai.com/blog/chatgpt/) from OpenAI talks a little bit about how this stage is achieved. There are roughly three steps to this stage.\n",
    "\n",
    "(1) They start to collect training data that looks specifically like what an assistant would do. They are documents that have the format where the question is on top and an answer is below. They have a large number of these, but of course, not on the order of the size of the internet. This is on the order of maybe thousands of examples. They then fine-tune the model to only focus on documents that look like that. So we're starting to slowly align it. It's going to expect a question at the top and it's going to complete the prompt with the answer. These very-very large models are very sample efficient during their fine-tuning, so this actually works.\n",
    "\n",
    "(2) They let the model respond to queries, and then different raters look at the different responses and rank them based on their preference as to which one is better than the other. They use that to train a reward model so they can predict (basically using a different network) how much desirable would each response be.\n",
    "\n",
    "(3) Once they have a reward model, they run PPO which is a form of policy gradient reinforcement learning optimizer to fine-tune this sampling policy such that the answers that ChatGPT generates are expected to score a high reward according to the reward model.\n",
    "\n",
    "They have a whole alignment/fine-tuning stage with multiple steps, which takes the model from being a document completer to a question answerer. This is a whole separate stage, and a lot of this data is not available publically, as it's internal to OpenAI. It's much harder to replicate this stage than the pre-training stage. Anytime we don't just want a document completer (we want the model to perform tasks, we want it to be aligned in a specific way, we want it to predict sentiment, etc.), we have to complete further stages of fine-tuning which we didn't cover. That could be simple supervised fine-tuning or something more fancy like we've seen in ChatGPT (train a reward model, do rounds of PPO to align the model wrt. the reward model).\n",
    "\n",
    "This is roughly what would give us a ChatGPT. nanoGPT just focuses on the pretraining stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-nn-L00z48Da-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1777287656d261b264802db79277cf28c62daaee414facac31cc4d9617e447f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
