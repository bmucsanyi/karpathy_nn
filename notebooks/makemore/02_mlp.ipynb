{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to implement an MLP to predict the next character in the sequence. This modelling approach follows the paper [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). This isn't the first work that proposed the use of neural networks for word (!) level language modelling, but it was nevertheless a very influential paper. It's often cited to refer to this idea.\n",
    "- Each of the 17000 words is embedded into an, e.g., 30-dimensional feature space. So we have 17000 points in a 30-dimensional space.\n",
    "- In the beginning, the embeddings are initialized randomly. Then we tune the embeddings using backpropagation. Words that have a very similar meaning (or synonyms) will usually end up being very close in this space. Very different things will usually be far away in the embedding space.\n",
    "- To train the neural network, we can maximize the log-likelihood of the training data under our model. This is exactly what we did previously.\n",
    "\n",
    "Example:\n",
    "- Consider the sentence fragment \"A dog was running in a\". Suppose that it wasn't a part of the training data, and we only encounter it in deployment. However, the dataset might contain very similar phrases, like \"The dog was running in a\". If the network learned that \"a\" and \"the\" are very similar, it put them very close in the embedding space. We can exploit this to be able to correctly predict the next word in the deployment example, even though we have never seen it before. This sample is therefore not actually OOD, as we have seen very similar examples. We *transfer knowledge through the embedding*, and so generalize better.\n",
    "- The look-up table $C$ in the paper is a $17000 \\times 30$ matrix. This is shared over words, and we optimize it during training. The second layer of the network contains $3 \\times 30 = 90$ neurons. In the hidden layer, all neurons are fully connected to the previous 90 neurons. The output layer has 17000 neurons, all of which are fully connected to the hidden neurons. The outputs are 17000 logits. We apply the softmax function to get a valid probability distribution $p(\\text{next word} \\mid \\text{previous words})$. We want to maximize the likelihood of the true next work in the dataset, given the previous words in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from karpathy_nn.makemore.data.load_data import load_names\n",
    "from karpathy_nn.micrograd.utils import apply_random_seed\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = load_names()\n",
    "words[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabuilary of tokens and mappings to/from integers\n",
    "characters = sorted(list(set(\"\".join(words))))\n",
    "string_to_integer = {string: integer + 1 for integer, string in enumerate(characters)}\n",
    "string_to_integer[\".\"] = 0\n",
    "integer_to_string = {integer: string for string, integer in string_to_integer.items()}\n",
    "print(integer_to_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "block_size = (\n",
    "    3  # Context length: how many characters do we take to predict the next one?\n",
    ")\n",
    "X, Y = [], []\n",
    "for word in words[:5]:\n",
    "    print(word)\n",
    "    context = [0] * block_size\n",
    "    for character in word + \".\":\n",
    "        idx = string_to_integer[character]\n",
    "        X.append(context)\n",
    "        Y.append(idx)\n",
    "\n",
    "        print(\n",
    "            \"\".join(integer_to_string[i] for i in context),\n",
    "            \"--->\",\n",
    "            integer_to_string[idx],\n",
    "        )\n",
    "\n",
    "        context = context[1:] + [idx]  # Crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the five words, we have created a dataset of 32 samples. Each input to the neural net is three integers, and we also have a label that is also an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3398,  0.6858],\n",
       "        [ 0.1480, -0.3134],\n",
       "        [-0.6270, -0.9299],\n",
       "        [-0.6622,  0.6997],\n",
       "        [-1.4983,  0.6663],\n",
       "        [ 0.6198, -0.7637],\n",
       "        [ 0.0102, -2.0646],\n",
       "        [-0.7267, -0.0279],\n",
       "        [ 0.5047, -1.3677],\n",
       "        [-0.8804, -0.1790],\n",
       "        [ 1.6936, -0.1093],\n",
       "        [ 0.0030,  0.1864],\n",
       "        [-1.2437,  0.6344],\n",
       "        [ 1.1227,  2.2106],\n",
       "        [-0.9022,  0.5803],\n",
       "        [ 0.9917,  0.0331],\n",
       "        [ 0.4016, -0.8094],\n",
       "        [ 0.3880, -0.4660],\n",
       "        [-0.8894,  0.2483],\n",
       "        [ 0.3130, -0.6143],\n",
       "        [-1.3125, -0.8157],\n",
       "        [-0.3948,  1.4741],\n",
       "        [ 1.8541,  1.4140],\n",
       "        [-0.7321, -1.2655],\n",
       "        [ 0.1353, -2.1952],\n",
       "        [ 0.4138,  0.5221],\n",
       "        [ 1.7441, -0.2574]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn(27, 2)\n",
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6198, -0.7637])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(\n",
    "    torch.tensor(5), num_classes=len(integer_to_string)\n",
    ").float() @ C  # First layer of our MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6198, -0.7637])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1480, -0.3134])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1480, -0.3134])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7304, -2.7130,  0.1615,  ..., -0.6084,  1.4205, -1.8027],\n",
      "        [-1.5892, -1.6765, -0.1116,  ..., -2.1646, -0.0938, -1.3554],\n",
      "        [ 0.9889, -3.2672,  1.0578,  ...,  4.3039,  0.9977, -5.0346],\n",
      "        ...,\n",
      "        [-0.0108, -0.4739, -0.3908,  ..., -1.1756, -1.0889, -2.0041],\n",
      "        [-1.1517,  0.1845, -1.0980,  ...,  0.1680,  1.0027, -0.8978],\n",
      "        [-1.3889, -0.8312,  0.5669,  ...,  0.0074, -0.1572,  0.1354]])\n",
      "tensor([[-1.7304, -2.7130,  0.1615,  ..., -0.6084,  1.4205, -1.8027],\n",
      "        [-1.5892, -1.6765, -0.1116,  ..., -2.1646, -0.0938, -1.3554],\n",
      "        [ 0.9889, -3.2672,  1.0578,  ...,  4.3039,  0.9977, -5.0346],\n",
      "        ...,\n",
      "        [-0.0108, -0.4739, -0.3908,  ..., -1.1756, -1.0889, -2.0041],\n",
      "        [-1.1517,  0.1845, -1.0980,  ...,  0.1680,  1.0027, -0.8978],\n",
      "        [-1.3889, -0.8312,  0.5669,  ...,  0.0074, -0.1572,  0.1354]])\n",
      "tensor([[-1.7304, -2.7130,  0.1615,  ..., -0.6084,  1.4205, -1.8027],\n",
      "        [-1.5892, -1.6765, -0.1116,  ..., -2.1646, -0.0938, -1.3554],\n",
      "        [ 0.9889, -3.2672,  1.0578,  ...,  4.3039,  0.9977, -5.0346],\n",
      "        ...,\n",
      "        [-0.0108, -0.4739, -0.3908,  ..., -1.1756, -1.0889, -2.0041],\n",
      "        [-1.1517,  0.1845, -1.0980,  ...,  0.1680,  1.0027, -0.8978],\n",
      "        [-1.3889, -0.8312,  0.5669,  ...,  0.0074, -0.1572,  0.1354]])\n"
     ]
    }
   ],
   "source": [
    "W1 = torch.randn(6, 100)\n",
    "b1 = torch.randn(100)\n",
    "\n",
    "print(emb.flatten(start_dim=1) @ W1 + b1)\n",
    "# Creates a new tensor, very inefficient (there is no way to concatenate\n",
    "# tensors just by manipulating the view attributes)\n",
    "print(torch.cat(torch.unbind(emb, dim=1), dim=1) @ W1 + b1)\n",
    "print(emb.view(-1, 6) @ W1 + b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()  # Always a 1D vector\n",
    "# .view() just changes how the storage is interpreted as a tensor.\n",
    "# No memory is being copied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9391, -0.9912,  0.1601,  ..., -0.5430,  0.8897, -0.9471],\n",
       "        [-0.9200, -0.9324, -0.1111,  ..., -0.9740, -0.0935, -0.8753],\n",
       "        [ 0.7569, -0.9971,  0.7848,  ...,  0.9996,  0.7606, -0.9999],\n",
       "        ...,\n",
       "        [-0.0108, -0.4414, -0.3721,  ..., -0.8261, -0.7965, -0.9643],\n",
       "        [-0.8183,  0.1824, -0.7998,  ...,  0.1664,  0.7627, -0.7152],\n",
       "        [-0.8829, -0.6811,  0.5131,  ...,  0.0074, -0.1560,  0.1346]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.flatten(start_dim=1) @ W1 + b1)\n",
    "h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final layer\n",
    "W2 = torch.randn(100, len(integer_to_string))\n",
    "b2 = torch.randn(len(integer_to_string))\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(dim=1, keepdims=True)\n",
    "prob.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.3340)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single cell that contains everything\n",
    "g = torch.Generator().manual_seed(2147483647)  # For reproducibility\n",
    "C = torch.randn(len(integer_to_string), 2, generator=g, requires_grad=True)\n",
    "W1 = torch.randn(6, 100, generator=g, requires_grad=True)\n",
    "b1 = torch.randn(100, generator=g, requires_grad=True)\n",
    "W2 = torch.randn(100, len(integer_to_string), generator=g, requires_grad=True)\n",
    "b2 = torch.randn(len(integer_to_string), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "# Number of parameters in total\n",
    "print(sum(param.nelement() for param in parameters))\n",
    "\n",
    "emb = C[X]  # (batch_size, 3, 2) -- embedded context for each sample\n",
    "h = torch.tanh(emb.flatten(start_dim=1) @ W1 + b1)  # (32, 100)\n",
    "logits = h @ W2 + b2  # (32, len(integer_to_string))\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(dim=1, keepdims=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``F.cross_entropy`` is much more efficient (no intermediate tensors) and much more numerically stable (maximum subtracted).\n",
    "- PyTorch clusters up all of these operations and has fused kernels that very efficiently evaluates the expression. (Forward prop efficient)\n",
    "- PyTorch also doesn't backpropagate through these individual steps: there is a simple form for the gradient of the entire cross-entropy function that can be used directly. (Backward prop efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 0.2561368942260742\n"
     ]
    }
   ],
   "source": [
    "def forward(X: Tensor, Y: Tensor, parameters: list[Tensor]) -> Tensor:\n",
    "    C, W1, b1, W2, b2 = parameters\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.flatten(start_dim=1) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_steps: int,\n",
    "    learning_rate: float,\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    parameters: list[Tensor],\n",
    "    verbose: bool = True,\n",
    ") -> None:\n",
    "    for step in range(num_steps):\n",
    "        loss = forward(X, Y, parameters)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Step {step}: loss = {loss.item()}\")\n",
    "\n",
    "        for param in parameters:\n",
    "            param.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        for param in parameters:\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    if not verbose:\n",
    "        print(f\"Final loss = {loss.item()}\")\n",
    "\n",
    "\n",
    "train(num_steps=1000, learning_rate=0.1, X=X, Y=Y, parameters=parameters, verbose=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only overfitting 32 samples. It is very easy to make the neural network fit to this single batch.\n",
    "\n",
    "Why can't we achieve 0 loss? Because we have mixed supervision in our dataset:\n",
    "- ... -> e\n",
    "- ... -> o\n",
    "- ... -> a\n",
    "- ... -> s\n",
    "\n",
    "For inputs without mixed supervision, we recover the exact correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    words: list[str], string_to_integer: dict[str, int], block_size: int = 3\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for character in word + \".\":\n",
    "            idx = string_to_integer[character]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def initialize_parameters(num_tokens: int) -> list[Tensor]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn(num_tokens, 2, generator=g, requires_grad=True)\n",
    "    W1 = torch.randn(6, 100, generator=g, requires_grad=True)\n",
    "    b1 = torch.randn(100, generator=g, requires_grad=True)\n",
    "    W2 = torch.randn(100, num_tokens, generator=g, requires_grad=True)\n",
    "    b2 = torch.randn(num_tokens, generator=g, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 19.505229949951172\n",
      "Step 1: loss = 17.08449363708496\n",
      "Step 2: loss = 15.776531219482422\n",
      "Step 3: loss = 14.83333683013916\n",
      "Step 4: loss = 14.002594947814941\n",
      "Step 5: loss = 13.253252029418945\n",
      "Step 6: loss = 12.579911231994629\n",
      "Step 7: loss = 11.9830961227417\n",
      "Step 8: loss = 11.470490455627441\n",
      "Step 9: loss = 11.051854133605957\n",
      "Step 10: loss = 10.709585189819336\n",
      "Step 11: loss = 10.407631874084473\n",
      "Step 12: loss = 10.127808570861816\n",
      "Step 13: loss = 9.864364624023438\n",
      "Step 14: loss = 9.61450481414795\n",
      "Step 15: loss = 9.376440048217773\n",
      "Step 16: loss = 9.148946762084961\n",
      "Step 17: loss = 8.931111335754395\n",
      "Step 18: loss = 8.722232818603516\n",
      "Step 19: loss = 8.521750450134277\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(words, string_to_integer)\n",
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "train(num_steps=20, learning_rate=0.1, X=X, Y=Y, parameters=parameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break up our dataset into mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    X: Tensor, Y: Tensor, parameters: list[Tensor], batch_size: int = 512\n",
    ") -> Tensor:\n",
    "    C, W1, b1, W2, b2 = parameters\n",
    "\n",
    "    if batch_size == len(X):\n",
    "        X_batch = X\n",
    "        Y_batch = Y\n",
    "    else:\n",
    "        indices = torch.randint(0, X.shape[0], (batch_size,))\n",
    "        X_batch = X[indices]\n",
    "        Y_batch = Y[indices]\n",
    "    emb = C[X_batch]\n",
    "    h = torch.tanh(emb.flatten(start_dim=1) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y_batch)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 2.655564785003662\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(words, string_to_integer)\n",
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "train(num_steps=1000, learning_rate=0.1, X=X, Y=Y, parameters=parameters, verbose=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are dealing with mini-batches, the quality of our gradient is lower. These are not the gradients over the entire dataset, only a small subset of it. But it is good enough to be useful, and it makes training very fast. It is much better to have an approximate gradient and make many steps than it is to evaluate the exact gradient and take fewer steps. Matter of fact, the noise stochastic gradient descent introduces can help in achieving better test accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lrs\n",
    "lrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs = torch.logspace(-3, 0, 1000)\n",
    "lrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.732702255249023\n",
      "20.39100456237793\n",
      "22.212892532348633\n",
      "18.168304443359375\n",
      "20.32712173461914\n",
      "17.5830135345459\n",
      "19.330341339111328\n",
      "19.388395309448242\n",
      "20.28860092163086\n",
      "18.201444625854492\n",
      "19.17503547668457\n",
      "20.564476013183594\n",
      "20.43487548828125\n",
      "21.17230796813965\n",
      "20.615503311157227\n",
      "16.429044723510742\n",
      "16.609027862548828\n",
      "20.133283615112305\n",
      "17.8748779296875\n",
      "19.95508575439453\n",
      "19.803279876708984\n",
      "19.71059799194336\n",
      "17.589736938476562\n",
      "19.43290901184082\n",
      "21.874675750732422\n",
      "20.45779037475586\n",
      "20.98611831665039\n",
      "19.025163650512695\n",
      "17.403331756591797\n",
      "18.158124923706055\n",
      "18.714292526245117\n",
      "16.56267547607422\n",
      "19.134443283081055\n",
      "13.251580238342285\n",
      "22.184017181396484\n",
      "15.918084144592285\n",
      "18.306089401245117\n",
      "22.428050994873047\n",
      "16.513673782348633\n",
      "21.244888305664062\n",
      "15.845444679260254\n",
      "18.71114158630371\n",
      "17.08005714416504\n",
      "16.25472640991211\n",
      "19.863779067993164\n",
      "18.442691802978516\n",
      "19.1829776763916\n",
      "18.874422073364258\n",
      "16.894176483154297\n",
      "17.677274703979492\n",
      "17.657405853271484\n",
      "18.603107452392578\n",
      "14.887605667114258\n",
      "17.483383178710938\n",
      "16.9281063079834\n",
      "18.145660400390625\n",
      "16.01116180419922\n",
      "17.75019073486328\n",
      "16.64190673828125\n",
      "16.507333755493164\n",
      "17.64509391784668\n",
      "19.081899642944336\n",
      "16.945219039916992\n",
      "23.601085662841797\n",
      "18.66862678527832\n",
      "16.714399337768555\n",
      "16.575450897216797\n",
      "17.232444763183594\n",
      "15.341827392578125\n",
      "17.26152229309082\n",
      "18.27154541015625\n",
      "15.399480819702148\n",
      "18.683120727539062\n",
      "17.06641960144043\n",
      "16.838645935058594\n",
      "17.695730209350586\n",
      "15.904590606689453\n",
      "17.129074096679688\n",
      "18.118139266967773\n",
      "17.857364654541016\n",
      "18.816537857055664\n",
      "19.567842483520508\n",
      "15.96049690246582\n",
      "14.32949447631836\n",
      "16.482561111450195\n",
      "17.735612869262695\n",
      "19.245540618896484\n",
      "14.670781135559082\n",
      "13.863068580627441\n",
      "17.715038299560547\n",
      "13.324914932250977\n",
      "17.151325225830078\n",
      "18.87198257446289\n",
      "14.509766578674316\n",
      "15.868025779724121\n",
      "20.05105209350586\n",
      "15.044612884521484\n",
      "16.492473602294922\n",
      "18.207660675048828\n",
      "16.794178009033203\n",
      "15.566004753112793\n",
      "14.238846778869629\n",
      "15.649213790893555\n",
      "17.7396183013916\n",
      "18.326522827148438\n",
      "17.69695472717285\n",
      "15.480161666870117\n",
      "16.472410202026367\n",
      "16.12417984008789\n",
      "14.25991153717041\n",
      "17.71710777282715\n",
      "16.945343017578125\n",
      "16.60796546936035\n",
      "17.675764083862305\n",
      "17.50788116455078\n",
      "16.18480110168457\n",
      "16.539226531982422\n",
      "16.6777400970459\n",
      "15.916814804077148\n",
      "17.551923751831055\n",
      "16.12630271911621\n",
      "16.66200065612793\n",
      "16.954030990600586\n",
      "16.887924194335938\n",
      "13.923715591430664\n",
      "13.77817440032959\n",
      "18.229415893554688\n",
      "19.663349151611328\n",
      "17.388748168945312\n",
      "13.517910957336426\n",
      "14.660887718200684\n",
      "15.454683303833008\n",
      "14.496332168579102\n",
      "18.715614318847656\n",
      "17.711416244506836\n",
      "13.942378044128418\n",
      "13.526516914367676\n",
      "15.158262252807617\n",
      "17.90399932861328\n",
      "20.2946720123291\n",
      "17.710308074951172\n",
      "16.708141326904297\n",
      "15.604813575744629\n",
      "15.53013801574707\n",
      "16.655452728271484\n",
      "14.717852592468262\n",
      "17.044761657714844\n",
      "12.756488800048828\n",
      "14.809165000915527\n",
      "14.080717086791992\n",
      "14.54175090789795\n",
      "15.088059425354004\n",
      "12.308319091796875\n",
      "14.851411819458008\n",
      "16.382122039794922\n",
      "15.639039993286133\n",
      "14.980963706970215\n",
      "16.47264289855957\n",
      "16.545696258544922\n",
      "12.491803169250488\n",
      "13.882640838623047\n",
      "14.142651557922363\n",
      "13.24476432800293\n",
      "16.301244735717773\n",
      "14.575589179992676\n",
      "13.665367126464844\n",
      "13.806121826171875\n",
      "15.5916748046875\n",
      "12.150761604309082\n",
      "14.512128829956055\n",
      "11.806815147399902\n",
      "14.769146919250488\n",
      "14.148283958435059\n",
      "15.908742904663086\n",
      "14.270429611206055\n",
      "13.983354568481445\n",
      "11.700952529907227\n",
      "15.535828590393066\n",
      "15.660443305969238\n",
      "15.68790054321289\n",
      "13.770148277282715\n",
      "13.605307579040527\n",
      "14.755983352661133\n",
      "15.57597541809082\n",
      "13.780838012695312\n",
      "13.522076606750488\n",
      "16.946529388427734\n",
      "12.589465141296387\n",
      "11.993673324584961\n",
      "15.768816947937012\n",
      "17.248260498046875\n",
      "14.09237003326416\n",
      "12.93375301361084\n",
      "13.131431579589844\n",
      "14.35564136505127\n",
      "12.662909507751465\n",
      "13.538959503173828\n",
      "13.026932716369629\n",
      "15.580367088317871\n",
      "14.330690383911133\n",
      "15.223342895507812\n",
      "13.640020370483398\n",
      "10.984281539916992\n",
      "13.940475463867188\n",
      "15.21217155456543\n",
      "12.218631744384766\n",
      "11.748621940612793\n",
      "13.156977653503418\n",
      "15.117386817932129\n",
      "14.558927536010742\n",
      "13.967791557312012\n",
      "12.265019416809082\n",
      "13.080513954162598\n",
      "14.079569816589355\n",
      "11.427367210388184\n",
      "11.849246978759766\n",
      "14.440665245056152\n",
      "11.80372142791748\n",
      "11.961246490478516\n",
      "9.9404296875\n",
      "12.805779457092285\n",
      "9.949164390563965\n",
      "13.083266258239746\n",
      "10.451813697814941\n",
      "11.81942367553711\n",
      "12.06619930267334\n",
      "13.81005573272705\n",
      "12.097107887268066\n",
      "11.466731071472168\n",
      "13.907562255859375\n",
      "11.375679969787598\n",
      "12.204163551330566\n",
      "13.640661239624023\n",
      "11.34111213684082\n",
      "13.159944534301758\n",
      "11.935555458068848\n",
      "14.251395225524902\n",
      "11.58011245727539\n",
      "14.44105052947998\n",
      "11.838492393493652\n",
      "10.220674514770508\n",
      "12.053811073303223\n",
      "11.916096687316895\n",
      "10.042479515075684\n",
      "11.133716583251953\n",
      "11.737862586975098\n",
      "12.959821701049805\n",
      "9.436564445495605\n",
      "13.621335983276367\n",
      "11.917875289916992\n",
      "13.197199821472168\n",
      "12.86460018157959\n",
      "12.031743049621582\n",
      "8.578666687011719\n",
      "11.415435791015625\n",
      "14.398277282714844\n",
      "11.287976264953613\n",
      "12.737005233764648\n",
      "11.99500846862793\n",
      "14.880599975585938\n",
      "13.53198528289795\n",
      "10.91624927520752\n",
      "12.771163940429688\n",
      "13.78754711151123\n",
      "13.056966781616211\n",
      "13.044647216796875\n",
      "12.538239479064941\n",
      "11.662641525268555\n",
      "10.479567527770996\n",
      "11.246709823608398\n",
      "11.734912872314453\n",
      "13.051102638244629\n",
      "12.245171546936035\n",
      "11.709938049316406\n",
      "11.396129608154297\n",
      "9.515085220336914\n",
      "9.429095268249512\n",
      "6.743277549743652\n",
      "11.530463218688965\n",
      "10.38644027709961\n",
      "12.757271766662598\n",
      "11.91444206237793\n",
      "13.08730411529541\n",
      "10.723855972290039\n",
      "9.470831871032715\n",
      "9.480490684509277\n",
      "10.662890434265137\n",
      "13.375934600830078\n",
      "12.5\n",
      "12.192471504211426\n",
      "8.010043144226074\n",
      "11.248262405395508\n",
      "12.470611572265625\n",
      "10.528074264526367\n",
      "13.19966983795166\n",
      "8.69603157043457\n",
      "13.019342422485352\n",
      "11.452627182006836\n",
      "11.253132820129395\n",
      "13.352794647216797\n",
      "10.173402786254883\n",
      "8.501096725463867\n",
      "11.656913757324219\n",
      "10.221587181091309\n",
      "9.627946853637695\n",
      "12.04784870147705\n",
      "12.202149391174316\n",
      "9.21351432800293\n",
      "11.751282691955566\n",
      "10.989361763000488\n",
      "10.473617553710938\n",
      "9.20096206665039\n",
      "11.651394844055176\n",
      "12.436877250671387\n",
      "10.367039680480957\n",
      "9.565240859985352\n",
      "11.531683921813965\n",
      "9.097610473632812\n",
      "11.279932022094727\n",
      "9.779878616333008\n",
      "10.347984313964844\n",
      "6.136132717132568\n",
      "8.600214958190918\n",
      "12.039816856384277\n",
      "11.584826469421387\n",
      "9.80733871459961\n",
      "10.806272506713867\n",
      "9.665756225585938\n",
      "10.573342323303223\n",
      "8.95053482055664\n",
      "10.992680549621582\n",
      "8.605218887329102\n",
      "11.207318305969238\n",
      "7.977313041687012\n",
      "8.951423645019531\n",
      "10.376401901245117\n",
      "10.055536270141602\n",
      "11.44580364227295\n",
      "10.240762710571289\n",
      "9.606993675231934\n",
      "10.890143394470215\n",
      "8.389726638793945\n",
      "10.56085205078125\n",
      "10.05629825592041\n",
      "9.669781684875488\n",
      "8.963738441467285\n",
      "8.13115119934082\n",
      "8.444561004638672\n",
      "11.567577362060547\n",
      "7.9405646324157715\n",
      "11.713455200195312\n",
      "9.439105033874512\n",
      "8.19589900970459\n",
      "9.884449005126953\n",
      "9.718012809753418\n",
      "11.679950714111328\n",
      "8.719144821166992\n",
      "9.101794242858887\n",
      "9.140323638916016\n",
      "7.336053848266602\n",
      "8.475397109985352\n",
      "9.0613431930542\n",
      "8.144408226013184\n",
      "9.614089012145996\n",
      "10.197089195251465\n",
      "8.148189544677734\n",
      "7.510673999786377\n",
      "7.872158527374268\n",
      "7.3588972091674805\n",
      "8.497604370117188\n",
      "9.88927173614502\n",
      "9.553153038024902\n",
      "8.529409408569336\n",
      "8.836612701416016\n",
      "7.18763542175293\n",
      "8.265295028686523\n",
      "7.84084939956665\n",
      "9.295527458190918\n",
      "7.7700653076171875\n",
      "9.548124313354492\n",
      "8.517416000366211\n",
      "8.658964157104492\n",
      "9.072347640991211\n",
      "9.188891410827637\n",
      "7.686203479766846\n",
      "7.8424859046936035\n",
      "8.571765899658203\n",
      "9.04944133758545\n",
      "8.687017440795898\n",
      "8.117074966430664\n",
      "8.404279708862305\n",
      "6.029212951660156\n",
      "8.603525161743164\n",
      "7.101378440856934\n",
      "7.759884357452393\n",
      "8.384214401245117\n",
      "6.370513439178467\n",
      "8.980022430419922\n",
      "7.364080429077148\n",
      "8.078657150268555\n",
      "6.332517623901367\n",
      "6.895728588104248\n",
      "8.434226036071777\n",
      "8.647773742675781\n",
      "8.352089881896973\n",
      "8.465899467468262\n",
      "7.66425085067749\n",
      "9.184582710266113\n",
      "7.812682151794434\n",
      "8.395501136779785\n",
      "8.433656692504883\n",
      "8.39629077911377\n",
      "7.6763916015625\n",
      "7.509160041809082\n",
      "7.854641437530518\n",
      "9.212564468383789\n",
      "7.4044647216796875\n",
      "7.5265679359436035\n",
      "8.213839530944824\n",
      "7.49892520904541\n",
      "7.0401291847229\n",
      "5.836579322814941\n",
      "6.30905294418335\n",
      "6.538321018218994\n",
      "5.8348388671875\n",
      "7.572753429412842\n",
      "8.544983863830566\n",
      "7.850287914276123\n",
      "7.188656330108643\n",
      "6.3438191413879395\n",
      "6.244436264038086\n",
      "8.30203628540039\n",
      "6.847767353057861\n",
      "7.555723190307617\n",
      "6.938148021697998\n",
      "5.282790660858154\n",
      "5.131222724914551\n",
      "7.417585372924805\n",
      "4.79393196105957\n",
      "8.680671691894531\n",
      "7.067693710327148\n",
      "5.243935585021973\n",
      "7.810885906219482\n",
      "6.905805587768555\n",
      "6.828940391540527\n",
      "6.03212833404541\n",
      "5.817997455596924\n",
      "7.387227535247803\n",
      "6.319223880767822\n",
      "6.930750370025635\n",
      "5.302661418914795\n",
      "7.614256381988525\n",
      "8.199700355529785\n",
      "6.811288833618164\n",
      "4.80958890914917\n",
      "5.711043834686279\n",
      "6.430364608764648\n",
      "5.5654683113098145\n",
      "6.319112777709961\n",
      "6.8329949378967285\n",
      "5.123787879943848\n",
      "6.9424824714660645\n",
      "5.020712852478027\n",
      "6.329761505126953\n",
      "6.278172492980957\n",
      "5.361441135406494\n",
      "6.547224521636963\n",
      "8.321125030517578\n",
      "6.839669227600098\n",
      "6.335254192352295\n",
      "6.686542987823486\n",
      "6.804293155670166\n",
      "6.158873081207275\n",
      "5.540233612060547\n",
      "6.783899307250977\n",
      "4.825080394744873\n",
      "6.507523059844971\n",
      "6.5220770835876465\n",
      "5.620449066162109\n",
      "7.207695960998535\n",
      "7.153838157653809\n",
      "8.410587310791016\n",
      "7.858876705169678\n",
      "5.885406494140625\n",
      "4.83630895614624\n",
      "5.1678338050842285\n",
      "7.563727855682373\n",
      "5.629269123077393\n",
      "4.2080230712890625\n",
      "3.1427934169769287\n",
      "6.115643501281738\n",
      "4.138995170593262\n",
      "5.359128952026367\n",
      "5.766622543334961\n",
      "5.5555033683776855\n",
      "3.8731722831726074\n",
      "4.790816307067871\n",
      "4.68593168258667\n",
      "5.151075839996338\n",
      "5.16728401184082\n",
      "6.262267112731934\n",
      "5.261269569396973\n",
      "6.2422637939453125\n",
      "5.249063491821289\n",
      "7.123711585998535\n",
      "4.716073036193848\n",
      "5.399234771728516\n",
      "6.26405143737793\n",
      "6.321558952331543\n",
      "5.638017177581787\n",
      "6.617240905761719\n",
      "4.026035785675049\n",
      "6.827410697937012\n",
      "5.618313312530518\n",
      "4.957659721374512\n",
      "5.943419456481934\n",
      "6.506533145904541\n",
      "4.030764579772949\n",
      "4.914910316467285\n",
      "5.1573166847229\n",
      "6.181315898895264\n",
      "5.406886100769043\n",
      "5.417940139770508\n",
      "4.440244197845459\n",
      "4.565494537353516\n",
      "5.906709671020508\n",
      "4.818332195281982\n",
      "4.8619489669799805\n",
      "4.694300651550293\n",
      "5.6686201095581055\n",
      "4.841231822967529\n",
      "4.115238666534424\n",
      "4.848031520843506\n",
      "5.008069038391113\n",
      "4.6388983726501465\n",
      "3.7196009159088135\n",
      "5.467813014984131\n",
      "3.6537795066833496\n",
      "6.298250198364258\n",
      "4.339572429656982\n",
      "4.807205677032471\n",
      "4.651432514190674\n",
      "3.503444194793701\n",
      "3.5012218952178955\n",
      "4.797824859619141\n",
      "3.8731727600097656\n",
      "5.690114974975586\n",
      "5.451676368713379\n",
      "4.769299030303955\n",
      "4.859259605407715\n",
      "4.0182671546936035\n",
      "5.152000427246094\n",
      "3.823542594909668\n",
      "4.202470302581787\n",
      "4.087793350219727\n",
      "4.306668281555176\n",
      "4.405178070068359\n",
      "3.3234918117523193\n",
      "4.729369163513184\n",
      "4.378171920776367\n",
      "4.041868209838867\n",
      "4.508281230926514\n",
      "4.285272598266602\n",
      "4.642301559448242\n",
      "3.417506456375122\n",
      "3.617520332336426\n",
      "3.903470993041992\n",
      "3.9896273612976074\n",
      "5.024093151092529\n",
      "4.308187484741211\n",
      "3.5544893741607666\n",
      "3.9760334491729736\n",
      "3.9153103828430176\n",
      "3.929921865463257\n",
      "5.592280387878418\n",
      "4.5964484214782715\n",
      "3.7362704277038574\n",
      "3.6150314807891846\n",
      "4.440110683441162\n",
      "3.607165813446045\n",
      "4.565478801727295\n",
      "4.647752285003662\n",
      "3.921173572540283\n",
      "3.909330129623413\n",
      "4.223657131195068\n",
      "3.557563066482544\n",
      "3.432541847229004\n",
      "3.485616683959961\n",
      "4.496074676513672\n",
      "3.8855819702148438\n",
      "3.716451644897461\n",
      "3.9214119911193848\n",
      "4.213665008544922\n",
      "3.5233876705169678\n",
      "4.668709754943848\n",
      "4.087869644165039\n",
      "3.185755968093872\n",
      "3.5807688236236572\n",
      "4.316976547241211\n",
      "3.905600070953369\n",
      "3.9268038272857666\n",
      "4.312053680419922\n",
      "4.7863664627075195\n",
      "3.0266330242156982\n",
      "4.464250087738037\n",
      "3.8693318367004395\n",
      "3.8036820888519287\n",
      "4.229706764221191\n",
      "3.39441180229187\n",
      "4.015264987945557\n",
      "3.937159776687622\n",
      "4.043885231018066\n",
      "3.970919370651245\n",
      "4.036652565002441\n",
      "2.9348318576812744\n",
      "4.1443328857421875\n",
      "3.705475091934204\n",
      "3.117889165878296\n",
      "3.5020384788513184\n",
      "3.065528631210327\n",
      "3.6887760162353516\n",
      "3.657364845275879\n",
      "3.5323240756988525\n",
      "3.9604251384735107\n",
      "2.9974775314331055\n",
      "3.5955939292907715\n",
      "3.3281078338623047\n",
      "3.9331324100494385\n",
      "3.8235702514648438\n",
      "3.3842577934265137\n",
      "4.085208892822266\n",
      "4.3039116859436035\n",
      "3.4371089935302734\n",
      "3.4150731563568115\n",
      "3.173901081085205\n",
      "3.1596312522888184\n",
      "4.184057712554932\n",
      "3.2619946002960205\n",
      "2.8443336486816406\n",
      "3.8296613693237305\n",
      "3.7578296661376953\n",
      "3.489307403564453\n",
      "3.937769889831543\n",
      "4.105751037597656\n",
      "3.4081428050994873\n",
      "3.9598751068115234\n",
      "3.6557352542877197\n",
      "3.1146368980407715\n",
      "3.946455478668213\n",
      "3.020233631134033\n",
      "3.1138486862182617\n",
      "3.252655506134033\n",
      "3.397702693939209\n",
      "3.2695822715759277\n",
      "3.6450743675231934\n",
      "3.3007216453552246\n",
      "3.7886548042297363\n",
      "3.30999493598938\n",
      "3.0773465633392334\n",
      "2.5902516841888428\n",
      "3.0733273029327393\n",
      "3.808659791946411\n",
      "3.209304094314575\n",
      "3.582448720932007\n",
      "3.240417242050171\n",
      "3.816272735595703\n",
      "2.8274214267730713\n",
      "2.8313848972320557\n",
      "3.360301971435547\n",
      "3.6175830364227295\n",
      "3.398488759994507\n",
      "3.1837682723999023\n",
      "3.3466174602508545\n",
      "3.751555919647217\n",
      "3.273345470428467\n",
      "3.927722692489624\n",
      "4.40465784072876\n",
      "3.300276279449463\n",
      "3.9627599716186523\n",
      "4.263436317443848\n",
      "3.7055883407592773\n",
      "3.014963150024414\n",
      "3.281191825866699\n",
      "3.143592119216919\n",
      "3.553664207458496\n",
      "3.811861038208008\n",
      "3.2449228763580322\n",
      "2.928278923034668\n",
      "3.1758809089660645\n",
      "3.1167173385620117\n",
      "2.849334716796875\n",
      "2.4049229621887207\n",
      "3.360260009765625\n",
      "3.267026424407959\n",
      "3.4814298152923584\n",
      "2.978137731552124\n",
      "4.152027606964111\n",
      "3.0323173999786377\n",
      "3.2563059329986572\n",
      "3.532460927963257\n",
      "2.740464210510254\n",
      "3.402660846710205\n",
      "3.045889139175415\n",
      "2.5319976806640625\n",
      "2.987276792526245\n",
      "3.255650281906128\n",
      "3.064018487930298\n",
      "3.764214038848877\n",
      "2.9883735179901123\n",
      "3.0785796642303467\n",
      "2.8870089054107666\n",
      "3.433373212814331\n",
      "2.6346077919006348\n",
      "2.904273509979248\n",
      "2.6546621322631836\n",
      "2.783897638320923\n",
      "3.0608878135681152\n",
      "3.4957261085510254\n",
      "3.122154951095581\n",
      "4.009383678436279\n",
      "2.923637866973877\n",
      "2.85046648979187\n",
      "2.6355204582214355\n",
      "3.0459156036376953\n",
      "3.5926074981689453\n",
      "3.0401792526245117\n",
      "2.8373160362243652\n",
      "4.033605098724365\n",
      "2.9301013946533203\n",
      "3.1626951694488525\n",
      "2.927673816680908\n",
      "2.7731521129608154\n",
      "2.8517889976501465\n",
      "2.7044436931610107\n",
      "3.204880952835083\n",
      "2.782909393310547\n",
      "2.8487188816070557\n",
      "2.843104362487793\n",
      "3.182309150695801\n",
      "2.658759593963623\n",
      "2.9409663677215576\n",
      "2.5932931900024414\n",
      "3.5944230556488037\n",
      "2.5018324851989746\n",
      "2.9553234577178955\n",
      "2.7133302688598633\n",
      "2.864974021911621\n",
      "3.1940088272094727\n",
      "3.095775604248047\n",
      "3.5267605781555176\n",
      "3.3730974197387695\n",
      "2.968865394592285\n",
      "3.5772135257720947\n",
      "3.6615734100341797\n",
      "3.0563347339630127\n",
      "3.3555846214294434\n",
      "3.159672260284424\n",
      "3.7313694953918457\n",
      "2.9805710315704346\n",
      "3.1256895065307617\n",
      "2.9329137802124023\n",
      "3.315716028213501\n",
      "2.805382251739502\n",
      "2.7401299476623535\n",
      "2.815256118774414\n",
      "3.039808750152588\n",
      "2.881103515625\n",
      "2.9593772888183594\n",
      "2.6372153759002686\n",
      "2.799372434616089\n",
      "3.7830493450164795\n",
      "3.8990390300750732\n",
      "3.2182703018188477\n",
      "3.3629705905914307\n",
      "2.832535743713379\n",
      "2.695443868637085\n",
      "2.883486747741699\n",
      "3.685537576675415\n",
      "3.0434868335723877\n",
      "2.862704038619995\n",
      "3.0226027965545654\n",
      "3.0583748817443848\n",
      "2.8876471519470215\n",
      "3.296377420425415\n",
      "2.7223129272460938\n",
      "3.652494430541992\n",
      "3.1887760162353516\n",
      "3.5312979221343994\n",
      "4.172987937927246\n",
      "3.3395917415618896\n",
      "2.5004162788391113\n",
      "3.4513370990753174\n",
      "3.1702117919921875\n",
      "3.379913568496704\n",
      "2.974946975708008\n",
      "2.7316131591796875\n",
      "3.2757744789123535\n",
      "3.3096909523010254\n",
      "3.1007890701293945\n",
      "2.953339099884033\n",
      "4.10173225402832\n",
      "2.731839179992676\n",
      "3.5981976985931396\n",
      "3.4899797439575195\n",
      "4.125173568725586\n",
      "3.9617464542388916\n",
      "3.8144662380218506\n",
      "2.8502860069274902\n",
      "3.5049290657043457\n",
      "3.582820415496826\n",
      "3.0177905559539795\n",
      "2.9563369750976562\n",
      "3.510143518447876\n",
      "3.5628244876861572\n",
      "4.422394275665283\n",
      "4.261119842529297\n",
      "5.918086051940918\n",
      "4.9895339012146\n",
      "4.119465351104736\n",
      "3.715899705886841\n",
      "3.210862159729004\n",
      "3.6552021503448486\n",
      "3.5488619804382324\n",
      "3.108765125274658\n",
      "3.4193551540374756\n",
      "3.434652090072632\n",
      "3.3022303581237793\n",
      "3.253847122192383\n",
      "3.083003520965576\n",
      "3.045696973800659\n",
      "4.023062705993652\n",
      "4.018720626831055\n",
      "3.635441780090332\n",
      "5.103662490844727\n",
      "2.8183672428131104\n",
      "3.8889331817626953\n",
      "4.111485958099365\n",
      "3.630751609802246\n",
      "3.629887580871582\n",
      "3.9061295986175537\n",
      "3.6072280406951904\n",
      "3.5880227088928223\n",
      "4.187137126922607\n",
      "2.7847774028778076\n",
      "3.373539924621582\n",
      "3.970195770263672\n",
      "2.9350688457489014\n",
      "3.7945632934570312\n",
      "4.226652145385742\n",
      "2.6698338985443115\n",
      "3.1258649826049805\n",
      "3.8354809284210205\n",
      "4.131811618804932\n",
      "4.531652927398682\n",
      "3.7562620639801025\n",
      "3.4068944454193115\n",
      "3.263073444366455\n",
      "4.645474910736084\n",
      "3.3286898136138916\n",
      "3.545820474624634\n",
      "3.120030641555786\n",
      "3.8165760040283203\n",
      "3.769808292388916\n",
      "3.340409517288208\n",
      "2.869058847427368\n",
      "3.610565185546875\n",
      "4.2059326171875\n",
      "3.7101030349731445\n",
      "4.809588432312012\n",
      "4.202589511871338\n",
      "3.5595953464508057\n",
      "4.379583358764648\n",
      "4.070370197296143\n",
      "5.421417236328125\n",
      "5.349225997924805\n",
      "5.521119117736816\n",
      "4.37294340133667\n",
      "5.11857795715332\n",
      "4.906012535095215\n",
      "4.452506065368652\n",
      "3.263639450073242\n",
      "4.509565830230713\n",
      "5.067665100097656\n",
      "5.6551713943481445\n",
      "5.166839122772217\n",
      "3.98996901512146\n",
      "3.404005527496338\n",
      "4.805929183959961\n",
      "3.984356164932251\n",
      "3.738556146621704\n",
      "4.239109039306641\n",
      "4.3320465087890625\n",
      "4.159628868103027\n",
      "6.4095072746276855\n",
      "5.216495037078857\n",
      "5.589499473571777\n",
      "4.143707275390625\n",
      "5.9821882247924805\n",
      "4.97962760925293\n",
      "6.502350807189941\n",
      "4.815829277038574\n",
      "4.287289619445801\n",
      "3.4174160957336426\n",
      "3.645806074142456\n",
      "4.383102893829346\n",
      "4.059452533721924\n",
      "4.148805618286133\n",
      "5.216536045074463\n",
      "4.743788719177246\n",
      "4.600874423980713\n",
      "5.41658353805542\n",
      "5.084006309509277\n",
      "5.933795928955078\n",
      "4.900230407714844\n",
      "5.296424388885498\n",
      "6.234168529510498\n",
      "4.29722261428833\n",
      "5.072258949279785\n",
      "5.697539806365967\n",
      "6.3347063064575195\n",
      "4.9947919845581055\n",
      "5.183926105499268\n",
      "4.733334541320801\n",
      "4.028289794921875\n",
      "3.775473117828369\n",
      "6.370595455169678\n",
      "5.731720924377441\n",
      "4.398096084594727\n",
      "6.247612953186035\n",
      "5.1913323402404785\n",
      "6.0823259353637695\n",
      "5.798513412475586\n",
      "3.9717183113098145\n",
      "4.480928897857666\n",
      "4.768271446228027\n",
      "4.539175510406494\n",
      "5.175062656402588\n",
      "6.019742012023926\n",
      "4.712630748748779\n",
      "3.8096659183502197\n",
      "5.076295852661133\n",
      "3.7954697608947754\n",
      "6.609382152557373\n",
      "6.599194049835205\n",
      "7.463079929351807\n",
      "5.687852382659912\n",
      "8.72004508972168\n",
      "8.750822067260742\n",
      "6.500101089477539\n",
      "6.105545997619629\n",
      "7.693253993988037\n",
      "6.719013214111328\n",
      "7.738780975341797\n",
      "9.048693656921387\n",
      "7.783837795257568\n",
      "4.506874084472656\n",
      "7.020900726318359\n",
      "8.976203918457031\n",
      "5.296772480010986\n",
      "7.5521697998046875\n",
      "9.726219177246094\n",
      "6.246577739715576\n",
      "9.729144096374512\n",
      "8.382579803466797\n",
      "9.50208568572998\n",
      "9.335992813110352\n",
      "8.960504531860352\n",
      "7.714150428771973\n",
      "8.334391593933105\n",
      "8.073904991149902\n",
      "8.945304870605469\n",
      "10.014039993286133\n",
      "9.503973960876465\n",
      "7.519698143005371\n",
      "7.878063678741455\n",
      "8.104159355163574\n",
      "8.179377555847168\n",
      "7.377048492431641\n",
      "5.752250671386719\n",
      "7.430034160614014\n",
      "9.592267036437988\n",
      "8.766654014587402\n",
      "8.886822700500488\n",
      "8.082719802856445\n",
      "6.066549301147461\n",
      "10.51104736328125\n",
      "6.967471599578857\n",
      "8.940640449523926\n",
      "8.249980926513672\n",
      "7.680236339569092\n",
      "6.508126735687256\n",
      "5.815784454345703\n",
      "7.998375415802002\n",
      "10.497540473937988\n",
      "9.698808670043945\n",
      "8.398005485534668\n",
      "9.06019115447998\n",
      "8.020445823669434\n",
      "8.260425567626953\n",
      "5.876376628875732\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "C, W1, b1, W2, b2 = parameters\n",
    "\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    idx = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    emb = C[X[idx]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[idx])\n",
    "    print(loss.item())\n",
    "\n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = lrs[i]\n",
    "\n",
    "    for param in parameters:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    # Track stats\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ed8cac0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABlxUlEQVR4nO3deXxU9bk/8M+ZfbJNSEI2CEvYZREEBFRcsYjWrf7qbrXXtre32N7qbW2tVq3aar1dbHvR3tZW7WK57lq1bii4ACIIorJDgLAkIfs+6/n9ceZ75pwzS2aS2cJ83q9XXpXJLN8MKeeZ53m+z1eSZVkGERERUZqYMr0AIiIiyi0MPoiIiCitGHwQERFRWjH4ICIiorRi8EFERERpxeCDiIiI0orBBxEREaUVgw8iIiJKK0umF2AUCARw5MgRFBYWQpKkTC+HiIiI4iDLMrq6ulBdXQ2TKXZuI+uCjyNHjqCmpibTyyAiIqJBqK+vx+jRo2PeJ+uCj8LCQgDK4ouKijK8GiIiIopHZ2cnampq1Ot4LFkXfIhSS1FREYMPIiKiYSaelgk2nBIREVFaMfggIiKitGLwQURERGnF4IOIiIjSisEHERERpRWDDyIiIkorBh9ERESUVgw+iIiIKK0YfBAREVFaMfggIiKitGLwQURERGnF4IOIiIjSKqeCj/X7WrByw8FML4OIiCinZd2ptql05R/WAwAmVRRg7tiSDK+GiIgoN+VU5kOob+3L9BKIiIhyVk4GH0RERJQ5DD6IiIgorRh8EBERUVrlZPAhQ870EoiIiHJWTgYfRERElDkMPoiIiCitGHwQERFRWjH4ICIiorRi8EFERERplZPBh8zNLkRERBmTk8EHERERZQ6DDyIiIkorBh9ERESUVgw+iIiIKK1yMvhgwykREVHm5GTwQURERJmTk8EHEx9ERESZkzPBR7/Xr/734ba+DK6EiIgot+VM8KElM/dBRESUMTkZfBAREVHm5EzwsaOhS/1v7nYhIiLKnJwJPo60h/o8GHsQERFlTs4EH1KmF0BEREQAcij4ICIiouzA4IOIiIjSKjeDD3acEhERZUzOBB+SpumDoQcREVHm5EzwQURERNmBwQcRERGlVQ4FH6G6C1s+iIiIMidngg+Jgz6IiIiyQs4EH1o8WI6IiChzcjL4ICIioszJyeCDPR9ERESZkzPBB1s+iIiIskPOBB9ERESUHXIy+GDVhYiIKHNyM/hg9EFERJQxORN8SBz0QURElBVyJvggIiKi7JCTwQeHjBEREWVOTgYfRERElDm5GXww8UFERJQxuRl8EBERUcbkZPDBxAcREVHm5GTwQURERJnD4IOIiIjSKieDD5kjTomIiDImR4OPTK+AiIgod+Vk8EFERESZk5PBx6Pv12V6CURERDkroeDj/vvvx/z581FYWIjy8nJccskl2Llzp+4+/f39WL58OUpLS1FQUIDLLrsMjY2NSV00ERERDV8JBR9r1qzB8uXLsX79erz55pvwer34whe+gJ6eHvU+N998M/75z3/i6aefxpo1a3DkyBF86UtfSvrCiYiIaHiyJHLn1157Tffnxx9/HOXl5di0aRNOP/10dHR04E9/+hOefPJJnH322QCAxx57DNOmTcP69euxcOHC5K2ciIiIhqUh9Xx0dHQAAEpKSgAAmzZtgtfrxZIlS9T7TJ06FWPGjMG6deuG8lJDJmX01YmIiEhIKPOhFQgE8N3vfhennnoqZsyYAQBoaGiAzWZDcXGx7r4VFRVoaGiI+Dxutxtut1v9c2dn52CXRERERMPAoDMfy5cvx2effYaVK1cOaQH3338/XC6X+lVTUzOk5yMiIqLsNqjg46abbsLLL7+Md955B6NHj1Zvr6yshMfjQXt7u+7+jY2NqKysjPhct912Gzo6OtSv+vr6wSyJiIiIhomEgg9ZlnHTTTfh+eefx9tvv43x48frvj937lxYrVasWrVKvW3nzp04ePAgFi1aFPE57XY7ioqKdF9ERER0/Eqo52P58uV48skn8eKLL6KwsFDt43C5XHA6nXC5XLjxxhtxyy23oKSkBEVFRfj2t7+NRYsWcacLERERAUgw+HjkkUcAAGeeeabu9sceeww33HADAODXv/41TCYTLrvsMrjdbixduhQPP/xwUhZLREREw19CwUc8p8E6HA6sWLECK1asGPSiiIiI6PiVk2e7EBERUebkTPAhccoYERFRVsiZ4IOIiIiyA4MPIiIiSisGH0RERJRWDD6IiIgorRh8EBERUVrlTPDR2e/N9BKIiIgIORR8eP0DD0gjIiKi1MuZ4IOIiIiyA4MPIiIiSisGH0RERJRWuRN8sOWDiIgoK+RM8CEz+iAiIsoKORN8EBERUXbImeBDZuKDiIgoK+RM8EFERETZIWeCD0nK9AqIiIgIyKHgg2UXIiKi7JAzwQcRERFlh5wJPpj4ICIiyg45E3wQERFRdmDwQURERGmVM8EHG06JiIiyQ84EH0RERJQdGHwQERFRWjH4ICIiorRi8EFERERplTPBh8xJH0RERFkhZ4IPIiIiyg45E3y4nNZML4GIiIiQQ8FHSZ4t00sgIiIi5FDwQURERNmBwQcRERGlFYMPIiIiSisGH0RERJRWORN8GKd8+AOc+0FERJQJORN8GK3Z1ZTpJRAREeWknA0++r2BTC+BiIgoJ+VM8CHLsf9MRERE6ZEzwYcRz3ohIiLKjJwNPjw+ll2IiIgyIWeDj62HOjK9BCIiopyUM8EHyyxERETZIXeCD8YeREREWSFngg8iIiLKDgw+iIiIKK1yJvhg1YWIiCg75EzwQURERNmBwQcRERGlVc4EHxPLCzK9BCIiIkIOBR/5NnOml0BERETIoeDDSJIyvQIiIqLclLPBBxEREWUGgw8iIiJKq5wNPjhunYiIKDNyNvggIiKizMjZ4IMNp0RERJmRs8EHERERZUbOBh/s+SAiIsqMHA4+GH0QERFlQs4GH74Agw8iIqJMyNngo9BhzfQSiIiIclLOBh8ymPkgIiLKhJwNPoiIiCgzcjb4kMBBH0RERJmQs8EHERERZQaDDyIiIkqrnA0+2HBKRESUGQkHH++++y4uvPBCVFdXQ5IkvPDCC7rv33DDDZAkSfd13nnnJWu9ycPYg4iIKCMSDj56enpw4oknYsWKFVHvc9555+Ho0aPq1z/+8Y8hLTIVth7qyPQSiIiIcpIl0QcsW7YMy5Yti3kfu92OysrKQS8qFYzT1Nfta8nMQoiIiHJcSno+Vq9ejfLyckyZMgX/8R//gZaW6Bd6t9uNzs5O3RcREREdv5IefJx33nn4y1/+glWrVuHnP/851qxZg2XLlsHv90e8//333w+Xy6V+1dTUJHtJRERElEUSLrsM5Morr1T/e+bMmZg1axYmTJiA1atX45xzzgm7/2233YZbbrlF/XNnZ2faApC9x7oxYWRBWl6LiIiIFCnfaltbW4uysjLs2bMn4vftdjuKiop0X+my+WB72l6LiIiIFCkPPg4dOoSWlhZUVVWl+qWIiIhoGEi47NLd3a3LYtTV1WHLli0oKSlBSUkJfvKTn+Cyyy5DZWUl9u7di1tvvRUTJ07E0qVLk7pwIiIiGp4SDj42btyIs846S/2z6Ne4/vrr8cgjj2Dr1q144okn0N7ejurqanzhC1/AvffeC7vdnrxVExER0bCVcPBx5plnQjYOzdB4/fXXh7QgIiIiOr7l7NkuRERElBkMPoiIiCitcjr4kDK9ACIiohyU08EHD7YlIiJKv5wOPoiIiCj9GHwQERFRWuV08MGeDyIiovTL6eCDPR9ERETpl9PBBxEREaVfTgcfLLsQERGlX04HH0RERJR+OR18tPV6Mr0EIiKinJPTwcej79VleglEREQ5J6eDj0CM03mJiIgoNXI6+GDoQURElH65HXww+iAiIkq7nA4+iIiIKP1yOvho7nZneglEREQ5J6eDDwDwB1h7ISIiSqecDz5amP0gIiJKq5wJPuzWnPlRiYiIslrOXJHzbJZML4GIiIiQQ8EHERERZQcGH0RERJRWDD6IiIgorRh8EBERUVox+CAiIqK0YvBBREREacXgg4iIiNKKwQcRERGlFYMPIiIiSisGH0RERJRWOR98dLt9mV4CERFRTsn54OOnr2zP9BKIiIhySs4HH5sOtmV6CURERDklp4KPyiJH2G2ynIGFEBER5bCcCj6umF8Tdlu/15+BlRAREeWunAo+LCYp7Da3L5CBlRAREeWunAo+zppanuklEBER5bycCj7slpz6cYmIiLISr8ZERESUVjkVfEjhLR9ERESUZjkVfBAREVHm5VjwwdQHERFRpuVY8BHZj1/4LNNLICIiyhk5FXzUlDgj3v7X9QfSvBIiIqLclVPBh91izvQSiIiIcl5OBR9ERESUeQw+iIiIKK0YfBAREVFaMfggIiKitGLwQURERGnF4IOIiIjSisEHERERpRWDDyIiIkorBh9ERESUVgw+iIiIKK0YfBAREVFaMfggIiKitGLwQURERGnF4IOIiIjSisEHERERpRWDDyIioiSQZRn+gJzpZQwLDD6IiIiS4JanPsEpD6xCZ7836c+9oa4Vl/9+HT4/0pH0584EBh9ERERJ8N7uY2jsdGN3Y3fSn/vZTYewYX8r/rb+QNKfOxMYfBARESVBr8cPAOhx+5L+3K29HgBKBuR4wOCDiIhoiGRZRp83dcFHezD42HusBy3d7qQ/f7ox+CAiIhoity8AOdhr2p2C4KOtN9RHsvFAW9KfP90SDj7effddXHjhhaiuroYkSXjhhRd035dlGXfeeSeqqqrgdDqxZMkS7N69O1nrTRmfP5DpJRAR0TDVH8x6AKnNfADAR8dB6SXh4KOnpwcnnngiVqxYEfH7Dz74IH7729/i97//PT788EPk5+dj6dKl6O/vH/JiU6m+rS/TSyAiomFK9HsAQI/mv5NBlmVd5uOj/cM/+LAk+oBly5Zh2bJlEb8nyzIeeugh3HHHHbj44osBAH/5y19QUVGBF154AVdeeeXQVptCUqYXQEREcfEHZHT1e1GcZ8v0UlR9msxHsssunf0+3fyQz450osftQ7494Ut41khqz0ddXR0aGhqwZMkS9TaXy4UFCxZg3bp1ER/jdrvR2dmp+yIiIormV2/uxEn3vom1e5szvRRVnyd1ZRdRcsm3mVHtcsAfkLGlvj2pr5FuSQ0+GhoaAAAVFRW62ysqKtTvGd1///1wuVzqV01NTTKXREREx5k1u44hIAMrN9Rneimq/hRmPlp7lOCjOM+G+eNLAAz/LbcZ3+1y2223oaOjQ/2qr0/tL9Ovrzgx4u0fHxz+3cNERMc7WZZRd6wHAPD2jia4fcntrxis3pRmPpR+jxH5VswbpwQfw73vI6nBR2VlJQCgsbFRd3tjY6P6PSO73Y6ioiLdVyqdP7Mq4u1Pfngwpa9LRERD19TlVhs6u90+rN3bkuEVKfp0u12SGxC1BcsuI/JsODkYfGw+2A7vMN6lmdTgY/z48aisrMSqVavU2zo7O/Hhhx9i0aJFyXyppAvIPAyIiGiw3tzWiEff26e77eHVe/DC5sNJfZ29x/Sjy1//LHJJP91SWXYRO11G5NkwqbwALqcVfV4/Pj8yfHskEw4+uru7sWXLFmzZsgWA0mS6ZcsWHDx4EJIk4bvf/S7uu+8+vPTSS/j000/xla98BdXV1bjkkkuSvPTkYuhBRDR4P3x2K+57ZTsOtvQCAA639+HB13bih89tRSCJJ73uC5ZcRuRZAQBvbGvMipNkU9lw2tYjMh9WmEwS5o8bAQDYOIxLLwkHHxs3bsScOXMwZ84cAMAtt9yCOXPm4M477wQA3Hrrrfj2t7+Nb3zjG5g/fz66u7vx2muvweFwJHflScbEBxHR4PgDMlqCF8j2PuV/O4Kf1vu9AfV7ySCCj4tnj0JxnhWtPZ6k9z/sbuzC0xvrISdwYUhlz4cou4itxdOrXQCQkgPs0iXhTcJnnnlmzL8QSZJwzz334J577hnSwlJFijLRY7hvWyIiypTOvtAALNHv0OcNXYCPdvRhZKEdgBKomE2Dn6xU16xccCdXFGLJtAo8s+kQXv+8AQtrSwf9nEY/fO5TbDrQhiqXE6dNKovrMamc8yEaTkvyleCjdmQ+AKCuuSepr5NOGd/tQkREw1uHJvgQQYc2E3CkXZlw3dDRj7n3vYkbH/9I1yORiH3BC+74snwsna5sZHjj88ZYD0nY4eDE640H4s+o6Mare/wJZU0GEtpqq5SaxpUGg48WBh9ERJSjtMGHCDq0wcfRDuVi/mFdC9p7vVi1ownf+cfmhM/Ucvv8qG9VekomjMxXex8Ot/clbcutLMvq8fWfJJAR1/Z8+AMy3L7k7UTR7nYBgHFlSvBxrMuNrn5v1Mdls5wLPiTOUSciSqr2iMGHtuyiZD4OBJtRAaVR9NZntyaUITjY0ouADBTYLRhZaEeRwwpLsITTmqS+kh6PH55g4PDJoY6419dryOQks/TSrtntAgAupxWlwRKM9j0dTnIu+CAiouTSlV0iZD6OtCuZD3GhXFRbCrNJwnMfH8brCZRMtCUXSZJgMkkYEbwIt3QnJ/ho0wQxrT0eHIrz0NF+w2FyyWo6VQ6VC2Y+8q3q7eOD2Y99w7Tvg8GHxtMb6wddhyQiylWRyi59EYKPg63KhfKqBWPw9cW1AIC/rt8f9+uInS6i4RKAmgFIVubD+DyfHGqP63F9Kcp89Hn9aglnhOYgPRF8iGmvww2DD43vP7MVD/xrR6aXQUQ0rHTqMh/hDafGssvYkjxct2gsTBLwwZ4W7GmKb8vovuCAsdqyAvW2klQHH3H2fRiDj2RNORUDxmxmE/JsZvV20fexf5g2neZc8DFQy8crnx5NyzqIiI4X4tRVIHLDaWNnP7r6vWjqcgMAxpbmYVSxE+dMUw4h/dv6A3G9jigxaDMfIvhI1iyR8MxHR1yP6zWWXTzJyXy0aXa6SJqmxVqWXY4v7EclolzT7/Xj2kc/xIp39gzq8bqyi1eUXUIX34AMbDygHN5Z5LCow7KuWzgWAPDspkNx9UjUaXo+hFDZxT2otRuJ/ooTqpRzxj491BHXrhxjyT5ZPR9iPSLIEsapZZdutSl29c4mTPvxa/jnJ0eS8tqpxOCDiCjHfXygDe/vacaf36+Leb9H39uHh97aFXZ7pIbTHkMmYP0+5QC4cZrA4bSJZRhflo8utw8vbol9wWzv9ahZCX3mQxlelqyyi8igzB83AgV2C/q8fuw5NnBZSPzcojSiDT76vf5BbwUWZRcx40MQsz46+33qff6y7gD6vP4B38tskHPBh8S9tkREOqIno6XHE7Xp3uML4GevbsdDb+3W7QgBjA2nykW3Lyz4UAZ2jSnJU28zmSRcs2AMAAz4aX1vsLGyyuVAni00nLukIDW7XcoK7Jg5ShljvrVe2XK7pb5dHRtvJHo+ygqUYKg72PPR3uvBwvtX4WtPbBzUetoNMz4Ep82MKpdybEldcw/6vX6s3dsMANh+NPsPnMu54IOIiPQaOvtD/93RH/E+7b0eiPPbjDs52nujz/kQn/c+O6z0TowtzdM99qSxyqCwg62x51XsaFAuqJMqCnW3lyW550M8z4h8G2bVKMHH6l1N+Pe/bsIlKz7Ad1Zujvg4EWyVBYMhkfnY0dCF9l4v3tvdHPW9jaWtx6uux0jd8dLcg3X7WtDvVcpDh9v7ogZJ2YLBBxHRccTnD+gaQOMhJpACwJGOyHMtWjXPadzZ0RljzseoYicAqCfPji3J1z129AinugZvjN4K8Wl+WpU++Ej2bheR+SjJt2H26GIAwKufNuCNbco8kg/rWiKuU7wn4gwbEXw0d4d6UURmIqH1qJkPa9j31L6P5m6s3tGk+962LM9+MPgwyIKTmYmIErahrhVX/3E9Zv3kDcy+582Emg61n8jFOSxG4hM4EF5S0ZZdRK+HuBhPGFmgu+8YQ+ZjZIEddosJARk4GuW1AWD70S4AoUZQoVQtuySn4bRV0+B5Yk2xevuUikIU2i3o9waws6FL9xhZliOUXZTg41hXaF3v7xlK8BGe+ajVZD7e2XkMAFDoUEpS2V56YfBh0Nztxg+e2ZrpZRARJeR3b+/G2r0tasbh44NtcT/2qCb4ONoeOfPRHiXz4fUHdM2lxjkfxuDDWHaRJEnNfhxqi1x6CQRk7FAzH/rgQzScdvb7YmZO4tWqyXxUFztxxwXT8P2lU/DSt0/FnGCJaLPhvXX7AhBT2EXwETHzsacl4QPnQg2n0csuH+xpwcHWXtjMJlw5vwYAMx/D0v9trM/0EoiIEtLZr1zsZoxSLs6dffFv9dRlPqL0JbRpegi0wYc26wFoej6CF9+J5aHgw24xoaLQEfbco0coAUl9lOCjvq0XPR4/bBaT+mlfKHZaETzeJawRNlE+f0D9eUQ552uLa7H8rImwW8yYE8yEbD7YrnucNhMkej5Ew6k289HQ2Z/wXI5QGSh62UWseUFtCeaNKwEAbDvC4COrcK8LER2PxMU+tAUzvoZDt8+va9Y8GqXno02b+fBEDz7Uno9ggKINPsaU5MFkCv9XOJT5iPza4kI6paIQFrP+smUySWpJYqhNpx19XjWDUewMv9jPGVMMANhsmHoqgjGb2YSi4ONCmQ/9mj5IsPQi3vdImY+aEXkwa97Ps6aUq2WpPU3d6gF52Sjngo9Iv/hERMOdyDhUFimZhXiPWm/q1PdKROu70GYVIgUftmBQ0Ov1Q5ZldT3VxQ7YLcr3jCUXoSa4/TZa8BGt2VRIVtOpeLzLaQ0LcgBgdjDzUdfco38/gsGHw2pCgV3puRATTkXZ5aRg4JJo8GE80VbLZjGpgRsAnDW1HKNHOFFot8DjD2BvHPNJMiXngo9EPPTWLvz6zfCBOkRE2UZsba0Mzn6It+wi+j3E0fTRdrsMVHYRr+sPyOj3BtRP3Xk2C6qDO17GGHa6COICWh9lu+22YLOpsd9DUJtOkxR8lEbY1goo2Qcx4GyLJvshgjGnzYz8YPBhbDi9ZM4oAMC6vS3qzp+BeHwB9XlKIgQfQKjvY1xpnnra77Rq5X3K5tILg48o1u5pxkNv7cZvVu3O+v3SREQi01ARzHzEW3YRZRZxYe/q90U8kbU9WtmlVx98AECLZtR5ns2sbrcdVxY58yF6PkTmwx+Q8eh7+9RdJdujNJsKpcGm06HueGnVzPiIZk5NeNOpCMacVnMo8+H2QZZlNfNx1pRyFDos6Oz3qTNPBiLec5MU2sViJN6Tc0+oUG8TpZds3vHC4COKqx/9UP1vbyB762ZERP6ArB67LoKArv74Mh+i2XRieYF6gYu046Utym4XkfkoK7DBalayJ2LaqElSmky/ecYEXDCrChfMrIq4BpH5aOzqh9vnx8tbj+C+V7bjuj99iCPtfTgcXM+0ysjBR9LKLlHOUdGK1PcRynxY1MxHj9uPjj4vvH4ly1FeZMfC2lIAwAdxzvvQ7nSJ1jLwzTMm4L5LZuDmcyert4ngQ+x4kWUZH+xpTtoslGRg8BGHBHdGERGlVa/mEDdtz0cgjvS+KLtUuhyodilBQKQdLwOVXVxOK5xW5VwT8Wk/z2aBJEk4bVIZVlx9EkqD21CNSvNtcFrNkGVlzsh7u5WLc1OXG8uf/BiAMqzMFWHQFhDfybayLA+4zVXdWRKlxAGE+j62HGxX399Q5sOEfHvwbBePTy25FDkssFvMOG1iGQDgvV3xBh+hE22jcTmtuHbhWN3I+Wma4EOWZfz36ztxzaMf4q6XPo/rddOBwUccZDD6IKLsJUouJik0YTMgx3esu8h8VLkcqCpWApcBMx8RGk5dTpt6ARSZD2fwkLWBaGd91Lf2Yt3eFvV7YltrtJILEOr5aI1wvovPH8DLW4/gkofXovZHr+K93ceiPk9LHGWXqZWFcFhN6HL71IZOcR6O0xYqu8hyaGR8WfDv5MwpIwEAG/a3xjWFNp5gKJJJFQUwmyS093rx6Ht1eHj1XgDA3qbsaUBl8BEPxh5ElMV61RNVLbBbTOrOk3hKL0eD57pUFjlQFSXz4Q/IEU+uBUK7MVxOq3qia3OPyHzEF3wAodLLun0tONzeB4tJwjlTy9XvnxBlpwsQvexyuL0PX/j1u7jpyc34pL4dsgy8FOPE17YBGk4BwGI2YVZw7LoIjMT777Sa4bSa1bkjdcGZHiODGZ+xpfmYWlkIf0DGm8Fx7bHEGjAWi8NqxsTgcLefvrpdvZ1ll2GGsQcRZTMxUyLPZoYkSShyKp++42k6bQg2nFa5nKh2Rc58dGrmXwDRyy55wZKDmvmwxh98iO22z318CIBS3vjppTORHwxgTqh2RX1sqOwSajj1+QP4zj82Y19zD0bkWXHe9EoAwCeH2qM+T6vY1hoj+BBrA0I9FdqeD0mSkB/MAB1o0Wc+AGDZDKXv5fXPG2K+BhD7XJeBaLcli9N5W3s8CU9YTRUGH3HIkr8rIqKIRDAgMg2FDuViNdB2W68/gKZgX0KFy65uiT1qyHy0GUoE2uCjUxt8WJWLbqjnI/HMR2Nw7siiCaWodDnwx6/Mw/KzJmDJtPKojxW7XbSf7B96azc2HWhDod2CF5efhnsumQ4A2N3UHXE3j/J45bUjTRPVEkfZi54Obc8HALXpdH+LPvMBAOfNUIKgd3c3R12HIEozAwVDkYgAaXxZPh69fh4AwOMPDPia6RJ57w7pBBh9EFEW05ZdAKXBERh40NixLjdkWZnxUZZvV3s+jhgyH2HBh7bs0hdqihQ9HiLzoW2CHIjYbissmqDsDDllYhlOCTZqRiN6Ptr7vPD5A/iwrhUrVu8BAPzsSzPVw+yqXQ4c6ejHZ4c71J0nWuLwPHFeTPTXU74vgqx+zVZbAGrTqRp8aDIfkysKML4sXzkMbkcTLjyxOurrtMRRBormypPHwGI24dwTKlBR5IDTakaf14/WHo8anGYSMx9xYPBBRNmsV1N2AaCO+B6o7NIQ7PeoKHLAZJI0u136dOl57Ym2QIyyi8242yXxzAegTO48acyIuB87Is8GSVKy1Me63bj1ma2QZeCqk2t0F3dxSu0nhvHoQmucDZ7i/Bbxc4rgzxH8eUXT6eHg3BJt5kOSJDX78doApRcRxEXbJRSLw2rGtQvHqnNf4tkRlE4MPuLA2IOIspma+Qhe9MS8joHKLtqdLkBoRki/N6A2kgKhzIc4RyTybpdQ5qNZzXwk0POhyXycNKYYjgT6RcwmST2L5Ym1B3C4vQ9lBXbc+cXpuvupwUeEvo8+j18NqkoKBgo+gkPNghdytewVLDuJsovY6VxWqH8+0X/yzo4mNWsSiehhKR1gPfGItSMoExh8xIHBBxFlMzHnIy94wS4KptWNZZc9Td34WDOZU5RXRNDhsJrhCl7Etc2bIhARM0TExbbf60e/VxluVqTJfIjeCWcCZZfiPKvaXHrKhNhllkjEJ/vHPqgDAHz11HFhW31njVYaLz+pD58wKgaM2cwmdR3RiOCjvdcLrz+AfrXhVN/zYby/dh1VLgd6PX68vzv6zA818zGIsotRsgaxJQuDjzhwzgcRZbNQ5sNYdtFnPq599ENc/vt1aqOkGGc+SlPyEBfeXk12Q2Q+qoM9ISLzIZpNTRJQaLeoPR7iE38imQ9JkjBrdDFMEnD21OjNpdGIplO3L4B8mxnXLhgbdp+Zo1yQJGULrvaoeyCUERiRb4UkxT6AtNhpVbNArT0e3Xh1IFR2EbQ9H4Dysy4NZj/e2Ba59CLL8pDKLkYsuwxDYjwuEVE26vEYdrvYRdkllPno9fjQ0NkPX0DGrkblzJQDwYbIcaWhA99EtqDHHR58iDkg4mIrSi5FTitMJilsa20iwQcA/PaqOXjpptMwY1T0bbXRaEeiX3XymIjTUAsdVnX+xVZD6SU0Wn3gC73JJKmvd6zLHer5CP78xp+7NMJziobabVHOX+ly++DxB4KPT0LZRc18DO38m2Rh8BGHX76xM9NLICKKqi9YdhHzJUTmQztkrEVT6xfDr8QcirEloX4Lkb3o84YeKxpOxVZcY/AhSjXGi268E06FkYX2QQUeQKhPw2KS8G+njY96v1Dfh770ok4THWCbrVCqySSEtjor750281GcZ4XNEn6pnViuBEF7m3oijsEXf18FdktC/S/RiKCKmY9h5F+fDTwMhogonVa8swe/fnMXgFDmw6nudgkfMtasOfF1f3MP/AEZ9W3B4KMslPnIi1F2GRUsu3h8Ad3U02jBR34CPR9DVRv8GS6ZM0oNkiKJtuNFHa0e5zRRUUpp7nJrxquH93wY+z2EMSV5sJgk9Hn96pRZ3Xq6k9dsCmgzH9kRfHDOBxHRMNPW48F/v65kZL966ji1B0Nc7AvtYsiYNvgIXXT2t/TgSHsfvH4ZNosJVcFGUkATfGjKLqLhVJRdACX7oR2trjxWf0lJNPMxFNcuHIsqlxPnxBhGBgAniqbTQ+2QZVnt7wgNGIvvYl+qmaraZyi7aIOPkVGCD6vZhHFl+djT1I29Td0YZQiYmpPYbAqw4ZSIiIZIHGgGKJ/YxXh1Z9icD23ZJZT5qGvuUUsuNSOcuuPaRQChPSlX9EOIXTGA0nQ6UOYj0Z6PoXBYzbhgVtWAJYqplUWwmU1o7/WqDbdAaFppeWF8zZ1l6qAxj+5sFwAosIfWUBbj+SaMVLI12r9PIbTNdujNpkCoLNXCrbZERDQY+471qP/d3hvqOci368suXVHKLvWtfahrVi542mZTQJP5CD6nLMu6Md/iAhsp+DBmOtIZfMTLZjGpu3a0k1zFWPdyTRYoFu2U035Dz0c8mQ8AmBBsft0T4bRZESSUHadlFwYfRETDjPaTcmuPN5T5sIohY6GzXcSkUm3ZxeMPYG3w2Hoxelwwll16PH51x19Jnk39fp83FHwU50Upu1izs7Ivpn42aHotGjXTXuNRpskkGLfa6no+CqMHDyL4iJj5ED0fcey+iYcou/R5/bqsVqYw+IjTZs1gHiKiTNJerNp6Q2l/NfMRnHDq8Qfg9inbNbWZDwB4f48y3MqY+XCqZRflOcUuELvFBKfNrJY1tMFHNpRdEiHKR42a4EM9YK8o0bKLWw0+HMGG04I4Gk4BzY4XTSZLaBbnuiQp81Fgt8BmVtaXDaUXBh9xuvThtfj8SPhUPCKidNtrKLv0GuZ85NssEG0cYseLuOCI28U23LGGzEe+mtnwBZ8/eMx8cBeIKK0M17ILEMpuiFKL2+dXyxEVhfFmPpSg4nB7nzoFW818aDJAxgFjWrXBno9jXW71vRRCu12Sk/mQJCmrmk4ZfCTg44PtmV4CEeU4jy+Ag6296p9be7xhp9qaTJL66Vuc7yIyH9OqinTPNzYs86EfMiaaTUOllVBwMmDmwz48yi6i2dRmNqk/50DUk3Q1Z+BEmnAaq+ej0GFVMy3G0ksyR6sL6vkuDD6Gl9gDd4mIUu9gqzKjQ1AyH/pTbYHwk23FHIt5Y0OnxZpNUtgWzzxD2aXdEHyoZRdPQBN8KBe1PEOPR14ShmOlgjijpjF4sF6o2dQ+4Gh1wbgl12Y2wWIWcz5CP3eszAeg6fswNJ22JrnsAmTXiHUGHwngkHUiyrQ9Tfr+gEgTNgHt4XI++PwBdVDYvHEl6n2qix1h0zfzDGUXUZ4Rz+fU9HwY53wYyy7pnPORiEqXEhCIzEdTgs2mgBKEidODlT+H3keX04pRxU6MKnYOmLmI1PfhD8hqxilZDafKc2XPiPXszIllqb+vP4DrFoYfVkRElC77gltk82xm9Hr8ONoR6jnQZj7EhbGzz4vWXg9kWen3mDOmWL2PsdlU+xyi7NId3ElTaAw+PD51iJk4R8VmMcFqluD1yzBJSpNqNioP9nU0dbohy7Jmp0tiF/qyArsanGkDLYvZhNdvPh1S8L9jibTjpS349yVJwIg4y0DxyKYR69n5m5GldjR06cYVExGl295g5mN2cEz4kfbQjg3twW7asktzlzi3xIZql1MNCozNpoDmbJdg2UXMChHBjAhOWnu86sFnIvOhXUOezRJ3CSPdRIbD4w+grdeLRnXAWPyZD0A/g8O4zbjAbtFtuY0mUtlF9HuMyLMNGLwkQu354G6XzIh3gl0kbm8giSshIkqM+IQsejdEb4DTatZNKtWWXcS0zLICO0wmSc14jC0Jz3w41SFj+rKLCD4cwe+LkoXFJKk7ZIDQRThbSy6AkqERJYiGjn4186Gd4BoPbUlksIe/TShX/g4OtPbCE9wWHZrxkbx+DyC7RqznZPDx5NcXDvqxMjs/iCgDmjr7sbuxSw0+5mp6NwB9kyOgL7s0Gw4pO31yGSwmCQtrS8NeRzyPGDJmDD5EZqOhQ5kO6nJadRmO0Hbf7A0+AO122340dSY240PQDhBzWgd3Oa0sciDfZoY/IONgq5LVSvaMDyGbGk5zsudjYnkBRhba1e1VCWHsQURpdqClB2f9YjXEJhdj7wYQnmmIVHYRsyl+dP40fHfJ5IhlAbFjpTes7KLfTtsQvGBrSy7adTjTeKLtYFQU2bHtqBJ8qD0fCZZdtJmPwWZ6JEnChPICbD3UgT1NPZhYXpj0GR9CNo1Yz8nMx1CIDmQiSlxbjwd/WbdfnZpJ8dnd2I1AsAFRkoBlM6pQ5LDqGkyNx9eLKadd/T409+hHdUuSFLUfQR0i5vUjEJDVw+nUsosh81FkCD7EmrJ1wJggSiwNnf1qCSnec10E7aFxQxklb2w6Vc91YdmFhO89/Umml0CUtQKB2KnBx9fux50vfo7H1+5Pz4KOEz3BOR6Lakux96fnY8U1JwEITR0Fwj95Fwe/d6itL5T5iHHOiKAt3/R5/ejuj7zbpa1Xf65LaB36xtRsJcou+5t71NJSwmWX/OjvfyLEdlsxRTvZJ9oKIvjsdvvg9vmT+tyJYvCRoM8Od2Z6CURZacU7ezDn3jexp6kr6n0agkOdmrr6o94n23xS345NBzJ7tpN2gqm2qXREfujCb8x8nDJB6efYdKBNvaiVxTEzwmEJXUR7PX50uZUgQ0ztNF5kjWUXMVjMmaUDxgQRfGw9pLw3eTazbjJpPPSZj8FfThfWKv07a/e2wB+Q1UMAjYPMhqrIaYEl+PuT6exHzgYfQ9kA1trjwWHNUcxEBKzZdQwdfV5sqIt+oRZb1UUqP1PuevEz/NdTn6gnvkbj8wdw7aMf4uo/rg87eyOdxKm1xqbSWJmP6mInTh6vXNR2NCgBYTyZD5NJ0szy8GuGjEXOaIQFH8Ol7BIMPvY1K02eFUWOhLcGa3ejDCXYmjW6GAV2C9p7vdh2pFPt+ShLcsOpJEkYIZpOM7zdNmeDj6E46d43ceoDb6OjlzM/iIT+4JTNthh9UeIC3pWG4KOhoz/ip7tjXW48se4Anv34kHqSaTQ9Hj+63D64fQFsOxLKevr8ATy76RDqNWespJLx7BZBG3xE2l1y8exq3Z/jnZYpgpwej0+z20U/Xl0ICz6Cj83Wc10E4zTTwYxg0GU+htBgazWb1J1H7+05pu5GSXbZBQgFTJne8ZKzwUcyZt/sbe4e+E5EOUJcIGOlc0XmoyvFw/p63D6c++s1uOh/3g/LbuxsCJWFBvr0JwZtAcD2o6HgY82uY/ivpz/B3S99nqQVxyZ6PowBRomu5yD84nf+jCo1zQ7oL5axONVBYh71HBnjVlvBGHwUB895KXYmbzJnKhhneiQyWl0o1BxTP9Qy0+JJZQCA93c3p+RQOaEkS0asZ3doSkTDRl8cwUe6Mh/7W5Qmwq5+JWuh/bS+oyEURLQM8A+wuOgDwDZN8HEk2LsiUvapJmZuGLMJ2mbPSJmPEfk2nDF5JFbtaAIQ/8VMbLcVW1DNJilqOcUYfFyzcAwCsoxrs/woihF5VtjMJnVKa6LNpoBSxigrsOFIRz+ctqF9lj91ohJ8bNzfpq4pFZmPEpZdMuuK+WMAQK2JEtHQiJNVY2Y++sTUzNRmPrQjx42Bzo4kZD7EmSZH2vsgyzJ2NnTh8t+vw4f7Woa07mjiyXxE67G4KFh6KbRb4p7CKUon4rTXAntoVPpAZZcqlxO3njcV1YbTcrONJEko1wQcg8l8AKEAYaiZjwkj81HlcqiBh9UsqX02yZQtsz5yNvj4ztkT8bcbF+CxG+ZneilEx4W+KD0fXn8AOxu64A/ImrJLajMfRzQN4eJgNEGf+Yj9D3CP5rG7G7vhDV4YRAbH7VPOBnlqYz027G/F3f/cNmATq9b7u5vxpYc/0K0pkuiZD03wEaXHYun0Slx0YjVuOnti3OsSgYzIfGh3gQy022U4qdQEHInO+BDGBM/HGTmEYzsAJRgS2Q9A6c9Jxdk44nA5Bh8ZYjGbcNqkMuTbLfjSSaMyvRxKsfrWXt2nWEquQEBGv1ecS6H/R+2Xb+zC0ofexbObDqmnr/Z6/PD5k3NOkizLeGpjPXY1hjIa2uBDm2Xx+QPY1ag9wCt22aVX8zvj8QfUIVCdmp0vR9r7cKBFaTzdfrQTH9a1xr32JzccwMcH2/HyJ0dj3i9q5iNv4MyHw2rGb6+ag38/Y0Lc63Iayi7ao+ONn/C1AdBwU6Hp+6gcZPBx+/nT8Msvn4hzplUMeT2i7wNI/mh1oaSADadZ41eXzx5UvS+BDziUQXXNPTj9v9/B8ic/zvRSjlsi6wGEZz7EBXudoSRhzEgM1vp9rbj1ma344bNb1dsO64KP0Ovsbwkd3gUMXHbpNQSsovTSYQg+xJkcAPD4B/vjXvu+Y8rjBpp7Em23i7bnI5lzNYyZD3FInfZ7wnDOfGjHqQ/mGgAoW5ovmzsa1iScPnvKBG3wkfx+D4Bll6xjztKjn2no9rf0QJah+2RMyaW9SPd6/Oq2WwDqhMzdhuFjySq97AkGNyL7ABgzH6HXMZY3Biy7ePRrFNttO/uNwUfotd/Y1hDXFtxAQEZdswg+Bmh8jTbnQ9PzEc/x7fHKN/R8aDMfA/V8DCeVrtAFvjzBc11SYWShHVMrCwEkf7S6kC0j1hl8BD187dxML4FSxB28EHZmcEjU8U4bbAD6f9jEBXxvk35nSGeSmk4PBS/0LT0eNauhbzgNvc6Oo0oAFJp1EPuiL0p14rPJ9uDjtZmPTw51oN8bgNkkYWFtCQIy8OKWwwOu+3B7H9zB9YpTVaMRwZ0xwCiJMWRsKETZRWRktMGH3WJS3w+b2QTHECZ7ZppoMi1yWJL6/g3FkmD5ZnxZfkqef3xZPm46ayK+tnh8Sp4/XsP3tybJZtcUJ/yY36/Zm1BzGWWG+Ae+y+0b8OwRAjy+QMK/18byhDb4EJmPPkOAkqzMx6G2UJajqasfXn8AjV2Rd7uInS6LgqPHByq7iMBpcnlh8PHhZZd1e5VyUnWxAyePV55XHFQWi3ab7kCZj16150MffDhtZtgtpojfGwpRWvH6xYyPUHZDkiR1hLorz5qSpsh0EQe6pepCPxjfPmci/ufqOfi301ITHFQUOfC9pVNwzYLMboVm8DEEb25r1DWXaWvJlD3Ep3JZBro9mR3rne2au91Y8LO38O1/bE7ocb2G97W1x6MGetF6O5IVfNS3hUoc4nh0beykfX0RPJwW3FUwYMNpcJfJpArlItXcrWRXxJZhIBRojC3JV0sQHX0D/2x1xzSNrz3umA244meI1FQ6eoSypXWouy208gzlnQJHeNADDO+SCwDMGOXC7689Cb++Ynaml6KyW8z44qzqpJbRshGDjyESn7p+9cZOTL7jX9h6qD2zC6Iwbk1QyNJLbJsOtKGt14s3tjUmtBvFuJPovle24cR73sD2o50xgo8klV00mY+GDreu5KJ9na5+r3pfsaWxx9CfYiQyOqNGOGEOTgpt6XFHLBmNKc1Tp3q2xxgxL2gzH7IM9TAxI79mJ1GkC9LvrjoJj1xzUlI/vecZ+joKDcGH6PsY7sEHAJw3owq1wQwIpQ+DjyESxxL/9u09AICfvrI9k8uhCLQXl3ScKZJt+jx+XPPoevzgma0D3lfsTPH4AmozZFyvYbiA72rsRle/D499UBdWkhGS8XfR4/bpSjwNnf26ZlMglDUQDceVRQ6MHuGE1SyCieiBgsjoFNgsaqNeXXNPxJ1uY0vy1ItxPEGu2OkiRNvxos0qRcp8nFBdhGUzqwZ8vUQYd9Voyy5AaGdNto9Qp+zF4GOIjKUWdhRkH7c3NzMfrT0evLvrGB5Zsxcf7GnB/22sHzCbob0gbm+If3dQtADDbonexDeUzEefx4/tRzt1WQ8AaOrsDztxWpygK5pFp1YVQpIk9ZC1WKUXdYur3YKy4NZH8R7ZzCbduSljS/PgyhNll3iCDyXQE88RrelUrMFsktT+jlQzll2MkzbzjpOyC2XO8V1USoOwPg9GH1mn3xe6MGb6KPd0+vGLn+GVrfrhVR193pjzA/Zp+hC2H+3ERSdWR72vVrQBbpFOuDWbJPgD8pAyHw++vgOPfbA/bEBgQ2e/mukYVezE4fY+9XXEgXJTglsZSwtsaOjsjyvzkWczq8ebi+yQK3g2iAh2xpTkq9mU9gGCjz6PXz0fZtZoFz4+2B616bRH0++RruZOY4YlWtmliMEHDRIzH0NkjDUC3P2SdXI18/Hxgbaw22JdFGVZxl5N5mPH0dgjv7WMDaeCsQQChCZJDiUQFD/b85uVLa2iH6OhI1R2EUFGdzDDIppNp1UWAQgNcYq146VHjDW3mTEyeH/xHrmcVozSnF8yplRfdom1s0qUtIrzrOo6o5ddgttsk7ibZSBiq60QVnZh5oOGiMHHEPkN/8Aw9Mg++sxH7gQf4tOpdvJlrEbI1h6PrlywI4GyS18wwDOWBYzNn0Bod0a8ZRdZlsOaVuuD5RYR659QpQQUjZ392B8cNiYu6l39PsiyrP48auZDPd0zRtnFG7rwi+PoRXbI5bSiqlgJpMoKbCiwW9RMQGCAnVX7mpXnqC3LV4dbNUYpu6iZD3v65lAYh5kVGBpdxcTTshSNAKfjX9KDj7vvvhuSJOm+pk6dmuyXyRrGTzec+5F99JmP3Cm7iCzP88tPwazRLgBAe2/0C77YfSEaK4929Me1awMA+kRjpuEi1Rjh0/zoEcpBXPGWXR56azdm3v26emKssckUAOaNGwFACUrqmntgkoBTg6Oqu/p9ONLRj65+HywmSZ3tIIKP/S29+NWbuyJOwO3VlDzE/UWZpchhQZVLCaTGlCg/k8MamrvREeu9DmZPakcWqCerHsuizMdAZZdvnF6L6xeNxQWz4ivLERmlJPMxffp0HD16VP16//33U/EyWeGXb+7S/ZkzrLJPv3ar7SAyH4GAjI/2t6b8GPhkkuXQCbJFDqt6+NeRjn6seGdPxHKI+EQ/vboINSXKRXVnnNkPcYE8b0YlTqgqUkdER4rFRyWY+Vi/rwWyDLyxrRGAfq6HMHesEnyITOTsmmL1dbrdPrWENLG8ALZgcCDKLv/YcBC/XbUb9/xzW9SfS9twKn4ml9OKaVXKzzljlEt9THEcTad7moKZj5GhzEfUng9P9BkfqeIcYLfLjFEu/OTiGWqgSpSolITSFosFlZWVqXjqlCp0WIa8/Y+xR/Zxa7aBDqbn45lNh3Drs1sxY1QRXv724mQuLWXcvoA6nbLIacWI4AXxF6/vREefF79ZtRu77lume4z4ND5hZAHc3gDqW/twbIAhXIIoT4wstOPV/1yMzQfbcOnDayPeN1R2ie//a8eCF+WPDyp9HgeDZZUCu0Utx0wqL0SRw6L2kSyeNFL9tN7t9qlnsoiSCxB+auhH+1vh9vl1O3R6NafJlhmGeLmcVnxxVjXKCx1qZknc3tjpjhl8fHakA4BSLhoRDAy1u112NnThu/+3Bd9dMkkddJbOoVPGOR/GjBbRUKUk87F7925UV1ejtrYW11xzDQ4ePBj1vm63G52dnbqvTLlyfs2gHvfw6j3qf7Pskn2Gmvl4elM9AOCzw5n73UyUCLJMknLhLHbqP417fAHsN8zx2KuWAvIxIl+5f1uM0oFWvyfUmAmENyJqGzNHFycWfIiMwOeHO+H2+dV+j9Mnl2HxpDLMGu3C+LJ8VGqORz99cpnugvlJcPjf5ApN8GH41O72BbD5YLvutp7gz+XU7HYRipxWmE0SFk0o1QUGLmfszEe326c2nM4c5VLPF2nudqtl3Nc/b8D2o514dtOhjGQ+tP0lBXaL2tBLlCxJDz4WLFiAxx9/HK+99hoeeeQR1NXVYfHixejqipy+vf/+++FyudSvmprBBQCZ9OBrO9X/ZuyRffp1mY/EM1u+YVhLExmAQody9kZxXnh6/B8f6T8UiLLLhJEF6qfxtjhPvhTlCWeUyZcTypU+C6tZQlVx/GWXXo9PzW54/AF8fqRTPTG2piQPf71xAV666TTYLCb1Il7osODE0cVwWM2wBY85//SwkmnQTgEdW6r0aRTnWbF4ktIfsjZ4TgsA+PwBdSt9vs2i7nYRou30cDmV9y5a8PH54Q7IMlDtcqC0wI6yAhskSfk9aw322IiR7U1d7oz0fNjMJjXgMPZ7ECVD0oOPZcuW4ctf/jJmzZqFpUuX4tVXX0V7ezueeuqpiPe/7bbb0NHRoX7V19cne0lpJbPwknXcQ8x8GHc0DQfi5xQXDlF20Xp20yH1k7bXH1CPhVcyH8HgI86GU1F2Eb0CxvkPE0cW4JZzJ+OuC6erF+0ej3/A99Y4eGvzwXYcCvZ81AQbVwWxhffUCWWwBIMOcSaJ2EkyrjQUfEwsL8Sfrp+HZ755Cs4PTghdrwk+ejVBq9NmRkm+DdoxG9FmXLjUEeuRf9dEICT6RCxmk5qFaRRBR/B/Gzv7M7LbRXt4HEsulAop32pbXFyMyZMnY8+ePRG/b7fbUVRUpPvKlGQM8NFmPjy+ALbUtw/Li9fxxD3E8eo+f/b+/cmyjF+9sTPsCHdRdhFbIiNlPpq7PeqE0IOtvfAFZOTZzKgscqjBiriAHmnvwwP/2hGxURUIL7tYzSbka8oEBQ4LvnPOJFy7cKzuk3T3AH8fxibMjw+2ob5VWUNNiT74uGTOKIwvy8eNmqPCjZ/ax5XpH3POtApMLC/AKcFTbjfXt6kD00SvhZgsajGb1IwQEHpvjQYqu3wWDD5mappURxqaTkWw1NTlVjM/6cx8AKFgh5kPSoWUBx/d3d3Yu3cvqqqSe/ZAKowx/GM2GNrg4+antuCSFR9gxTuRAy9Kj+M587H3WA9++/Ye3PH8Z7p+oy617KJcOIrzIvdgfB5sfBTNpuPL8nVlGrGl9a/rD+D3a/biibX7I66j16u8nnamiLYsUaD51G41m9R1He2MHMwIYvCW2KGy+UCbutulZoRTd99TJ5bhne+difnjStTbtBfO8kJ72JklwpiSPFS7HPD6ZWw8oJxUrZ1uKj6YaPs+opVdIu12+eu6/Zh19+v4YE9zKPOhaVKtENttg0GHKLv4A7JaZkpn5gMIne9i3OlClAxJDz6+973vYc2aNdi/fz/Wrl2LSy+9FGazGVdddVWyXyrprpxfg5vOmoibl0zGmu+fOajnEAdsrdl1TB1t/eh7+5K1RBqEfsNul0Sbgn2B+E93HSpvAifJAqEBWV2G2RfqNtvgBVL7id1uMeHUicon/W3BLahiZLg43bMkeH8x56M5+Ik80jZXQNPzocl2FOmCD/0FTEwZFbtQohE7XU6dUAqb2YQjHf3o9fghSaEtu7FoSwbjYpz6KkkSFgazHx/VieBDn80BoG63BYAiZ+RAxni4XL/Xj1+/tRud/T7c/vyn6jwVbeajPLiTprGzHz5/AM2aXUaiOTXdmQ8RSDLzQamQ9ODj0KFDuOqqqzBlyhRcfvnlKC0txfr16zFy5Mhkv1TSWcwmfG/pFPznkkkYW5qv+xQXr7rmHvz8tR24/s8b1NusZg6SzSRt5iMgh3YwxCtdmY83Pm/A9LteDyuhxKLdjVKvOWQtVuajutiJ6dXKhU9c/EWzaW3wAi12u4gGSBHMHO2IPAirL8KFWpsZME7MPKE6vuBDlCHGlubjP5dMUm+vLHLEPLRO0H5qH18aPfgAgBnB92RXo/JeRGr01AYf0RtOgyWrPuW9e3HLYTUw3N/SC1kGqlwO3XONKlayrvVtvTjW7dZlUMXfazp3uwChvzNmPigVkh7Srly5MtlPmTGDbR59ZPVe3Z8ZfGRWv+G4984+b0JNdP4UbWGqb+1Fvj10VPuGulZ4fAF8WNeKi2ePGuDRio6+ULajvrUXs2uKAcTu+ahyOUIX/6Mi+AhtswVCmZL2Hm/wdZT/bYgWfHhjBx/GT89iHPq2Ac6PEQ2n5UV2fPP0CVi/rwXv7W6Ou0Sqfd1YmQ8AmFShZH12Nyk789QtrprAqTSOsou250OWZTz2wX4Ays8sfl7tUDJlbcrPs7+5N2zMugh+0znnA9A0DzPzQSnAq2IacAdMZrkNJw8n2veRiqpLU1c/Fj/4Ds791Rr1NnGxS6QpVpv50B4vH5puGrqAiK2TVS6nOoH0aEc/Wns8ailAjB4XwUeX2wevP6BuURZlASORJXBE7fnQX6i1wU+sMpjo+RhZYIfJJOGhK2bj2oVjcMu5k6M+RqtQW3YpjR2wTCpX3pP9Lb3w+AJqw2meNTzzocxPiVJ20fR8rNvbgh0NXcizmfHXG09W1zDLGHwEszL7W3rUHS9G6c58cLcLpRKDjzSIdmBUoupbe6OeHkqRrd7ZpH5yFNsZE531kYqej3XBLZ0tPR64gwffiaCjO4HgqF1Xdgn1Y4jnEn0XkiSpwUB1sQOFDqt6Idx8sE0tC4jtq0VOq7qttK03dOBcQEbY1FN/QFbnYWgbOmOVXSZVFMBiktDe61WPlo9E9HyUB7fRlhbYcd8lM7GgtjTqY7S0JYOBMh8VRXYU2i3wB2Tsb+kJNZxq1i5mfRQ5rTBFGbyl3Wr79KZDAIDLThqN0gI7VlxzEq5dOAbXLhyre4xYW1OXW+3xMEp35mNaMDslAkWiZGLwEUM2DQzb2dCFxQ++g7N/sWbgOxMAZUjUDY99pP55ZLCpL9ER69qej1jHpCdC2zvRHDzSXWypTCTzoT34rb61Fx29Xlz3pw/x4pYjAPTbQUXfhzgMTVzwtgdLASYpVKYwmyR1Kmp7r1eXLTKeVKvd1aH9dK7tMyk0ZD7sFjMmBgePafs+PtjTjPN/8x4+PaTsCBE9H+WG0ebxKtCWXQbo+ZAkCRNF6aWxO3LPR6ESwMY6Sl58r6vfh7V7mwEAy2Yox01Mr3bhvktmqnNUtI8R5TdxgJ6x5yzdmY/vnDMRG350Ds6ZVpHW16XcwOAjhlTFHrIs47erdqu7YeLxxucNAEJb8Ghgxou4SJknWnbRTjj1JLgbJRrtgW3i070YJmU8Pj6WdkPZ5c8f1OG93c3qbdqeB1FSEZ9kRWlFHEHvMnyaF99v7nbr3kvR9+HxBbD8yY9x0r1vAgAkCeqJruL5BGPmQ7sObfCx8qN6bDvaid+v2QuPL6BmZAYbfIifv7LIoduJE82kYEC0q7Er4g6eeeNKcOJoFy6fF30Ss/bnbux0w2qWMGfMiAFfW0xc3bhfOcNmpqE0k+7dLpIkqRknomRj8JEm2lHSa/e24Fdv7sLyJz+O+/FezQXwjhc+TerajlfGIMO4BTJefs2QMWP/SDwaO/vx8tYjul6J7ZpGS7GNVVzg48189Hv96o4KADjc1he2VVe73fUXXz4RLy4/VW1KFZkJccaLcRCZ+HQu5kwIRzv6IMsybnryY10AbbeYdIP6dFttIzQtiqZT7XtxoEVZyzs7m9Sj6y0mSbdVOBEVweFdopl0IKLvY09Tt+5QOaHIYcWLN52G5WdNjPocxgFrs0YXxxX4iN04XcHgU3tYHZD+OR9EqcTgI5Ykpj5+9Pxn6j9m+1tCNd07Xvg0rrkTfk3fwd/Whx/Ud6zLrZvzMBztaerGb1ftTuiTfyzacsA5U8vVuQydCU451WY7PFGCD58/gP9cuTniTJfzHnoXNz25Gc99fFh9DjFXAwj1UITKLgMHRw+v3oPpd72O9ftadesUu1YEbebD5bTixGDgAQDFzvDMh5aYciq+Lxzt6McnhzrwxrZG9ewUAOj36t8b8XxWsxRxW+zY4MX2sGZq6oHga/V6/Hj+Y6VfoizYbDoYZ0wZiXsvmYGfXDQ9rvtrd7z0uPUj4xOhfS9PHl8S454hxp6UmaMzm/kgSiUGH7Ek8SDHf35yBCfc+TqaukJnNQBKIHHni5/jgz3NMR4de8R3v9eP+T99Cyfd+2ZWT+McyLLfvItfvbkLv3h9Z9T7tPd68PzmQ+j1+HCsy40bHtuAd3Y2RbyvaCydVF6AP90wX+1/iOfiLgQCsi7b0eP24X/e3o1djfqDEl//vBEvbjmC+17ZHvYcYkfKumAtf19zt3rcPRC57DJQQPrurmMR/643BY+dF6KNAAdCszzEQCvjFFSRbTjQog9oGjr61cmoCyeU4jpD86QgLsDRGiWri5WsxNEOJfho1zS2AspUVUDZZjtYVrMJ1y0cqw5PG8ik4Km3dc09auYsfxC9Fi5NpmZBnMHHWMNunGlVRbCaQ/8IDWbuEFG2YigdwykTSrF657GkPudrnzWEnWfx1/UH8Nf1B7D/gQuiPi7WyaraaYj9Xn/cXfHHutzY3diFRRNKk3KuzVCJC/JH+1uj3ufrf9mIj/a34Yp5rShwWLB65zGs3nkMdfefH/YziIuHuKgWqz0M8WWIjnb0qYGBcNdLn2PNrmN4fO1+bLzjXPX2aGeeaAPNquCR71sMx7Yf63JDlmU18xGQlU/+sf4exfkmwpiSPBxs7Q1bb6zplMZMR1jmI1h22d8cXnYRfRrTqgpxy7mTUeCwYI4mq6J8rwgnjnZh7tjIF9/qYONrc7cH/V6/mvUQRND25bmjo/4MyVbtciDfZkaPx6+Wg/IGscvEFcyymSRg7tiB+z0A/Ym7AFDpcqC80IHD7X3Is5kHnf0hykYMPmJIxXCw5m4P2qP0HAQCctR/YCLNVogk1kCsDXWtuOnJj/GTi6Zj2cwqnP2L1ehy+/CH6+biC9Mr43r+dLDE+Ef2o2Az3vNbDuOaBWPU23c3dWNy8FOrIHo7xEVVfLI0fpKPZEdDJ8576L2w29fsUoJRYwCjzaaIE2L7PH7djilRetgQDK5cTis6+rxo7nbD7QvosiHdbl/U4MPjC+BIhz74mDnKpZ5KK9SOzI94oJxg7KMoNgQfImgT75fdYoLbF9Dt1Dmhqgh2ixk/OG9q2PM7rGa8eNNpUV+/OM8Kp9WMPq8fDR39OBBc/5wxxWjo6EdTlxv3XTIDV508JupzJJskSZhYXoBPDnWoTcGD2WUifudmjHLFPSF0rGY3jtNqRqHdgooiOw6396V9my1RqrHsEkMqPmf8dtVu/GXdgYjfq/3Rq2HNfYIx8xHtDBBvjIbIGx//CE1dbvzH35VGV9HYFq1skQxH2vvCJowOxBzHJzyTpO+/iLRzqMMw5VNMxTR+wo5k+wCTNwElIBQzMrTlgh63D1f87zp86ZG12FIfKoOInp8NwbNDxDHux7rcugwJELs0dKS9L2wbuBgaJrzzvTPxr/9cHPO9NAYfLsOfy/KVcocYRz812CDa0NmPz4OZD9E0OhiSJKEqWHo50t6Hg8Egp7asAC8uPxWrbjkjrYGHILaWiv/PDSb4KA3urDp5XHwlF0C/3baiyA5JklAR3G0ymNIPUTZj8BGnS+eMwv4HLtBNTEyFB6P0Oxh7PrQXdO33vDF6Q3qjBAGpmmfywZ5mnPLA27j9+c8SepwljoyTBAltmhkXr34aHnwYD1cTmY+WHs+A223jGUT21MZDuPThtfj6XzaiuUc7b6MPzd0eeHwBrA0OEwOUUsrRjj4cauuD2SRh6XTlInes2x3WZBtrx4sxwwGEHy9f5Rr47BNjj4cx8zF+pL4MMKEsHyeNKYYsK7t+bBZTWKkgUeJ03SMd/Wpj67jSPJQXOQYcCpYq1y0cq+uvGEyj51dPGYerF4zB1xbXJvQ4MfhNBB3if6Odxks0XDH4iNOX5wXrzikuu3b2efHkhwdx23OfYtwPX8F5D72L5m53WObjxic24uev7QCgn8CpzYgYMw6JnuY6VLc8tQUA8Gxw10IsfZrD3sxx9J+YJKBFU/rY3dSN3YYmUBFAiBHjhQ6reiT6wQGyH/Fsx73/VaW5dNOBNhzSBAR7joXWselAKPPR4/Gpf55eXaQOvWrucocFGyIY+cO7e3H2L1fj2U2H1L+/yMFH6ITXIodFN+Y8mrDgw/DnWsPFv8hpxWWa/ospFYVxBYqxiD4YJfOh/FxjBhiDnmoj8m24Yn5ojsdgMh+TKgrxs0tnotKV2JwMEXAZg49Ic1KIhjMGHzFor4GnTCgDAJhS3Ji5Ztcx/Oj5T/GPDcp22h0NXfjF6zvDLuAb6lrVA+y8usyHEnz8btVuTP3xa+q0REBpZIxksDHJu7uO4a/r9kf9fiJj5bXzKgJxLMgk6TMfgPJeaRkzH0Corr5/gL6PeAaRdWmyFZ8EJ3ICypZhoUnTANrn8auNopPKC9WJqz0ef1ijqAhG/rGhHvuO9eC/nv5EnVoa6Vh7MRYdCF2wBlJgt+j6a4zBR0m+TdeEWuS04ouzqmELDhIbSslFqA5mPo529OFAq/J3MnaASaTpcONp49WSVaQZJakimlOnBwewjQ8eOFdeyGFfdHxh8JEgMQExnVZ+VB/1e/6ArCu7iAzJL9/cBQC488XPB3z+RA++e3NbI3Y3duErf96AH7/4ObbUt4c/Z4QAoqmzH5f/7zq89MkR3e0t3W6s0ewq6onn/BoJ6lwTkbo39k2oPR/a4KMkNEVSG5gZJXr+i5Y2+NDq8fjVQ8PKi+zIt1vUWRrG91DsiNKeIiuaXSP1BY0stKvTRePdmipJki7gcDltYd+v1ZReihwWuJxWfHGW0qsyP84tpLGIHS97m3rUYHWgA+DSoaYkD3dfNB1Xzq/BjGrXwA9Ikqvmj8Fbt5yBrwfLNUumVeCXXz4RP7pgWtrWQJQODD5ikCLUWO688IQMrCS6Pq8/riFYRqLpMZZ3djbhkOFT9rObDuHrf9moOzMl0jHrxkmggYCMn7+2ExvqWvGdf2zWfW/pQ+/hh8+FprYatyJH0tXvU3eciJJDt9uHox2hBlfjsfIAMGqEct/H1+7HFX9Yj7+tj9z8GyvzEWv7KhA9+OgLziYBQuPCZ44uBgD1DBDt6/e4fepR9UBoJLsou9x94Qk4bWIZHvvqfEiShNHBny2RT8na3TCRziuZoJmPIb7/00tm4i//djK+NGdU3K8Tjch8iB1ARQ5LzB066XTdwrF44LJZad3iajIpu23Ea1rMJlw2d7QaYBMdLxh8JGhW8GKRLfo8ft023Ld36HeuRKsSXf6/69T/jlTlWL2zCV997COc9vN3ACg7O9bubcb9/1L6TLRTKSPtqND2cABAt8eH1p7IZZhmwympiU44FbtYXthyGKf9/B3c/H9b4PUH1E/SYrIpEDpUTbjjhc/UU2W1RPDxnXMmqX0iwnnTK+GwKv/XMe4yAcInggq9msyHKI3MDk6xFFuIhd+8tRt//1AfGO051g1/QMbhNuW9X1Bbir99bQHOmlIOINR0msg5KNomU2PZBYA+8xG8r9NmxumTRybloix2uwinTSob8nMSUfZj8BFDtAt3ItvnUq3f68f7mumov3pzF3Y0hG8TPdoReQhWNNqx3QDwxLoDuPqPH4YFCoB+9Ltg3FnT1e+D2RTfr1tXv0+3dVWI1jArhmF9drgT/oCMf33WgF+8vhOH2/tgs5jU8zqA8IsdAGw2DP0CQmWXOTXF+Oj2JVgyrVz93vzxJdj84y9g0x1L8K//XIyfXzYT588MzUmJNmW21+MPO6U1WjDb5fbhZ68qgd6EkflwWs3w+ALY2dClDt+qMjQzzg0eXmYcyx3LQJmP2rLwzEcyVRuCwX87dXzSX4OIsg+Dj0G46eyJiPShb8e956V9LX1eP3739h7dbdrsx46GLnx35eaIY7+FSJdKYx/I42vroj5enIGhW5fHGHx4Ee/GCLcvgBN/8gZWvLNHF3BEOlF23tgREbc/iy2uP7loutrYCYRf7ADg/n/twMX/8z5ufeYT9fVCzaoWSJKkNlkCSmnAaTOjtECZxXDF/DF4+Jq5uHh2dcyfq8ftC8t8zKoZOFCoLnaqZ468t1vp+7BbTGHBwPKzJuL9H5yFL86KvQ4tke0osFsiDtWboOv5SH7woT1wzW4xxT0NlIiGNwYfMUTLfJw+eSQ+vXspFhtSxA6rGTfFOO0yFbZqdlkID76mnxXywpYjWL83enNlPLtdxsXYgRCpQdQYfPz8Xzt05RlRKoo1ufW/X9+JC377vrqDx/icAPCHr8xDQYTgQ/RdGHdkRMp8fFLfjk8OdeCpjYfU81eM/SLaA9SiTayMNotBbPVt6nKrvTAiICovdKBak8HQnuUhVBY51Omt7+1WslyVLkfYOHmTScLoEYk1a4qG12hZDe2210jvczKIo+Pv+OIJWTHmn4hSj8HHIOXb9bMU3v3+WQCAW86djDXfPzNt6/je05/Edb+E/003BCSxRp6/vPUo5t33lm5Sap+h7PLOzmNo6wmVUjr7ffD5A7hN02gaybajnfiorhXL//4xvvTIWt33asvyUZJvQ0GEYEC8vnHnx0Cf3n+7ajfcPr968q3oc9BmPqI1nBZEmcVgDAhcTqvud+f/aWZnRMo+VLkcmBIMPkRjZrzbaQciyi6R+j0AZST8g5fNwveXTknZ/I3/uXoO/nDdXFy7IP3TTIkoMxh8xBBpt4vWnV88AbUj8/HTS2eo/zCbTFJWzCkIF/1nESWWAy09apbBmAxp7fEgmg11rWjuduOrmh0wxuAD0DepdvR58dzmw3h608ADyG5+agte+fRo1OPio30ilyTlOPZ4TK4ogNUsYf2+VnztiY1q34aa+bAMPvMhdtgIxobQG08LTcH88tzRWFRbqmtkrXSFyi5iN1Nl0oKP2JkPALh8fg2WpzCjN7Y0H1+YXsmsB1EO4czeIagpycPb/3VmppcRF1+EplDhuY8P44KZVbjxiY2YXVMMSdJPAH3pkyO6IVrx6ItQitFO5uzo86KuOXzQ18hCe9jArWjDysRhW9EyESV5tpiHA06uKMCuRqU8s2xGFeaMKcZXH/9ILW1YTJK6q8ViGjjzoZ1C6bCa0O9V3nPjNkljNsaVZ8WvrzgRj75Xh68trkVNSR5W72xStzNXuRyYYthVUzGEY+a1zpg8EjNHuUITfImI0oCZjxhKCwY/b+CiE2M3/Rm3b8bjwx+do14ME9UboV9C68YnNgJQhl1tPtiOFk2m49Zn4ivtaKlljyjbPjv6vGEzSU4eVxK2gyMWUbqIduLnyCivvfIbC3HJ7Gr8/tq56m3jy/Jx5pRynDKhVL2tyGlVP41rm12jBR/azMcUzQm75UV2XeakIsIcjkvnjMYr31msbpedWhnqVSnJt6GyyKF73WSVXUaPyMM/v30aLp3D4IOI0ofBRwy3nDsZZ08tx4qrT0r4sb+5cnbMSY1XJ3ha56r/OgMVRQ7103Si4h0+FokzjnNChKc21uO1zxrUYOfEmmKce0JF2P1+/q8d+NP7oR00BXYLVn5jYUJNjaIPJdpjyqNcoBfWluKhK+egVjNAa944ZZfFxSeGBmdp21zcmvc92oFt2syHNngoclh154NUxzEwqqLIjtqR+RiRZ8WkigJIkqQ2nQJI+MwQIqJswrJLDMV5Nvz5hvmDeqwkSZg7tiTqwCmL2YSKIrtaUpg7doTuEDIj8Uk3Ulki1cRcCaPakflhfRi3PrMVANSAI89mhiPCxXqb4cj6gCzDZJJ0gcSlc0bh+c2Ho67LGswmaDMC2jWVRGmi1Hr3+2eho8+rNoVeNLsaf/vwALYe6sA0zU6ZSIPIjLSnn2rLJBaThHybBe3B97F25MA9QZIk4dXvLIbHH1AzKpMrCtXfkWT1fBARZQIzHyl05xdPwI2njccDX5qp3iaGVf2/uaN1/Qi/vnw2zp9Ziee/dUrE53IEL7R/vn4+FiThTI3B0M6x2HD7OfjxBdFHzb+5rRGAkjURQ8BiEVtQtYd4aS/geTYzZhmGZ4ntr9qARTvyO57SxJjSPN1QLofVjBe+dSqe/NoC/Ory2WHri0Vb/tEecW82Sbp5FtqMSywOq1m3O2dKRehxySq7EBFlAoOPFHLlWfHjL56AS08ahUKHBbVl+fjjV+Zh+z3nobrYqZsdMaY0Dw9fMxdzxkQesiSOLp852oX/+/dF+N/r5uq+f0HwsK+BRBqFHq+fXDQdt543Ba9/93SUFzpwxuSR+N1Vc2I+xmkzoyR/4AyE2F2iHRh2kua9kAA8ev08XQlHzMTQljQmVRRi5TcWYun0ClyZYGlLMJkknDKxTNczEk/woV1HlcuBc6aWoyTfhi9Mr9Sd1BtP5iOSyZX6PhIiouGKwUca2C1mbLxjCd685QxIUuhTsBhSNpjhTUunV+LHXwxlHi6O0OD68rdP0/3ZYpIGbISNpTjPhm+dOVHNSJhMEi48sRqf/2Rp1Mc4rWaM0IzwzrOZMarYibICG752Wvgobe02Vm2zao/Hj/JCB/74lXmhnycYkGm3aE4qL8DC2lL873XzML4seVuerXEEbdrMR3WxE3/8yjys/eHZcDmtaNQcvjfYSaEzR7kwIs+KE0e7ovadEBENB+z5SJNIF4sfLJuKmpK8iA2ZwuJJZThzSnnE72lnQWh3UwgnVBXBapbg9SufumUA06uL1D6KC2ZW4fL5Nbj+zxsGXL8txpbVfLsFNSVO1Lf2qWsW21VL8m2YM6ZYve+0qiL88Svz4A/IGFlox6Pv68e252maNqPtYhG0AcHL3z4Nzd3uuEsaibrjiydg77Fu/PsZE6LepzTfBklSZmaMyFN2yjhMys/TM8Buo3gUOqx4/wdnx9w+TEQ0HPBfsQzKs1nwtcW1UYeS2Swm/PXGBbgxQoYA0F+cbWYT7r1khu77JlMo8ACUQEC7A+T+y2bijMkj8afrQ9mEaOeTXDhAxuSei2fAJAHLZlRi6fTQIWvzx5XojmUfPcKJknybWtIQPTDif7WTVPPtZtxy7mQAwNWa6ZciI7J0Ruh1ZoxyRQ3SkmF8WT5Wf/8sXBWjlFNaYMcfrpuHP10/L2UDs/LtloiBJhHRcMLMxzCmHedttZhw3cKxeGdHk+5gOe2wqy/NGYUKTSlD9FecM60CP79sJjbUteGBy2bixS1Hwl7rPkNgY3TWlHKsu+0cjMiz4bXPG9Tbp1cXQZIk/ObK2fjT+3X43hem6B73y8tn41+fHsWyGUrPinaqrNNqxrfOnID540p02ZM3bj4d+1t6MbumGNkmWhbryvk1WPlRPb6/dErE7xMR5RIGH1lobGkeDrT04vRJI2Per8Ae6h0QqXjj8LK/3rgANz7+EU6dWIabz50Mm9mEC2ZVYfQIp+7T+RXzx+CK+cqn+jduPh1f+PW76vfmjh2h260RjdiBcd70Slx0YjXmjy9R+zIunj0KF88eFfYYl9OqawzVJgwkSYLFLGGRZvAXoPSezM4b/AC4TPjJxdPx5XmjMbuGp7YSETH4yEJPfn0hnv/4EK5ZMDbm/bRDrcRuinnjSvDUxtB5KfPHlWDr3fqG0IGGpk2uKMTr3z0dSx9SApARcczL0LJZTPjtALtgYj32eGS3mDF3bGa2SBMRZRsGH1loVLETN509acD7aYdaiQmm/++k0YAMzB03tE/YUyoLcfeFJ+DR9+tw5xenD+m5EnHpnFF47IP9OD24E4iIiI4/DD6GMZOmOVM0YZpMEi6fX5OU57/h1PG44dTIza6pUuiw4u3/OoMnnBIRHccYfAxzT39zEZo6U7fFNBMYeBARHd8YfAxz88exj4CIiIaX47O7j4iIiLIWgw8iIiJKKwYfRERElFYMPoiIiCitGHwQERFRWjH4ICIiorRi8EFERERpxeCDiIiI0orBBxEREaUVgw8iIiJKKwYfRERElFYMPoiIiCitGHwQERFRWmXdqbayLAMAOjs7M7wSIiIiipe4bovreCxZF3x0dXUBAGpqajK8EiIiIkpUV1cXXC5XzPtIcjwhShoFAgEcOXIEhYWFkCQpac/b2dmJmpoa1NfXo6ioKGnPS+H4XqcH3+f04PucPnyv0yNV77Msy+jq6kJ1dTVMpthdHVmX+TCZTBg9enTKnr+oqIi/1GnC9zo9+D6nB9/n9OF7nR6peJ8HyngIbDglIiKitGLwQURERGmVM8GH3W7HXXfdBbvdnumlHPf4XqcH3+f04PucPnyv0yMb3uesazglIiKi41vOZD6IiIgoOzD4ICIiorRi8EFERERpxeCDiIiI0uq4Cj5WrFiBcePGweFwYMGCBdiwYUPM+z/99NOYOnUqHA4HZs6ciVdffTVNKx3+Enmv//jHP2Lx4sUYMWIERowYgSVLlgz4d0OKRH+nhZUrV0KSJFxyySWpXeBxItH3ub29HcuXL0dVVRXsdjsmT57Mfz/ikOj7/NBDD2HKlClwOp2oqanBzTffjP7+/jStdnh69913ceGFF6K6uhqSJOGFF14Y8DGrV6/GSSedBLvdjokTJ+Lxxx9P+TohHydWrlwp22w2+c9//rP8+eefy1//+tfl4uJiubGxMeL9P/jgA9lsNssPPvigvG3bNvmOO+6QrVar/Omnn6Z55cNPou/11VdfLa9YsULevHmzvH37dvmGG26QXS6XfOjQoTSvfHhJ9H0W6urq5FGjRsmLFy+WL7744vQsdhhL9H12u93yvHnz5PPPP19+//335bq6Onn16tXyli1b0rzy4SXR9/nvf/+7bLfb5b///e9yXV2d/Prrr8tVVVXyzTffnOaVDy+vvvqqfPvtt8vPPfecDEB+/vnnY95/3759cl5ennzLLbfI27Ztk3/3u9/JZrNZfu2111K6zuMm+Dj55JPl5cuXq3/2+/1ydXW1fP/990e8/+WXXy5fcMEFutsWLFgg//u//3tK13k8SPS9NvL5fHJhYaH8xBNPpGqJx4XBvM8+n08+5ZRT5EcffVS+/vrrGXzEIdH3+ZFHHpFra2tlj8eTriUeFxJ9n5cvXy6fffbZuttuueUW+dRTT03pOo8n8QQft956qzx9+nTdbVdccYW8dOnSFK5Mlo+LsovH48GmTZuwZMkS9TaTyYQlS5Zg3bp1ER+zbt063f0BYOnSpVHvT4rBvNdGvb298Hq9KCkpSdUyh73Bvs/33HMPysvLceONN6ZjmcPeYN7nl156CYsWLcLy5ctRUVGBGTNm4Gc/+xn8fn+6lj3sDOZ9PuWUU7Bp0ya1NLNv3z68+uqrOP/889Oy5lyRqWth1h0sNxjNzc3w+/2oqKjQ3V5RUYEdO3ZEfExDQ0PE+zc0NKRsnceDwbzXRj/4wQ9QXV0d9gtPIYN5n99//3386U9/wpYtW9KwwuPDYN7nffv24e2338Y111yDV199FXv27MG3vvUteL1e3HXXXelY9rAzmPf56quvRnNzM0477TTIsgyfz4dvfvOb+NGPfpSOJeeMaNfCzs5O9PX1wel0puR1j4vMBw0fDzzwAFauXInnn38eDocj08s5bnR1deG6667DH//4R5SVlWV6Oce1QCCA8vJy/OEPf8DcuXNxxRVX4Pbbb8fvf//7TC/tuLJ69Wr87Gc/w8MPP4yPP/4Yzz33HF555RXce++9mV4aJcFxkfkoKyuD2WxGY2Oj7vbGxkZUVlZGfExlZWVC9yfFYN5r4Re/+AUeeOABvPXWW5g1a1YqlznsJfo+7927F/v378eFF16o3hYIBAAAFosFO3fuxIQJE1K76GFoML/PVVVVsFqtMJvN6m3Tpk1DQ0MDPB4PbDZbStc8HA3mff7xj3+M6667Dl/72tcAADNnzkRPTw++8Y1v4Pbbb4fJxM/OyRDtWlhUVJSyrAdwnGQ+bDYb5s6di1WrVqm3BQIBrFq1CosWLYr4mEWLFunuDwBvvvlm1PuTYjDvNQA8+OCDuPfee/Haa69h3rx56VjqsJbo+zx16lR8+umn2LJli/p10UUX4ayzzsKWLVtQU1OTzuUPG4P5fT711FOxZ88eNbgDgF27dqGqqoqBRxSDeZ97e3vDAgwR8Mk8kixpMnYtTGk7axqtXLlSttvt8uOPPy5v27ZN/sY3viEXFxfLDQ0NsizL8nXXXSf/8Ic/VO//wQcfyBaLRf7FL34hb9++Xb7rrru41TZOib7XDzzwgGyz2eRnnnlGPnr0qPrV1dWVqR9hWEj0fTbibpf4JPo+Hzx4UC4sLJRvuukmeefOnfLLL78sl5eXy/fdd1+mfoRhIdH3+a677pILCwvlf/zjH/K+ffvkN954Q54wYYJ8+eWXZ+pHGBa6urrkzZs3y5s3b5YByL/61a/kzZs3ywcOHJBlWZZ/+MMfytddd516f7HV9vvf/768fft2ecWKFdxqm6jf/e538pgxY2SbzSaffPLJ8vr169XvnXHGGfL111+vu/9TTz0lT548WbbZbPL06dPlV155Jc0rHr4Sea/Hjh0rAwj7uuuuu9K/8GEm0d9pLQYf8Uv0fV67dq28YMEC2W63y7W1tfJPf/pT2efzpXnVw08i77PX65XvvvtuecKECbLD4ZBramrkb33rW3JbW1v6Fz6MvPPOOxH/vRXv7fXXXy+fccYZYY+ZPXu2bLPZ5NraWvmxxx5L+TolWWb+ioiIiNLnuOj5ICIiouGDwQcRERGlFYMPIiIiSisGH0RERJRWDD6IiIgorRh8EBERUVox+CAiIqK0YvBBREREacXgg4iIiNKKwQcRERGlFYMPIiIiSisGH0RERJRW/x8G2ZB7OeU0KwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.15 seems to be a good candidate. With a single LR, it's a good idea to choose it a little lower such that we can squeeze out performance at later epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 2.36897349357605\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(words, string_to_integer)\n",
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "train(\n",
    "    num_steps=20000, learning_rate=0.1, X=X, Y=Y, parameters=parameters, verbose=False\n",
    ")\n",
    "\n",
    "# We see different results if we keep rerunning the cell, as the mini-batch sampling was\n",
    "# not seeded.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually add a proper LR scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    def __init__(\n",
    "        self, initial_lr: float, decay_steps: list[int], decay_factor: float = 0.1\n",
    "    ) -> None:\n",
    "        self._decay_steps = decay_steps\n",
    "        self._decay_factor = decay_factor\n",
    "        self._counter = -1\n",
    "        self._lr = initial_lr\n",
    "\n",
    "    def step(self) -> float:\n",
    "        self._counter += 1\n",
    "\n",
    "        if self._counter in self._decay_steps:\n",
    "            self._lr *= self._decay_factor\n",
    "\n",
    "        return self._lr\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_steps: int,\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    parameters: list[Tensor],\n",
    "    scheduler: Scheduler,\n",
    "    verbose: bool = True,\n",
    ") -> None:\n",
    "    for step in range(num_steps):\n",
    "        learning_rate = scheduler.step()\n",
    "        loss = forward(X, Y, parameters)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Step {step}: loss = {loss.item()}\")\n",
    "\n",
    "        for param in parameters:\n",
    "            param.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        for param in parameters:\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    if not verbose:\n",
    "        print(f\"Final loss = {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 2.384399652481079\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(words, string_to_integer)\n",
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "scheduler = Scheduler(initial_lr=1, decay_steps=[5, 18000])\n",
    "train(\n",
    "    num_steps=20000, X=X, Y=Y, parameters=parameters, scheduler=scheduler, verbose=False\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell multiple times, we can see that it's not always better than training with a fixed LR. The performance generally fluctuates. Still, the bigram model in the last notebook achieved 2.45, and we usually get a better loss than that here. Still, we cannot say that our model is better than the bigram model based on this information, because we can only see the training loss. It might very well happen that the model severely overfits to the training set (~memorizes it verbatim), and that's why it achieves a better loss. We need to check how well the learned rules of the model *generalize*.\n",
    "\n",
    "Let's create a train/dev (validation)/test split. We should only evavaluate our model on the test set very sparingly and very few times. This is because every single time we evaluate our model on the test set we learn something from it. I.e., we start overfitting to the test set characteristics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = create_dataset(words=words[:n1], string_to_integer=string_to_integer)\n",
    "Xdev, Ydev = create_dataset(words=words[n1:n2], string_to_integer=string_to_integer)\n",
    "Xte, Yte = create_dataset(words=words[n2:], string_to_integer=string_to_integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    num_steps: int,\n",
    "    Xtr: Tensor,\n",
    "    Ytr: Tensor,\n",
    "    Xdev: Tensor,\n",
    "    Ydev: Tensor,\n",
    "    parameters: list[Tensor],\n",
    "    scheduler: Scheduler,\n",
    "    val_freq: int = 1000,\n",
    "    batch_size: int = 512,\n",
    ") -> None:\n",
    "    for step in range(num_steps):\n",
    "        learning_rate = scheduler.step()\n",
    "        loss = forward(Xtr, Ytr, parameters, batch_size=batch_size)\n",
    "\n",
    "        if step % val_freq == 0:\n",
    "            with torch.no_grad():\n",
    "                val_loss = forward(Xdev, Ydev, parameters, batch_size=len(Xdev)).item()\n",
    "            print(\n",
    "                f\"Step {step}: training loss = {loss.item()}, validation loss = {val_loss}\"\n",
    "            )\n",
    "\n",
    "        for param in parameters:\n",
    "            param.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        for param in parameters:\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    loss = forward(Xtr, Ytr, parameters, batch_size=len(Xtr))\n",
    "    with torch.no_grad():\n",
    "        val_loss = forward(Xdev, Ydev, parameters, batch_size=len(Xdev)).item()\n",
    "    print(f\"Final training loss = {loss.item()}, validation loss = {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: training loss = 19.007413864135742, validation loss = 19.446435928344727\n",
      "Step 5000: training loss = 2.45619535446167, validation loss = 2.4714863300323486\n",
      "Step 10000: training loss = 2.414703369140625, validation loss = 2.436706781387329\n",
      "Step 15000: training loss = 2.1490132808685303, validation loss = 2.4215428829193115\n",
      "Step 20000: training loss = 2.1514761447906494, validation loss = 2.357757091522217\n",
      "Step 25000: training loss = 2.375187635421753, validation loss = 2.3561882972717285\n",
      "Step 30000: training loss = 2.0933547019958496, validation loss = 2.354271650314331\n",
      "Step 35000: training loss = 2.1225461959838867, validation loss = 2.3492422103881836\n",
      "Step 40000: training loss = 2.374159812927246, validation loss = 2.3495259284973145\n",
      "Step 45000: training loss = 2.5334131717681885, validation loss = 2.3474912643432617\n",
      "Step 50000: training loss = 2.198753595352173, validation loss = 2.3458380699157715\n",
      "Step 55000: training loss = 2.5449318885803223, validation loss = 2.341794013977051\n",
      "Step 60000: training loss = 2.214048147201538, validation loss = 2.3412530422210693\n",
      "Step 65000: training loss = 2.400311231613159, validation loss = 2.3407094478607178\n",
      "Step 70000: training loss = 2.4041545391082764, validation loss = 2.3407480716705322\n",
      "Step 75000: training loss = 2.473127841949463, validation loss = 2.3406200408935547\n",
      "Step 80000: training loss = 2.3850300312042236, validation loss = 2.3403499126434326\n",
      "Step 85000: training loss = 2.2617738246917725, validation loss = 2.340451717376709\n",
      "Step 90000: training loss = 2.3326711654663086, validation loss = 2.3399617671966553\n",
      "Step 95000: training loss = 2.343001127243042, validation loss = 2.339825391769409\n",
      "Final training loss = 2.331535577774048, validation loss = 2.339454412460327\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "scheduler = Scheduler(initial_lr=1, decay_steps=[5, 18000, 50000])\n",
    "train(\n",
    "    num_steps=100000,\n",
    "    Xtr=Xtr,\n",
    "    Ytr=Ytr,\n",
    "    Xdev=Xdev,\n",
    "    Ydev=Ydev,\n",
    "    parameters=parameters,\n",
    "    scheduler=scheduler,\n",
    "    val_freq=5000,\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and dev loss are about equal, so we're not overfitting. This model is not powerful enough to purely memorize the dataset. Here we're actually underfitting, because the network is so small. We can fit the dataset better by scaling up our model, but we have to be careful to not overfit.\n",
    "\n",
    "Note: If we use the entire dataset for each update, training becomes prohibitively slow. When using minbatches, the final loss fluctuates quite a bit, even if we choose a high batch size (e.g., 4096).\n",
    "\n",
    "Let's scale up our model and see first how it behaves with a fixed learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(num_tokens: int) -> list[Tensor]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn(num_tokens, 2, generator=g, requires_grad=True)\n",
    "    W1 = torch.randn(6, 500, generator=g, requires_grad=True)\n",
    "    b1 = torch.randn(500, generator=g, requires_grad=True)\n",
    "    W2 = torch.randn(500, num_tokens, generator=g, requires_grad=True)\n",
    "    b2 = torch.randn(num_tokens, generator=g, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "C, W1, b1, W2, b2 = parameters\n",
    "\n",
    "lossi = []\n",
    "stepi = list(range(30000))\n",
    "\n",
    "for i in range(30000):\n",
    "    idx = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    emb = C[X[idx]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[idx])\n",
    "\n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1\n",
    "\n",
    "    for param in parameters:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    # Track stats\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "_ = plt.plot(stepi, lossi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's quite a bit of \"thickness\" to the plot. This is because we are optimizing over mini-batches, so the loss fluctuates quite a bit simply because of different mini-batches. As we also made the network bigger, it will generally need more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: training loss = 38.79114532470703, validation loss = 35.912689208984375\n",
      "Step 5000: training loss = 3.3986098766326904, validation loss = 3.1375415325164795\n",
      "Step 10000: training loss = 2.751959800720215, validation loss = 2.7457165718078613\n",
      "Step 15000: training loss = 2.4897124767303467, validation loss = 2.821732759475708\n",
      "Step 20000: training loss = 2.2685723304748535, validation loss = 2.426490068435669\n",
      "Step 25000: training loss = 2.2788336277008057, validation loss = 2.397268533706665\n",
      "Step 30000: training loss = 2.317457914352417, validation loss = 2.398296356201172\n",
      "Step 35000: training loss = 2.408932685852051, validation loss = 2.382577896118164\n",
      "Step 40000: training loss = 2.4099040031433105, validation loss = 2.373685836791992\n",
      "Step 45000: training loss = 2.2173573970794678, validation loss = 2.3585045337677\n",
      "Step 50000: training loss = 2.2197988033294678, validation loss = 2.360724925994873\n",
      "Step 55000: training loss = 2.1227633953094482, validation loss = 2.296186923980713\n",
      "Step 60000: training loss = 2.0162696838378906, validation loss = 2.3001868724823\n",
      "Step 65000: training loss = 2.5006065368652344, validation loss = 2.296896457672119\n",
      "Step 70000: training loss = 2.3860092163085938, validation loss = 2.2889137268066406\n",
      "Step 75000: training loss = 2.284440279006958, validation loss = 2.2843387126922607\n",
      "Step 80000: training loss = 2.2212061882019043, validation loss = 2.2726142406463623\n",
      "Step 85000: training loss = 2.4088804721832275, validation loss = 2.277012348175049\n",
      "Step 90000: training loss = 2.313023567199707, validation loss = 2.258396863937378\n",
      "Step 95000: training loss = 2.3560783863067627, validation loss = 2.3055193424224854\n",
      "Final training loss = 2.249492645263672, validation loss = 2.2709310054779053\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "scheduler = Scheduler(\n",
    "    initial_lr=1, decay_steps=[5, 600, 18000, 50000], decay_factor=0.5\n",
    ")\n",
    "train(\n",
    "    num_steps=100000,\n",
    "    Xtr=Xtr,\n",
    "    Ytr=Ytr,\n",
    "    Xdev=Xdev,\n",
    "    Ydev=Ydev,\n",
    "    parameters=parameters,\n",
    "    scheduler=scheduler,\n",
    "    val_freq=5000,\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not *so* much improvement, even after tweaking the rates a bit. It could be that the bottleneck of the model is the 2-dimensional embedding layer. Let's make that bigger too. But before we do that, let's visualize the character embeddings our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.grid(\"minor\")\n",
    "plt.scatter(C[:, 0].data, C[:, 1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(\n",
    "        C[i, 0].item(),\n",
    "        C[i, 1].item(),\n",
    "        integer_to_string[i],\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        color=\"white\",\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seemingly, the network managed to cluster the vowels closer. It believes that these tokens are quite similar. p and g have special embedding vectors that stand out. There is a little bit of structure here and this is definitely not random.\n",
    "\n",
    "By scaling up the embedding size, we lose the ability to visualize them. However, we expect the larger embeddings to capture the character's semantic meaning much better, thereby resolving the current bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: training loss = 22.98038101196289, validation loss = 22.816099166870117\n",
      "Step 5000: training loss = 3.4159483909606934, validation loss = 2.763704776763916\n",
      "Step 10000: training loss = 2.5071725845336914, validation loss = 2.4731125831604004\n",
      "Step 15000: training loss = 2.2573249340057373, validation loss = 2.423353910446167\n",
      "Step 20000: training loss = 2.6320953369140625, validation loss = 2.366960048675537\n",
      "Step 25000: training loss = 2.016991138458252, validation loss = 2.345845937728882\n",
      "Step 30000: training loss = 2.246249198913574, validation loss = 2.304871082305908\n",
      "Step 35000: training loss = 2.1622166633605957, validation loss = 2.281041383743286\n",
      "Step 40000: training loss = 1.97430419921875, validation loss = 2.2995946407318115\n",
      "Step 45000: training loss = 1.9738420248031616, validation loss = 2.2797701358795166\n",
      "Step 50000: training loss = 2.495920181274414, validation loss = 2.2713499069213867\n",
      "Step 55000: training loss = 2.077303171157837, validation loss = 2.287130355834961\n",
      "Step 60000: training loss = 2.029036521911621, validation loss = 2.299309015274048\n",
      "Step 65000: training loss = 2.0275962352752686, validation loss = 2.2738330364227295\n",
      "Step 70000: training loss = 2.1788830757141113, validation loss = 2.253347873687744\n",
      "Step 75000: training loss = 3.3454768657684326, validation loss = 2.2574117183685303\n",
      "Step 80000: training loss = 2.1975817680358887, validation loss = 2.2662954330444336\n",
      "Step 85000: training loss = 2.153982639312744, validation loss = 2.2636921405792236\n",
      "Step 90000: training loss = 2.311411142349243, validation loss = 2.269059181213379\n",
      "Step 95000: training loss = 2.192249059677124, validation loss = 2.2643778324127197\n",
      "Step 100000: training loss = 2.118537664413452, validation loss = 2.265329360961914\n",
      "Step 105000: training loss = 2.337409496307373, validation loss = 2.163517951965332\n",
      "Step 110000: training loss = 2.0838942527770996, validation loss = 2.1575217247009277\n",
      "Step 115000: training loss = 1.9859702587127686, validation loss = 2.1583614349365234\n",
      "Step 120000: training loss = 2.041696310043335, validation loss = 2.1568827629089355\n",
      "Step 125000: training loss = 2.1281771659851074, validation loss = 2.1574888229370117\n",
      "Step 130000: training loss = 2.2450475692749023, validation loss = 2.1551148891448975\n",
      "Step 135000: training loss = 1.9468258619308472, validation loss = 2.157461404800415\n",
      "Step 140000: training loss = 1.9815508127212524, validation loss = 2.155505418777466\n",
      "Step 145000: training loss = 1.833756685256958, validation loss = 2.154421091079712\n",
      "Step 150000: training loss = 2.349961280822754, validation loss = 2.154151678085327\n",
      "Step 155000: training loss = 2.213533639907837, validation loss = 2.15169095993042\n",
      "Step 160000: training loss = 2.3975913524627686, validation loss = 2.152541160583496\n",
      "Step 165000: training loss = 1.8331986665725708, validation loss = 2.1578187942504883\n",
      "Step 170000: training loss = 2.1231021881103516, validation loss = 2.1518397331237793\n",
      "Step 175000: training loss = 2.0597026348114014, validation loss = 2.1547749042510986\n",
      "Step 180000: training loss = 1.8725099563598633, validation loss = 2.152270793914795\n",
      "Step 185000: training loss = 2.5198116302490234, validation loss = 2.1533737182617188\n",
      "Step 190000: training loss = 2.062049388885498, validation loss = 2.1518361568450928\n",
      "Step 195000: training loss = 2.126692771911621, validation loss = 2.1523261070251465\n",
      "Final training loss = 2.071760654449463, validation loss = 2.1539194583892822\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters(num_tokens: int) -> list[Tensor]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn(num_tokens, 50, generator=g, requires_grad=True)\n",
    "    W1 = torch.randn(150, 150, generator=g, requires_grad=True)\n",
    "    b1 = torch.randn(150, generator=g, requires_grad=True)\n",
    "    W2 = torch.randn(150, num_tokens, generator=g, requires_grad=True)\n",
    "    b2 = torch.randn(num_tokens, generator=g, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "scheduler = Scheduler(initial_lr=0.1, decay_steps=[100000], decay_factor=0.1)\n",
    "train(\n",
    "    num_steps=200000,\n",
    "    Xtr=Xtr,\n",
    "    Ytr=Ytr,\n",
    "    Xdev=Xdev,\n",
    "    Ydev=Ydev,\n",
    "    parameters=parameters,\n",
    "    scheduler=scheduler,\n",
    "    val_freq=5000,\n",
    "    batch_size=32,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks notably better than before, so it *is* the case that the embedding size was holding us back. But now we can see that the training and validation performance start to slowly depart. The number of parameters might be large enough that we slowly start to overfit.\n",
    "\n",
    "In practice, we run lots of experiments and we choose which hyperparameter setting gives us the best final dev performance. Once we find that, we evaluate our model on the test set **once**. That's the nubmer we report in our paper.\n",
    "\n",
    "There are many ways to go from here. We can keep tuning the optimization hyperparameters, we can play with the neural network architecture (increase the number of neurons in the hidden layer, in the embedding, etc.), or we can play with the context size as well. These could all further improve the loss.\n",
    "\n",
    "Note: If we just increase both the embedding size and the hidden layer size without thinking (e.g. embedding size 50, hidden size 500), we are likely to see quite severe overfitting. In this case, actually *decreasing* the hidden size and increasing the embedding size until a certain threshold seems to work best.\n",
    "\n",
    "Increasing the length of the context would also decrease the loss further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carmahzati',\n",
       " 'hari',\n",
       " 'kim',\n",
       " 'shreet',\n",
       " 'khalessa',\n",
       " 'jazhuel',\n",
       " 'deliah',\n",
       " 'jareen',\n",
       " 'nellara',\n",
       " 'chriha']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(parameters: list[Tensor], num_samples: int) -> list[str]:\n",
    "    C, W1, b1, W2, b2 = parameters\n",
    "    g = torch.Generator().manual_seed(2147483657)\n",
    "\n",
    "    out_list = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        gen_list = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            emb = C[context]  # (block_size, d)\n",
    "            h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "            if idx == 0:\n",
    "                break\n",
    "\n",
    "            gen_list.append(idx)\n",
    "        out_list.append(\"\".join(integer_to_string[integer] for integer in gen_list))\n",
    "\n",
    "    return out_list\n",
    "\n",
    "\n",
    "sample(parameters=parameters, num_samples=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: The notebook was not careful with the initialization of the network.\n",
    "\n",
    "(a) What is the loss you woudl expect if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve?\n",
    "\n",
    "$$p(\\text{GT class} \\mid X) = \\frac{1}{27} \\implies -\\log p(\\text{GT class} \\mid X) = \\log 27 \\approx 3.2958$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.816099166870117"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters(num_tokens=len(string_to_integer))\n",
    "with torch.no_grad():\n",
    "    val_loss = forward(Xdev, Ydev, parameters, batch_size=len(Xdev)).item()\n",
    "val_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Can you tune the initialization to get a starting loss that is much more similar to the one in (a)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters2(num_tokens: int) -> list[Tensor]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn(num_tokens, 50, generator=g, requires_grad=True)\n",
    "    W1 = torch.randn(150, 150, generator=g, requires_grad=True)\n",
    "    b1 = torch.zeros(150, requires_grad=True)  # Set biases to zero\n",
    "    W2 = torch.randn(150, num_tokens, generator=g, requires_grad=True)\n",
    "    b2 = torch.zeros(num_tokens, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "def initialize_parameters3(num_tokens: int) -> list[Tensor]:\n",
    "    \"\"\"Xavier / Glorot initialization.\"\"\"\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = (\n",
    "        torch.randn(num_tokens, 50, generator=g, requires_grad=True)\n",
    "        * (2 / (num_tokens + 50)) ** 0.5\n",
    "    )\n",
    "    W1 = (\n",
    "        torch.randn(150, 150, generator=g, requires_grad=True)\n",
    "        * (2 / (150 + 150)) ** 0.5\n",
    "    )\n",
    "    b1 = torch.zeros(150, requires_grad=True)\n",
    "    W2 = (\n",
    "        torch.randn(150, num_tokens, generator=g, requires_grad=True)\n",
    "        * (2 / (150 + num_tokens)) ** 0.5\n",
    "    )\n",
    "    b2 = torch.zeros(num_tokens, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.790441513061523"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters2(num_tokens=len(string_to_integer))\n",
    "with torch.no_grad():\n",
    "    val_loss = forward(Xdev, Ydev, parameters, batch_size=len(Xdev)).item()\n",
    "val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.31752872467041"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters3(num_tokens=len(string_to_integer))\n",
    "with torch.no_grad():\n",
    "    val_loss = forward(Xdev, Ydev, parameters, batch_size=len(Xdev)).item()\n",
    "val_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly exactly a uniform random prediction's loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-nn-L00z48Da-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1777287656d261b264802db79277cf28c62daaee414facac31cc4d9617e447f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
