{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from karpathy_nn.makemore.data.load_data import load_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = load_names()\n",
    "words[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(word) for word in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(word) for word in words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are simply going to count how often any of the combinations of two characters occurs in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counter = {}\n",
    "\n",
    "for word in words:\n",
    "    # Note: word is just a string\n",
    "    # These kinds of braces are conventionally used in NLP\n",
    "    # to denote special tokens.\n",
    "    tokens = [\"<S>\"] + list(word) + [\"<E>\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        bigram = (token1, token2)\n",
    "        bigram_counter[bigram] = bigram_counter.get(bigram, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combinations sorted by how common they are\n",
    "sorted(bigram_counter.items(), reverse=True, key=lambda key_value: key_value[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store this data as a matrix, in which the rows correspond to the first character, the columns correspond to the second one, and the entry at that location gives how often the second character follows the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for a very small dataset it is not guaranteed that all letters of the\n",
    "# alphabet will occur. Therefore, this is a more flexible approach.\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "\n",
    "string_to_integer = {string: integer for integer, string in enumerate(chars)}\n",
    "string_to_integer[\"<S>\"] = len(chars)\n",
    "string_to_integer[\"<E>\"] = len(chars) + 1\n",
    "\n",
    "integer_to_string = {integer: string for string, integer in string_to_integer.items()}\n",
    "\n",
    "# We add the two special tokens.\n",
    "co_occurrence_matrix = torch.zeros((len(chars) + 2, len(chars) + 2), dtype=torch.int32)\n",
    "\n",
    "for word in words:\n",
    "    # Note: word is just a string\n",
    "    tokens = [\"<S>\"] + list(word) + [\"<E>\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "        co_occurrence_matrix[idx1, idx2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "plt.imshow(co_occurrence_matrix, cmap=\"Blues\")\n",
    "\n",
    "for i in range(co_occurrence_matrix.shape[0]):\n",
    "    for j in range(co_occurrence_matrix.shape[1]):\n",
    "        bigram_string = integer_to_string[i] + integer_to_string[j]\n",
    "\n",
    "        # ha = horizontal alignment\n",
    "        # va = vertical alignment\n",
    "        plt.text(j, i, bigram_string, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(\n",
    "            j, i, co_occurrence_matrix[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\"\n",
    "        )\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"bigram_visualization.pdf\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row is a row of zeros, because the end token will never be followed by any other token. The penultimate columns is also a column of zeros, because the start token will never follow any other token. Let's make our plot prettier. Instead of having a separate start of word and end of word token, we will use the \".\" token for both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for a very small dataset it is not guaranteed that all letters of the\n",
    "# alphabet will occur. Therefore, this is a more flexible approach.\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "\n",
    "# We put . to the first index, shift all indices by one\n",
    "string_to_integer = {string: integer + 1 for integer, string in enumerate(chars)}\n",
    "string_to_integer[\".\"] = 0\n",
    "\n",
    "integer_to_string = {integer: string for string, integer in string_to_integer.items()}\n",
    "\n",
    "# We add the two special tokens.\n",
    "co_occurrence_matrix = torch.zeros((len(chars) + 1, len(chars) + 1), dtype=torch.int32)\n",
    "num_tokens = len(co_occurrence_matrix)\n",
    "\n",
    "for word in words:\n",
    "    # Note: word is just a string\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "        co_occurrence_matrix[idx1, idx2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "plt.imshow(co_occurrence_matrix, cmap=\"Blues\")\n",
    "\n",
    "for i in range(co_occurrence_matrix.shape[0]):\n",
    "    for j in range(co_occurrence_matrix.shape[1]):\n",
    "        bigram_string = integer_to_string[i] + integer_to_string[j]\n",
    "\n",
    "        # ha = horizontal alignment\n",
    "        # va = vertical alignment\n",
    "        plt.text(j, i, bigram_string, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(\n",
    "            j, i, co_occurrence_matrix[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\"\n",
    "        )\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"bigram_visualization.pdf\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. never happens because we don't have empty words. First row corresponds to word beginnings, first column corresponds to word endings.\n",
    "\n",
    "This matrix has all the information necessary for us to sample from the bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor\n",
      "axx\n",
      "minaymoryles\n",
      "kondlaisah\n",
      "anchshizarie\n",
      "odaren\n",
      "iaddash\n",
      "h\n",
      "jhinatien\n",
      "egushl\n",
      "h\n",
      "br\n",
      "a\n",
      "jayn\n",
      "ilemannariaenien\n",
      "be\n",
      "f\n",
      "akiinela\n",
      "trttanakeroruceyaaxatona\n",
      "lamoynayrkiedengin\n"
     ]
    }
   ],
   "source": [
    "g_cpu = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "co_occurrence_matrix_float = co_occurrence_matrix.float()\n",
    "\n",
    "for _ in range(20):\n",
    "    out_list = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        unnormalized_probability_vector = co_occurrence_matrix_float[idx]\n",
    "        idx = torch.multinomial(\n",
    "            input=unnormalized_probability_vector,\n",
    "            num_samples=1,\n",
    "            replacement=True,\n",
    "            generator=g_cpu,\n",
    "        ).item()\n",
    "        # There is no need to normalize to obtain a valid categorical probability vector.\n",
    "        # If we needed it, we could simply do\n",
    "        # >>> probability_matrix = co_occurrence_matrix / co_occurrence_matrix.sum(dim=1, keepdims=True)\n",
    "        # because the float conversion happens automatically.\n",
    "        # Interesting note: by construction, the sums over rows and columns match.\n",
    "        # This is because there are exactly as many bigrams whose first element is e.g. \"b\"\n",
    "        # as there are bigrams whose second element is \"b\". For \".\" this also works because we do not\n",
    "        # distinguish between start and end tokens.\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "        out_list.append(integer_to_string[idx])\n",
    "\n",
    "    print(\"\".join(out_list))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the bigram language model is pretty terrible. We can also see that \"h\" is both a likely beginning and ending of a name (and that's exactly why we get multiple generated \"h\"s).\n",
    "\n",
    "To see that the bigram model is still a bit more plausible than sampling random characters, we can compare to the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qvsaayxbqrqmyqwuznivanukotdjvdhd\n",
      "qnoymtzduqkatdetkpfjdgigvlejfkrsqlwnirghhzwlu\n",
      "idcx\n",
      "cekmzucjnjoeovjvrggqrjr\n",
      "cfbhabkslpokc\n",
      "xtxwbpmknuusxdgzfexhwqpldpdnwzvkyxsqjforqqpfxstwkfoufhvwfhmsuyyotvcvvqpfcbydjcouhkajkhqnnpqmmllaordqy\n",
      "gszpw\n",
      "zlgijinangzzuulsyvqrufuawavsdbnwvlmrypvgrsfgpshgnmwafqmsjdvbhngvoiigxhkwdltrdkwnagzyknqv\n",
      "lfstdqigvncdoidetsukgdp\n",
      "cfpjsxeqjcsmjwguzes\n",
      "woflfjxflylgbegpjdpovdtw\n",
      "dlzysqtrbhxhcdneiuum\n",
      "xtyslfbmaboaanyjpojuujflcsaucqcgtjmlzqtbaisvxrtgupkppigxudejdzsroqeigovuxmvt\n",
      "jlxfolkozci\n",
      "tkhdivkdifaxcevlpktkwwvuxlymtwylgpzauwdvxfvbooflddphmjeomjgjcqeqwt\n",
      "\n",
      "wlxclcjbm\n",
      "quuyijtnzmycshclormjyrerqslomdrlbuwqnlmitbrmqhtbdwbyvlsmwnborwcdhjotezwnsxuvffvinrmedelubhdfgtavxqfgmnyqrygyevxaapbjtnwfnwewqxerdytttvfo\n",
      "iauarz\n",
      "tynoqkyp\n"
     ]
    }
   ],
   "source": [
    "g_cpu = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(20):\n",
    "    out_list = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        # We consider the uniform distribution\n",
    "        probability_vector = torch.ones(\n",
    "            num_tokens\n",
    "        )  # No need to normalize actually\n",
    "        idx = torch.multinomial(\n",
    "            input=probability_vector, num_samples=1, replacement=True, generator=g_cpu\n",
    "        ).item()\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "        out_list.append(integer_to_string[idx])\n",
    "\n",
    "    print(\"\".join(out_list))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much worse, right? But the bigram model is still quite terrible. Btw, the *parameters* of our bigram language model are stored in the ``co_occurrence_matrix``. The training was to compute these co-occurrences. Inference is just the loop we implemented above.\n",
    "\n",
    "Now we want to evaluate the quality of the model. We want to summarize the model's performance into a single number. How good is our model at predicting the training set, for example? This will give us the training loss.\n",
    "\n",
    "Now we actually need the normalized probabilities, so I wasn't that smart with the optimization. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_matrix = co_occurrence_matrix / co_occurrence_matrix.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0478\n",
      "em: 0.0377\n",
      "mm: 0.0253\n",
      "ma: 0.3899\n",
      "a.: 0.1960\n",
      ".o: 0.0123\n",
      "ol: 0.0780\n",
      "li: 0.1777\n",
      "iv: 0.0152\n",
      "vi: 0.3541\n",
      "ia: 0.1381\n",
      "a.: 0.1960\n",
      ".a: 0.1377\n",
      "av: 0.0246\n",
      "va: 0.2495\n",
      "a.: 0.1960\n"
     ]
    }
   ],
   "source": [
    "for word in words[:3]:\n",
    "    # Note: word is just a string\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "        # These give the conditional probabilities p(idx2 | idx1)\n",
    "        bigram_probability = probability_matrix[idx1, idx2]\n",
    "\n",
    "        print(f\"{token1}{token2}: {bigram_probability:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sampled uniformly at random, we would have $1/27$ as all of the probabilities above, which is roughly $4\\%$. We can see that the probabilities above are mostly higher than that, thus, our model is doing a better job than random guessing. If we had a very good model, we would expect these probabilities to be near 1, because then the model is correctly predicting what is next on the training set.\n",
    "\n",
    "How to summarize the probabilities into a single number? We can take the product of all the probabilities, as that is the likelihood of the entire dataset under the model. (This is because for this model, we assume independence between bigrams.) Therefore, under a good model, we expect to have a high likelihood for the dataset.\n",
    "\n",
    "Of course, working with the product of many probabilities is never a good idea considering numerical stability. It is much more stable to consider the log-likelihood of the dataset, which allows us to work with the sum of log-probabilities\n",
    "of the form $\\log p(\\texttt{idx2} \\mid \\texttt{idx1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood = tensor(-559891.7500)\n",
      "nll = tensor(559891.7500)\n",
      "nll / n = tensor(2.4541)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0\n",
    "n = 0\n",
    "for word in words:\n",
    "    # Note: word is just a string\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "        bigram_log_probability = probability_matrix[idx1, idx2].log()\n",
    "\n",
    "        log_likelihood += bigram_log_probability\n",
    "        n += 1\n",
    "\n",
    "        # print(f\"{token1}{token2}: {bigram_log_probability:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood = }\")\n",
    "\n",
    "nll = -log_likelihood  # Very nice loss function\n",
    "\n",
    "print(f\"{nll = }\")\n",
    "print(f\"{nll / n = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to maximize the likelihood of the data wrt. the model parameters (defined by the co-occurrence matrix in the bigram model case). The probabilities will not be stored explicitly later on, they will rather be calculated by a neural network whose weights we want to tweak in order to maximize the data likelihood. Maximizing the data likelihood is equivalent to minimizing a scaled version of the negative log-likelihood (average instead of sum), which is what people most often do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".a: -1.9829\n",
      "an: -1.8296\n",
      "nd: -3.2594\n",
      "dr: -2.5620\n",
      "re: -2.0127\n",
      "ej: -5.9171\n",
      "jq: -inf\n",
      "q.: -2.2736\n",
      "log_likelihood = tensor(-inf)\n",
      "nll = tensor(inf)\n",
      "nll / n = tensor(inf)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0\n",
    "n = 0\n",
    "for word in [\"andrejq\"]:\n",
    "    # Note: word is just a string\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "        bigram_log_probability = probability_matrix[idx1, idx2].log()\n",
    "\n",
    "        log_likelihood += bigram_log_probability\n",
    "        n += 1\n",
    "\n",
    "        print(f\"{token1}{token2}: {bigram_log_probability:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood = }\")\n",
    "\n",
    "nll = -log_likelihood  # Very nice loss function\n",
    "\n",
    "print(f\"{nll = }\")\n",
    "print(f\"{nll / n = }\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here jq is infinitely unlikely according to our model (because the training data did not contain that bigram), therefore we get an infinite loss. People usually don't like that, so they use *model smoothing*. This adds some fake counts to make all bigrams at least a tiny bit plausible. The usual choice is a count of one added to each combination, but we can add as many counts as we like. The more we add, the more uniform model we are going to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix_p1 = co_occurrence_matrix + 1\n",
    "probability_matrix = co_occurrence_matrix_p1 / co_occurrence_matrix_p1.sum(dim=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".a: -1.9835\n",
      "an: -1.8302\n",
      "nd: -3.2594\n",
      "dr: -2.5646\n",
      "re: -2.0143\n",
      "ej: -5.9004\n",
      "jq: -7.9817\n",
      "q.: -2.3331\n",
      "log_likelihood = tensor(-27.8672)\n",
      "nll = tensor(27.8672)\n",
      "nll / n = tensor(3.4834)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0\n",
    "n = 0\n",
    "for word in [\"andrejq\"]:\n",
    "    # Note: word is just a string\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    # zip only goes until the shorter iterator ends\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "        bigram_log_probability = probability_matrix[idx1, idx2].log()\n",
    "\n",
    "        log_likelihood += bigram_log_probability\n",
    "        n += 1\n",
    "\n",
    "        print(f\"{token1}{token2}: {bigram_log_probability:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood = }\")\n",
    "\n",
    "nll = -log_likelihood  # Very nice loss function\n",
    "\n",
    "print(f\"{nll = }\")\n",
    "print(f\"{nll / n = }\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modification, jq is still very unlikely but not totally unlikely.\n",
    "\n",
    "Now, we arrived at this model by doing something that felt sensible: counting and normalizing counts. We will now cast the problem of bigram character level language modeling into the neural network framework. We are going to approach things slightly differently but end up in a very similar spot.\n",
    "\n",
    "Our neural network will still be a bigram character level language model, so it receives a single character as input and it outputs the probability distribution over the next character in the sequence. We will optimize the model's parameters on the dataset using gradient-based optimization such that it assigns high probability to the tokens that actually follow the given token in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set of bigrams (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "# The word \"emma\" alone gives 5 examples for the NN:\n",
    "# x = ., y = e\n",
    "# x = e, y = m\n",
    "# x = m, y = m\n",
    "# x = m, y = a\n",
    "# x = a, y = .\n",
    "# These are represented as integers.\n",
    "# The input is a token category, and the output is also a token category.\n",
    "# We can stack these together into mini-batches.\n",
    "for word in words[:1]:  # Only consider first name for simplicity\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ``torch.tensor`` infers the dtype automatically, while ``torch.Tensor`` returns a ``torch.FloatTensor``. The recommendation is to stick to ``torch.tensor``, whose dtype we can explicitly modify when needed.\n",
    "\n",
    "It doesn't make sense to have a neuron act directly on an integer input. The integer value itself doesn't have any meaning: there is no ordering between the tokens. A common way of encoding categories is the one-hot encoding. Here, both the inputs and the outputs will be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize len(co_occurrence_matrix) neurons' weights.\n",
    "# Each neuron receives 27 inputs and are independent in their forward propagation.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(num_tokens, num_tokens, generator=g)\n",
    "\n",
    "# We encode the input token to the network as a one-hot vector.\n",
    "# Sadly F.one_hot doesn't support a dtype argument, so we\n",
    "# have to cast our result to float afterwards.\n",
    "xenc = F.one_hot(xs, num_classes=num_tokens).float()\n",
    "\n",
    "# xenc @ W tells us what the firing rate of these neurons is for each of the examples provided.\n",
    "# The neurons have their weights in the columns of W.\n",
    "# \"@\" is just performing the dot product between all input examples and all output neuron weight\n",
    "# vectors efficiently, in parallel (SIMD).\n",
    "logits = xenc @ W  # Predict log-counts, i.e., unnormalized log-probabilities.\n",
    "counts = logits.exp()  # Roughly equivalent to the counts (unnormalized probabilities)\n",
    "\n",
    "# The output of the NN is a probability distribution over the next character in the sequence.\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "\n",
    "# This way of turning logits into probs is called the softmax operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Bigram example 1: .e (indices 0, 5)\n",
      "Input to the neural net: 0\n",
      "Output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "Label (actual next character): 5\n",
      "Probability assigned by the net to the correct character: 0.012286253273487091\n",
      "Log-likelihood: -4.3992743492126465\n",
      "Negative log-likelihood (duh): 4.3992743492126465\n",
      "--------\n",
      "Bigram example 2: em (indices 5, 13)\n",
      "Input to the neural net: 5\n",
      "Output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "Label (actual next character): 13\n",
      "Probability assigned by the net to the correct character: 0.018050702288746834\n",
      "Log-likelihood: -4.014570713043213\n",
      "Negative log-likelihood (duh): 4.014570713043213\n",
      "--------\n",
      "Bigram example 3: mm (indices 13, 13)\n",
      "Input to the neural net: 13\n",
      "Output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "Label (actual next character): 13\n",
      "Probability assigned by the net to the correct character: 0.026691533625125885\n",
      "Log-likelihood: -3.623408794403076\n",
      "Negative log-likelihood (duh): 3.623408794403076\n",
      "--------\n",
      "Bigram example 4: ma (indices 13, 1)\n",
      "Input to the neural net: 13\n",
      "Output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "Label (actual next character): 1\n",
      "Probability assigned by the net to the correct character: 0.07367684692144394\n",
      "Log-likelihood: -2.6080667972564697\n",
      "Negative log-likelihood (duh): 2.6080667972564697\n",
      "--------\n",
      "Bigram example 5: a. (indices 1, 0)\n",
      "Input to the neural net: 1\n",
      "Output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "Label (actual next character): 0\n",
      "Probability assigned by the net to the correct character: 0.0149775305762887\n",
      "Log-likelihood: -4.201204299926758\n",
      "Negative log-likelihood (duh): 4.201204299926758\n",
      "========\n",
      "Average per-sample negative log-likelihood, i.e., loss =  3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(len(xs))\n",
    "for i in range(5):\n",
    "    # ith bigram:\n",
    "    x = xs[i].item()  # Input character index\n",
    "    y = ys[i].item()  # Label character index\n",
    "    print(\"-\" * 8)\n",
    "    print(f\"Bigram example {i + 1}: {integer_to_string[x]}{integer_to_string[y]} \"\n",
    "          f\"(indices {x}, {y})\"\n",
    "    )\n",
    "    print(\"Input to the neural net:\", x)\n",
    "    print(\"Output probabilities from the neural net:\", probs[i])\n",
    "    print(\"Label (actual next character):\", y)\n",
    "    prob = probs[i, y]\n",
    "    print(\"Probability assigned by the net to the correct character:\", prob.item())\n",
    "    log_prob = prob.log()\n",
    "    print(\"Log-likelihood:\", log_prob.item())\n",
    "    nll = -log_prob\n",
    "    print(\"Negative log-likelihood (duh):\", nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print(\"=\" * 8)\n",
    "print(\"Average per-sample negative log-likelihood, i.e., loss = \", nlls.mean().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optimize W to decrease the NLL loss value as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Create the training set\n",
    "xs, ys = [], []\n",
    "\n",
    "for word in words:\n",
    "    tokens = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    for token1, token2 in zip(tokens, tokens[1:]):\n",
    "        idx1, idx2 = string_to_integer[token1], string_to_integer[token2]\n",
    "\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num_elements = xs.nelement()\n",
    "print(\"Number of examples:\", num_elements)\n",
    "\n",
    "# Initialize the \"network\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(num_tokens, num_tokens, generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953332901001\n",
      "3.371100664138794\n",
      "3.154043197631836\n",
      "3.020373821258545\n",
      "2.927711248397827\n",
      "2.8604021072387695\n",
      "2.8097290992736816\n",
      "2.7701022624969482\n",
      "2.7380728721618652\n",
      "2.711496591567993\n",
      "2.6890032291412354\n",
      "2.6696884632110596\n",
      "2.65293025970459\n",
      "2.638277292251587\n",
      "2.6253881454467773\n",
      "2.613990545272827\n",
      "2.60386323928833\n",
      "2.5948216915130615\n",
      "2.5867116451263428\n",
      "2.579403877258301\n",
      "2.572789192199707\n",
      "2.5667760372161865\n",
      "2.5612881183624268\n",
      "2.5562589168548584\n",
      "2.551633596420288\n",
      "2.547365665435791\n",
      "2.5434155464172363\n",
      "2.5397486686706543\n",
      "2.5363364219665527\n",
      "2.5331544876098633\n",
      "2.5301806926727295\n",
      "2.5273966789245605\n",
      "2.5247862339019775\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.517857789993286\n",
      "2.515810489654541\n",
      "2.513878345489502\n",
      "2.512051820755005\n",
      "2.510324001312256\n",
      "2.5086867809295654\n",
      "2.5071349143981934\n",
      "2.5056610107421875\n",
      "2.5042612552642822\n",
      "2.5029289722442627\n",
      "2.5016608238220215\n",
      "2.5004520416259766\n",
      "2.4992988109588623\n",
      "2.498197317123413\n",
      "2.497144937515259\n",
      "2.496137857437134\n",
      "2.495173215866089\n",
      "2.4942493438720703\n",
      "2.49336314201355\n",
      "2.4925124645233154\n",
      "2.491694927215576\n",
      "2.4909095764160156\n",
      "2.4901537895202637\n",
      "2.4894261360168457\n",
      "2.488725423812866\n",
      "2.488049268722534\n",
      "2.4873974323272705\n",
      "2.4867680072784424\n",
      "2.4861602783203125\n",
      "2.4855730533599854\n",
      "2.4850046634674072\n",
      "2.484455108642578\n",
      "2.4839231967926025\n",
      "2.483407735824585\n",
      "2.4829084873199463\n",
      "2.482424259185791\n",
      "2.48195481300354\n",
      "2.481499195098877\n",
      "2.4810571670532227\n",
      "2.4806275367736816\n",
      "2.480210065841675\n",
      "2.479804515838623\n",
      "2.479410409927368\n",
      "2.4790267944335938\n",
      "2.4786534309387207\n",
      "2.478290557861328\n",
      "2.4779367446899414\n",
      "2.4775924682617188\n",
      "2.477257251739502\n",
      "2.4769303798675537\n",
      "2.476611852645874\n",
      "2.4763011932373047\n",
      "2.4759981632232666\n",
      "2.4757025241851807\n",
      "2.475414276123047\n",
      "2.475132703781128\n",
      "2.474858045578003\n",
      "2.4745898246765137\n",
      "2.474327802658081\n",
      "2.474071979522705\n",
      "2.4738218784332275\n",
      "2.4735772609710693\n",
      "2.4733383655548096\n",
      "2.47310471534729\n",
      "2.4728763103485107\n",
      "2.4726529121398926\n",
      "2.4724340438842773\n",
      "2.4722204208374023\n",
      "2.472010850906372\n",
      "2.4718058109283447\n",
      "2.4716055393218994\n",
      "2.4714086055755615\n",
      "2.471216917037964\n",
      "2.4710280895233154\n",
      "2.470843553543091\n",
      "2.4706625938415527\n",
      "2.4704854488372803\n",
      "2.4703118801116943\n",
      "2.4701411724090576\n",
      "2.4699742794036865\n",
      "2.4698104858398438\n",
      "2.4696500301361084\n",
      "2.4694924354553223\n",
      "2.4693377017974854\n",
      "2.4691860675811768\n",
      "2.4690372943878174\n",
      "2.468890905380249\n",
      "2.468747615814209\n",
      "2.468606948852539\n",
      "2.468468427658081\n",
      "2.468332529067993\n",
      "2.4681990146636963\n",
      "2.4680681228637695\n",
      "2.4679393768310547\n",
      "2.4678127765655518\n",
      "2.46768856048584\n",
      "2.4675662517547607\n",
      "2.4674463272094727\n",
      "2.4673283100128174\n",
      "2.467211961746216\n",
      "2.467097759246826\n",
      "2.4669854640960693\n",
      "2.4668750762939453\n",
      "2.466766595840454\n",
      "2.4666597843170166\n",
      "2.466554641723633\n",
      "2.4664509296417236\n",
      "2.4663491249084473\n",
      "2.4662492275238037\n",
      "2.4661505222320557\n",
      "2.466053009033203\n",
      "2.4659576416015625\n",
      "2.4658637046813965\n",
      "2.4657704830169678\n",
      "2.46567964553833\n",
      "2.4655895233154297\n",
      "2.465500593185425\n",
      "2.4654135704040527\n",
      "2.465327262878418\n",
      "2.465242385864258\n",
      "2.4651591777801514\n",
      "2.4650766849517822\n",
      "2.4649956226348877\n",
      "2.4649155139923096\n",
      "2.464836597442627\n",
      "2.46475887298584\n",
      "2.46468186378479\n",
      "2.464606285095215\n",
      "2.464531660079956\n",
      "2.4644582271575928\n",
      "2.464385747909546\n",
      "2.4643142223358154\n",
      "2.464243173599243\n",
      "2.4641737937927246\n",
      "2.4641048908233643\n",
      "2.4640369415283203\n",
      "2.4639699459075928\n",
      "2.4639039039611816\n",
      "2.4638383388519287\n",
      "2.463773727416992\n",
      "2.463710308074951\n",
      "2.4636473655700684\n",
      "2.463585376739502\n",
      "2.4635238647460938\n",
      "2.463463544845581\n",
      "2.4634037017822266\n",
      "2.4633448123931885\n",
      "2.4632863998413086\n",
      "2.463228702545166\n",
      "2.46317195892334\n",
      "2.4631154537200928\n",
      "2.463059902191162\n",
      "2.4630050659179688\n",
      "2.4629507064819336\n",
      "2.4628970623016357\n",
      "2.462843894958496\n",
      "2.462791681289673\n",
      "2.462739944458008\n",
      "2.462688684463501\n",
      "2.4626379013061523\n",
      "2.462587833404541\n",
      "2.462538003921509\n",
      "2.462489366531372\n",
      "2.4624407291412354\n",
      "2.462392807006836\n",
      "2.462345600128174\n",
      "2.4622981548309326\n",
      "2.462252140045166\n",
      "2.4622061252593994\n",
      "2.462161064147949\n",
      "2.462116003036499\n",
      "2.462071657180786\n",
      "2.4620275497436523\n",
      "2.461984157562256\n",
      "2.4619410037994385\n",
      "2.4618988037109375\n",
      "2.4618561267852783\n",
      "2.4618144035339355\n",
      "2.461773157119751\n",
      "2.4617319107055664\n",
      "2.4616920948028564\n",
      "2.461651563644409\n",
      "2.4616119861602783\n",
      "2.4615728855133057\n",
      "2.461534023284912\n",
      "2.4614953994750977\n",
      "2.4614572525024414\n",
      "2.4614193439483643\n",
      "2.4613821506500244\n",
      "2.4613449573516846\n",
      "2.461308240890503\n",
      "2.4612717628479004\n",
      "2.461236000061035\n",
      "2.46120023727417\n",
      "2.461164712905884\n",
      "2.461129903793335\n",
      "2.461095094680786\n",
      "2.4610605239868164\n",
      "2.461026906967163\n",
      "2.4609932899475098\n",
      "2.4609596729278564\n",
      "2.4609262943267822\n",
      "2.460893392562866\n",
      "2.4608612060546875\n",
      "2.4608287811279297\n",
      "2.46079683303833\n",
      "2.4607653617858887\n",
      "2.460733652114868\n",
      "2.460702896118164\n",
      "2.4606716632843018\n",
      "2.4606411457061768\n",
      "2.460610866546631\n",
      "2.460580825805664\n",
      "2.4605510234832764\n",
      "2.4605214595794678\n",
      "2.460491895675659\n",
      "2.460463047027588\n",
      "2.4604344367980957\n",
      "2.4604055881500244\n",
      "2.4603774547576904\n",
      "2.4603493213653564\n",
      "2.4603214263916016\n",
      "2.460293769836426\n",
      "2.46026611328125\n",
      "2.4602391719818115\n",
      "2.460212230682373\n",
      "2.4601855278015137\n",
      "2.4601588249206543\n",
      "2.460132598876953\n",
      "2.460106611251831\n",
      "2.460080862045288\n",
      "2.460054874420166\n",
      "2.4600296020507812\n",
      "2.4600043296813965\n",
      "2.4599790573120117\n",
      "2.459954023361206\n",
      "2.4599297046661377\n",
      "2.4599053859710693\n",
      "2.459881067276001\n",
      "2.4598567485809326\n",
      "2.4598326683044434\n",
      "2.4598090648651123\n",
      "2.4597856998443604\n",
      "2.4597620964050293\n",
      "2.4597384929656982\n",
      "2.4597158432006836\n",
      "2.45969295501709\n",
      "2.459670305252075\n",
      "2.4596478939056396\n",
      "2.459625244140625\n",
      "2.4596030712127686\n",
      "2.459581136703491\n",
      "2.459559440612793\n",
      "2.4595375061035156\n",
      "2.4595160484313965\n",
      "2.4594945907592773\n",
      "2.4594733715057373\n",
      "2.4594521522521973\n",
      "2.4594314098358154\n",
      "2.4594106674194336\n",
      "2.4593899250030518\n",
      "2.459369659423828\n",
      "2.4593491554260254\n",
      "2.459329128265381\n",
      "2.4593088626861572\n",
      "2.4592888355255127\n",
      "2.4592692852020264\n",
      "2.459249496459961\n",
      "2.459230422973633\n",
      "2.4592106342315674\n",
      "2.45919132232666\n",
      "2.459172487258911\n",
      "2.459153652191162\n",
      "2.459134817123413\n",
      "2.459115982055664\n",
      "2.459097146987915\n",
      "2.4590790271759033\n",
      "2.4590606689453125\n",
      "2.4590423107147217\n",
      "2.459024429321289\n",
      "2.4590065479278564\n",
      "2.458988666534424\n",
      "2.458970546722412\n",
      "2.458953380584717\n",
      "2.458935499191284\n",
      "2.458918333053589\n",
      "2.4589014053344727\n",
      "2.458883762359619\n",
      "2.458867073059082\n",
      "2.458850383758545\n",
      "2.4588332176208496\n",
      "2.4588162899017334\n",
      "2.4587998390197754\n",
      "2.4587838649749756\n",
      "2.4587671756744385\n",
      "2.4587509632110596\n",
      "2.4587345123291016\n",
      "2.4587182998657227\n",
      "2.458702802658081\n",
      "2.4586868286132812\n",
      "2.4586708545684814\n",
      "2.45865535736084\n",
      "2.4586398601531982\n",
      "2.4586243629455566\n",
      "2.458608627319336\n",
      "2.4585936069488525\n",
      "2.458578586578369\n",
      "2.4585635662078857\n",
      "2.4585485458374023\n",
      "2.45853328704834\n",
      "2.4585185050964355\n",
      "2.4585037231445312\n",
      "2.458489418029785\n",
      "2.458475112915039\n",
      "2.458460569381714\n",
      "2.4584460258483887\n",
      "2.4584317207336426\n",
      "2.4584174156188965\n",
      "2.4584035873413086\n",
      "2.4583895206451416\n",
      "2.4583754539489746\n",
      "2.4583616256713867\n",
      "2.458348035812378\n",
      "2.45833420753479\n",
      "2.4583206176757812\n",
      "2.4583070278167725\n",
      "2.4582936763763428\n",
      "2.458280324935913\n",
      "2.4582669734954834\n",
      "2.4582536220550537\n",
      "2.458240509033203\n",
      "2.4582273960113525\n",
      "2.458214521408081\n",
      "2.4582014083862305\n",
      "2.458189010620117\n",
      "2.4581758975982666\n",
      "2.458163261413574\n",
      "2.458150625228882\n",
      "2.4581384658813477\n",
      "2.458125591278076\n",
      "2.458113431930542\n",
      "2.4581010341644287\n",
      "2.4580888748168945\n",
      "2.4580764770507812\n",
      "2.458064317703247\n",
      "2.458052396774292\n",
      "2.458040475845337\n",
      "2.4580283164978027\n",
      "2.4580166339874268\n",
      "2.458004951477051\n",
      "2.457993268966675\n",
      "2.457981586456299\n",
      "2.457969903945923\n",
      "2.457958459854126\n",
      "2.45794677734375\n",
      "2.457935333251953\n",
      "2.4579241275787354\n",
      "2.4579129219055176\n",
      "2.4579014778137207\n",
      "2.457890510559082\n",
      "2.4578793048858643\n",
      "2.4578683376312256\n",
      "2.457857370376587\n",
      "2.4578464031219482\n",
      "2.4578356742858887\n",
      "2.45782470703125\n",
      "2.4578139781951904\n",
      "2.457803249359131\n",
      "2.4577927589416504\n",
      "2.45778226852417\n",
      "2.4577715396881104\n",
      "2.45776104927063\n",
      "2.4577505588531494\n",
      "2.457740306854248\n",
      "2.4577298164367676\n",
      "2.457719564437866\n",
      "2.457709312438965\n",
      "2.4576995372772217\n",
      "2.4576892852783203\n",
      "2.457679271697998\n",
      "2.4576690196990967\n",
      "2.4576594829559326\n",
      "2.4576497077941895\n",
      "2.457639455795288\n",
      "2.457629680633545\n",
      "2.457620143890381\n",
      "2.4576101303100586\n",
      "2.4576005935668945\n",
      "2.4575910568237305\n",
      "2.4575815200805664\n",
      "2.4575722217559814\n",
      "2.4575629234313965\n",
      "2.4575531482696533\n",
      "2.4575438499450684\n",
      "2.4575343132019043\n",
      "2.4575250148773193\n",
      "2.4575161933898926\n",
      "2.4575068950653076\n",
      "2.4574978351593018\n",
      "2.457488536834717\n",
      "2.45747971534729\n",
      "2.457470417022705\n",
      "2.4574618339538574\n",
      "2.4574530124664307\n",
      "2.457443952560425\n",
      "2.457435131072998\n",
      "2.4574263095855713\n",
      "2.4574177265167236\n",
      "2.457408905029297\n",
      "2.457400321960449\n",
      "2.4573915004730225\n",
      "2.457383155822754\n",
      "2.4573745727539062\n",
      "2.4573662281036377\n",
      "2.45735764503479\n",
      "2.4573490619659424\n",
      "2.457340955734253\n",
      "2.4573326110839844\n",
      "2.457324504852295\n",
      "2.4573161602020264\n",
      "2.457308053970337\n",
      "2.4572997093200684\n",
      "2.4572913646698\n",
      "2.4572837352752686\n",
      "2.457275390625\n",
      "2.4572675228118896\n",
      "2.457259178161621\n",
      "2.45725154876709\n",
      "2.4572436809539795\n",
      "2.457235813140869\n",
      "2.4572277069091797\n",
      "2.4572200775146484\n",
      "2.457212209701538\n",
      "2.457204818725586\n",
      "2.4571967124938965\n",
      "2.4571890830993652\n",
      "2.457181453704834\n",
      "2.4571738243103027\n",
      "2.4571664333343506\n",
      "2.4571590423583984\n",
      "2.4571516513824463\n",
      "2.457144021987915\n",
      "2.457136869430542\n",
      "2.4571292400360107\n",
      "2.4571220874786377\n",
      "2.4571146965026855\n",
      "2.4571075439453125\n",
      "2.4571001529693604\n",
      "2.457092761993408\n",
      "2.457085609436035\n",
      "2.457078456878662\n",
      "2.457071304321289\n",
      "2.457064151763916\n",
      "2.457056999206543\n",
      "2.457050323486328\n",
      "2.457043170928955\n",
      "2.457036256790161\n",
      "2.457029104232788\n",
      "2.4570224285125732\n",
      "2.4570155143737793\n",
      "2.4570086002349854\n",
      "2.4570016860961914\n",
      "2.4569950103759766\n",
      "2.4569880962371826\n",
      "2.4569811820983887\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=num_tokens).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()  # + 0.01 * (W**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    # More efficient than setting it to zero, but still has the same effect\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    learning_rate = 50  # !!!\n",
    "    W.data -= learning_rate * W.grad\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: When we optimized just by counting, our loss was originally 2.476x. Here we expect a very similar loss, as we are doing something similar, just by gradient-based optimization (the actual job of W doesn't have to be counting, only in this abstract way).\n",
    "\n",
    "Note that we are not taking any additional information compared to the first setting, we just want to predict the next word based on the previous one. Instead of counting and normalizing, we are doing this prediction based on gradient-based learning. It just so happens that the original explicit approach optimizes the loss function very well, without any need for gradient-based optimization. This is because the setup for bigram language models is so simple, so we can just estimate the joint probabilities in a table. The gradient-based approach is significantly more flexible, though. We can make the neural net much more complex, and we can also take in multiple previous tokens. But the outputs of the NN will always be logits, and the loss will also stay the same. As we complexify the neural nets and work our way up to transformers, no fundamental change is being made.\n",
    "\n",
    "For the original bigram model, it's not so straightforward how we could have extended it to multiple input tokens, because eventually the tables would get way too large (as there are too many combinations of what previous characters could be: the size of the probability tensor grows exponentially in the context size). This is an unscalable approach, whereas the NN-based bigram approach is very much scalable.\n",
    "- Take one token as the context: 26 possibilities.\n",
    "- Take two tokens as the context: 26<sup>2</sup> = 676 possibilities. The matrix would have 676 rows (or a new axis).\n",
    "- Take three tokens as the context: 26<sup>3</sup> = 19683 possibilities. The matrix would have this many rows, which clearly doesn't scale well with the context size. It also doesn't work very well.\n",
    "- With NNs, however, we can get very expressive models that predict correct conditionals based on a longer context.\n",
    "\n",
    "Note: ``xenc`` is a one-hot vector. Multiplying it with ``W`` just selects the ith row of ``W``, i.e., only the ith weight of each neuron matters for calculating the pre-activations. We then simply call softmax on this row of ``W``. But that's exactly what happened before in the first bigram formulation. The first character of the bigram was used as a lookup into the ``probability_matrix`` to get the probability distribution we need. ``W.exp() == co_occurrence_matrix``. It contains the pseudo log-counts. ``co_occurrence_matrix`` was filled in by counting, whereas the ``W`` matrix was filled in by gradient-based optimization. This is also why we obtain roughly the same loss value at the end of training.\n",
    "\n",
    "Note: At the smoothing step of the first approach we added fake counts to make the distributions more uniform. The gradient-based framework has an equivalent to smoothing: When ``W`` elements are all equal to each other, the probabilities become completely uniform. Trying to incentivize ``W`` to be constant is equivalent to label smoothing. The more we incentivize it in the loss function, the more we smooth the labels. We can pick that constant to be 0, and we can force the ``W`` matrix to be close to the 0 matrix. This is what *regularization* is for. We can augment the loss function by a small component that we call the regularization term. The $L_2$ regularization takes all elements of ``W``, squares them and sums them together. A hyperparameter $\\lambda$ can control the strength of the regularization. The regularization strength is analogous to the number of fake counts we add.\n",
    "\n",
    "Finally, we want to sample from this \"neural net\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor\n",
      "axx\n",
      "minaymoryles\n",
      "kondlaisah\n",
      "anchshizarie\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(5):\n",
    "    out_list = []\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        # Before: prob = probability_matrix[idx]\n",
    "        # Now:\n",
    "        xenc = F.one_hot(torch.tensor([idx]), num_classes=num_tokens).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        prob = counts / counts.sum(dim=1, keepdims=True)\n",
    "\n",
    "        idx = torch.multinomial(prob, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "        out_list.append(integer_to_string[idx])\n",
    "\n",
    "    print(\"\".join(out_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same result as the first approach gave! This just shows that the two approaches are very-very similar, and our hand-crafted matrix is just as good as the optimized matrix. If we train the gradient-based approach for less epochs, we can see deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-nn-L00z48Da-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1777287656d261b264802db79277cf28c62daaee414facac31cc4d9617e447f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
