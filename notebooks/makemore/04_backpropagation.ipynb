{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ``micrograd``, we stayed on the level of individual scalars. Now we will thing about backprop on the level of tensors. Back in 2012-2014, most people used backpropagation to train networks (another possibility is, e.g., contrastive divergence), but everyone wrote their backward pass manually and used gradient checkers to verify the correctness of their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from karpathy_nn.makemore.data.load_data import load_names\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = load_names()\n",
    "words[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabuilary of tokens and mappings to/from integers\n",
    "characters = sorted(list(set(\"\".join(words))))\n",
    "string_to_integer = {string: integer + 1 for integer, string in enumerate(characters)}\n",
    "string_to_integer[\".\"] = 0\n",
    "integer_to_string = {integer: string for string, integer in string_to_integer.items()}\n",
    "num_tokens = len(integer_to_string)\n",
    "print(integer_to_string)\n",
    "print(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words: list[str], block_size: int = 3) -> tuple[Tensor, Tensor]:\n",
    "    # Create the dataset\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for character in word + \".\":\n",
    "            idx = string_to_integer[character]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            context = context[1:] + [idx]  # Crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "block_size = 3\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boilerplate is done. Now we get to the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(string: str, dt: Tensor, t: Tensor) -> None:\n",
    "    exact_equality = torch.all(dt == t.grad).item()\n",
    "    approximate_equality = torch.allclose(dt, t.grad)\n",
    "    max_abs_diff = (dt - t.grad).abs().max().item()\n",
    "    print(\n",
    "        f\"{string:25s} | exact: {str(exact_equality):5s} | approximate: {str(approximate_equality):5s} | max_abs_diff: {max_abs_diff}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "dim_embedding = 10  # Dimensionality of the character embedding vectors\n",
    "dim_hidden = 64  # Number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn(num_tokens, dim_embedding, generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn(dim_embedding * block_size, dim_hidden, generator=g)\n",
    "    * (5 / 3)\n",
    "    / (dim_embedding * block_size) ** 0.5\n",
    ")\n",
    "b1 = (\n",
    "    torch.randn(dim_hidden, generator=g) * 0.1\n",
    ")  # Using b1 just for fun, it is useless because of batch norm\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn(dim_hidden, num_tokens, generator=g) * 0.1\n",
    "b2 = torch.randn(num_tokens, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bn_gain = torch.randn(1, dim_hidden, generator=g) * 0.1 + 1.0\n",
    "bn_bias = torch.randn(1, dim_hidden) * 0.1\n",
    "\n",
    "# Note: We are initializing many of these parameters in non-standard ways\n",
    "# because sometimes initializing with, e.g., all zeros could mask an\n",
    "# incorrect implementation of the backward pass.\n",
    "# (When everything is zero, everything simplifies.)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "print(\n",
    "    sum(parameter.nelement() for parameter in parameters)\n",
    ")  # Total number of parameters\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size  # A shorter variable name for conveniance\n",
    "# Construct a minibatch\n",
    "idx = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[idx], Ytr[idx]  # Batch X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5975, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass, chunked into smaller steps that are possible to\n",
    "# backward through one at a time\n",
    "# Xb: (32, block_size)\n",
    "embedding = C[Xb]  # (32, block_size, dim_embedding)\n",
    "embedding_cat = embedding.flatten(start_dim=1)  # (32, block_size * dim_embedding)\n",
    "\n",
    "# Linear layer 1\n",
    "hidden_pre_bn = embedding_cat @ W1 + b1  # (32, dim_hidden)\n",
    "\n",
    "# BatchNorm layer\n",
    "bn_mean_i = 1 / n * hidden_pre_bn.sum(dim=0, keepdim=True)  # (1, dim_hidden)\n",
    "bn_diff = hidden_pre_bn - bn_mean_i  # (32, dim_hidden)\n",
    "bn_diff2 = bn_diff**2  # (32, dim_hidden)\n",
    "\n",
    "# Note: Bessel's correction (dividing by n - 1, not n).\n",
    "# The paper doesn't use Bessel's correction during training,\n",
    "# but uses it during inference (they multiply the running variance\n",
    "# by n / (n - 1)). I.e., before storing the variance in the running\n",
    "# buffer (to be later used in eval mode), the unbiased estimator\n",
    "# is used, as they multiply by the correction term.\n",
    "# This introduces a train-test mismatch, where in training they use\n",
    "# the bias version, but in testing they use the unbiased one.\n",
    "# Andrej finds this very confusing. One should use the corrected\n",
    "# version during training as well, as the uncorrected one almost\n",
    "# always underestimates the true variance. He considers this a bug,\n",
    "# and the paper doesn't justify this discrepancy. As PyTorch doesn't\n",
    "# have a flag for that, everyone using BatchNorm has a bug in their code\n",
    "# in Andrej's opinion. This turns out to be much less of a problem\n",
    "# when the batch size is larger, but usually it's quite small.\n",
    "# The PyTorch docs mention that they use the biased estimate, but\n",
    "# it's actually not true! They do it just as described in the paper.\n",
    "bn_var = 1 / (n - 1) * bn_diff2.sum(dim=0, keepdim=True)  # (1, dim_hidden)\n",
    "bn_std_inv = (bn_var + 1e-5) ** -0.5  # (1, dim_hidden)\n",
    "bn_raw = bn_diff * bn_std_inv  # (32, dim_hidden)\n",
    "hidden_pre_activation = bn_gain * bn_raw + bn_bias  # (32, dim_hidden)\n",
    "\n",
    "# Non-linearity\n",
    "hidden_activation = torch.tanh(hidden_pre_activation)  # Hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = hidden_activation @ W2 + b2  # Output layer\n",
    "\n",
    "# Cross-entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(dim=1, keepdim=True)[0]\n",
    "norm_logits = logits - logit_maxes  # Subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)  # (32, 1)\n",
    "\n",
    "# We use this formulation because something is wrong with the backward\n",
    "# pass implementation of 1 / x in PyTorch... it gives a weird result\n",
    "counts_sum_inv = counts_sum**-1  # (32, 1)\n",
    "probs = counts * counts_sum_inv  # (32, 27) * (32, 1)\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for parameter in parameters:\n",
    "    parameter.grad = None\n",
    "\n",
    "# AFAIK, there is no cleaner way\n",
    "for variable in [\n",
    "    log_probs,\n",
    "    probs,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,\n",
    "    norm_logits,\n",
    "    logit_maxes,\n",
    "    logits,\n",
    "    hidden_activation,\n",
    "    hidden_pre_activation,\n",
    "    bn_raw,\n",
    "    bn_std_inv,\n",
    "    bn_var,\n",
    "    bn_diff2,\n",
    "    bn_diff,\n",
    "    hidden_pre_bn,\n",
    "    bn_mean_i,\n",
    "    embedding_cat,\n",
    "    embedding,\n",
    "]:\n",
    "    variable.retain_grad()\n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Backprop through the whole computational graph manually, backpropagating through all of the variables as they are defined in the forward pass above, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs                  | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "probs                     | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "counts_sum_inv            | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "counts_sum                | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "counts                    | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "norm_logits               | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "logit_maxes               | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "logits                    | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "hidden_activation         | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "W2                        | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "b2                        | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "hidden_pre_activation     | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_gain                   | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_bias                   | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_raw                    | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_std_inv                | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_var                    | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_diff2                  | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_diff                   | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "bn_mean_i                 | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "hidden_pre_bn             | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "embedding_cat             | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "W1                        | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "b1                        | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "embedding                 | exact: True  | approximate: True  | max_abs_diff: 0.0\n",
      "C                         | exact: True  | approximate: True  | max_abs_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlog_probs = torch.zeros_like(log_probs)  # (32, 27)\n",
    "dlog_probs[range(n), Yb] = -1 / n  # Others don't influence the loss\n",
    "\n",
    "# Boosts gradient for samples with very low GT label probabiliy\n",
    "dprobs = 1 / probs * dlog_probs  # (32, 27)\n",
    "\n",
    "# Backpropagating through multiplication, then replication.\n",
    "# Replication: The correct thing during backpropagation is to\n",
    "# sum the gradients of all outgoing branches to obtain the final\n",
    "# result.\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)  # (32, 1)\n",
    "\n",
    "dcounts_sum = -(counts_sum**-2) * dcounts_sum_inv  # (32, 1)\n",
    "\n",
    "# Both branches are summed up, chain rule solves broadcasting.\n",
    "# The addition just routes the gradient.\n",
    "dcounts = counts_sum_inv * dprobs + dcounts_sum  # (32, 27)\n",
    "\n",
    "dnorm_logits = counts * dcounts  # (32, 27)\n",
    "\n",
    "# Note: this is zero, because we have seen that an additive constant\n",
    "# does not change the softmax value and, consequently, the loss.\n",
    "# It won't be exactly zero because of numerical wonkiness, though.\n",
    "dlogit_maxes = -dnorm_logits.sum(dim=1, keepdim=True)  # (32, 1)\n",
    "\n",
    "dlogits = (\n",
    "    dnorm_logits\n",
    "    + F.one_hot(logits.max(dim=1)[1], num_classes=logits.shape[1]) * dlogit_maxes\n",
    ")  # (32, 27)\n",
    "# dlogits = dnorm_logits\n",
    "# argmaxes = logits.argmax(dim=1, keepdim=True)\n",
    "# dlogits[range(n), argmaxes] += dlogit_maxes  # (32, 27)\n",
    "\n",
    "dhidden_activation = dlogits @ W2.T  # (32, 27) @ (27, dim_hidden) = (32, dim_hidden)\n",
    "\n",
    "dW2 = hidden_activation.T @ dlogits  # (dim_hidden, 32) @ (32, 27) = (dim_hidden, 27)\n",
    "\n",
    "db2 = dlogits.sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "dhidden_pre_activation = (\n",
    "    1 - hidden_activation**2\n",
    ") * dhidden_activation  # (32, dim_hidden)\n",
    "\n",
    "dbn_gain = (bn_raw * dhidden_pre_activation).sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "dbn_bias = dhidden_pre_activation.sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "dbn_raw = bn_gain * dhidden_pre_activation  # (32, dim_hidden)\n",
    "\n",
    "dbn_std_inv = (bn_diff * dbn_raw).sum(dim=0, keepdim=True)  # (1, dim_hidden)\n",
    "\n",
    "dbn_var = -0.5 * (bn_var + 1e-5) ** (-3 / 2) * dbn_std_inv  # (1, dim_hidden)\n",
    "\n",
    "dbn_diff2 = 1 / (n - 1) * dbn_var.tile((32, 1))  # (32, dim_hidden)\n",
    "\n",
    "# Sum different paths\n",
    "dbn_diff = 2 * bn_diff * dbn_diff2 + bn_std_inv * dbn_raw  # (32, dim_hidden)\n",
    "\n",
    "dbn_mean_i = -dbn_diff.sum(dim=0, keepdim=True)  # (1, dim_hidden)\n",
    "\n",
    "dhidden_pre_bn = dbn_diff + 1 / n * dbn_mean_i  # (32, dim_hidden)\n",
    "\n",
    "dembedding_cat = dhidden_pre_bn @ W1.T\n",
    "# (32, dim_hidden) @ (dim_hidden, dim_embedding * block_size)\n",
    "# = (32, dim_embedding * block_size)\n",
    "\n",
    "dW1 = embedding_cat.T @ dhidden_pre_bn\n",
    "# (dim_embedding * block_size, 32) @ (32, dim_hidden)\n",
    "# = dim_embedding * block_size, dim_hidden)\n",
    "\n",
    "db1 = dhidden_pre_bn.sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "dembedding = dembedding_cat.reshape(embedding.shape)  # (32, block_size, dim_embedding)\n",
    "\n",
    "dC = torch.zeros_like(C)  # (num_tokens, dim_embedding)\n",
    "for k in range(Xb.shape[0]):  # batch index\n",
    "    for l in range(Xb.shape[1]):  # context index\n",
    "        idx = Xb[k, l]\n",
    "        dC[idx] += dembedding[k, l]  # Accumulate gradients of tokens\n",
    "\n",
    "cmp(\"logprobs\", dlog_probs, log_probs)\n",
    "cmp(\"probs\", dprobs, probs)\n",
    "cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n",
    "cmp(\"counts_sum\", dcounts_sum, counts_sum)\n",
    "cmp(\"counts\", dcounts, counts)\n",
    "cmp(\"norm_logits\", dnorm_logits, norm_logits)\n",
    "cmp(\"logit_maxes\", dlogit_maxes, logit_maxes)\n",
    "cmp(\"logits\", dlogits, logits)\n",
    "cmp(\"hidden_activation\", dhidden_activation, hidden_activation)\n",
    "cmp(\"W2\", dW2, W2)\n",
    "cmp(\"b2\", db2, b2)\n",
    "cmp(\"hidden_pre_activation\", dhidden_pre_activation, hidden_pre_activation)\n",
    "cmp(\"bn_gain\", dbn_gain, bn_gain)\n",
    "cmp(\"bn_bias\", dbn_bias, bn_bias)\n",
    "cmp(\"bn_raw\", dbn_raw, bn_raw)\n",
    "cmp(\"bn_std_inv\", dbn_std_inv, bn_std_inv)\n",
    "cmp(\"bn_var\", dbn_var, bn_var)\n",
    "cmp(\"bn_diff2\", dbn_diff2, bn_diff2)\n",
    "cmp(\"bn_diff\", dbn_diff, bn_diff)\n",
    "cmp(\"bn_mean_i\", dbn_mean_i, bn_mean_i)\n",
    "cmp(\"hidden_pre_bn\", dhidden_pre_bn, hidden_pre_bn)\n",
    "cmp(\"embedding_cat\", dembedding_cat, embedding_cat)\n",
    "cmp(\"W1\", dW1, W1)\n",
    "cmp(\"b1\", db1, b1)\n",
    "cmp(\"embedding\", dembedding, embedding)\n",
    "cmp(\"C\", dC, C)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For $D = AB$,\n",
    "$$\\frac{\\partial L}{\\partial B} = A^\\top \\frac{\\partial L}{\\partial D}$$\n",
    "and\n",
    "$$\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial D} B^\\top.$$\n",
    "(Andrej differentiates through matmul without using this, he never remembers it. He only leverages that shapes have to match up, just like I do. This, of course, fails when the matrices are square.)\n",
    "\n",
    "It turns out that in the first exercise, we were doing way too much work. It is not what we would do in practice. E.g., when calculating the cross-entropy loss, we broke the calculation up into its most atomic parts and backpropagated through all of those individually. However, if we just look at the mathematical expression of the loss, we can do the differentiation on a paper, and many terms cancel and simplify. The mathematical expression we end up with is significantly shorter and easier to implement than backpropagating through all the little pieces.\n",
    "\n",
    "Exercise 2: backropagate through cross_entropy but all in one go. To complete this challenge, look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5974924564361572 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), \"diff:\", (loss_fast - loss).item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the \"exact\" same loss, but it's calculated much faster and takes less time to write. It's also much faster to backprop through this: if we differentiate by hand, we end up with a much shorter expression that runs very fast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= -\\frac{1}{n}\\sum_{i = 1}^n \\log p(y_i \\mid x_i)\\\\\n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\log \\frac{\\exp\\left(\\text{logit}_{i, y_i}\\right)}{\\sum_{j = 1}^C \\exp\\left(\\text{logit}_{i, j}\\right)}\\\\\n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\left(\\text{logit}_{i, y_i} - \\log\\sum_{j = 1}^C \\exp \\left(\\text{logit}_{i, j}\\right)\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{logit}_{k, l}} &= -\\frac{1}{n} \\sum_{i = 1}^n \\left(\\delta_{ik}\\delta_{y_il} - \\frac{\\sum_{j = 1}^C \\exp\\left(\\text{logit}_{i, j}\\right) \\delta_{ik}\\delta_{jl}}{\\sum_{j = 1}^C \\exp\\left(\\text{logit}_{i, j}\\right)}\\right)\\\\\n",
    "&= -\\frac{1}{n}\\sum_{i = 1}^n \\left(\\delta_{ik}\\delta_{y_il}-\\frac{\\exp\\left(\\text{logit}_{i, l}\\right)\\delta_{ik}}{\\sum_{j = 1}^C \\exp\\left(\\text{logit}_{i, j}\\right)}\\right)\\\\\n",
    "&= -\\frac{1}{n} \\left(\\delta_{y_kl} - \\frac{\\exp\\left(\\text{logit}_{k, l}\\right)}{\\sum_{j = 1}^C \\exp\\left(\\text{logit}_{k, j}\\right)}\\right)\\\\\n",
    "&= \\frac{1}{n}\\left(\\text{softmax}_{k, l} - \\text{onehot}_{k, l}\\right).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits                    | exact: False | approximate: True  | max_abs_diff: 6.05359673500061e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "dlogits = 1 / n * (probs - F.one_hot(Yb, num_classes=logits.shape[1]))\n",
    "\n",
    "# dlogits = F.softmax(logits, dim=1)\n",
    "# dlogits[range(n), Yb] -= 1\n",
    "# dlogits /= n\n",
    "\n",
    "cmp(\"logits\", dlogits, logits)  # Andrej gets the same result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some floating point wonkiness, but we basically get the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13419b370>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0182, 0.0116, 0.0343, 0.0327, 0.0500, 0.0450, 0.0310, 0.0246, 0.0716,\n",
       "        0.0350, 0.0222, 0.0254, 0.0380, 0.0562, 0.0424, 0.0464, 0.0213, 0.0406,\n",
       "        0.0439, 0.0571, 0.0661, 0.0300, 0.0173, 0.0346, 0.0396, 0.0377, 0.0272],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0182,  0.0116,  0.0343,  0.0327,  0.0500,  0.0450,  0.0310,  0.0246,\n",
       "         0.0716,  0.0350,  0.0222,  0.0254,  0.0380,  0.0562, -0.9576,  0.0464,\n",
       "         0.0213,  0.0406,  0.0439,  0.0571,  0.0661,  0.0300,  0.0173,  0.0346,\n",
       "         0.0396,  0.0377,  0.0272], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.0536e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``dlogits`` rows sum to zero. Each gradient value in each row is like a force: we are pulling down on the probabilities of the incorrect characters, and we are pulling up the probability of the GT character. (Increasing the GT prob a tiny bit will decrease the loss quite a lot. Increasing any other probability increases the loss.) The amount of push and pull is equalized because the sum is zero. If our predicted probabilities were perfect, we would have 0 gradient, as it should be. The lower our GT prediction is, the more force we apply at the GT label location to increase that probability. The higher the probability is at incorrect indices, the more force we apply to pull it down.\n",
    "\n",
    "Exercise 3: Backpropagate through batch norm but all in one go. To complete this challenge, look at the mathematical expression of the output of batch norm, take the derivative wrt. its input, simplify the expression, and just write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass:\n",
    "hidden_pre_activation_fast = (\n",
    "    bn_gain\n",
    "    * (hidden_pre_bn - hidden_pre_bn.mean(dim=0, keepdim=True))\n",
    "    / torch.sqrt(hidden_pre_bn.var(dim=0, keepdim=True, unbiased=True) + 1e-5)\n",
    "    + bn_bias\n",
    ")\n",
    "print(\"Max diff:\", (hidden_pre_activation - hidden_pre_activation_fast).abs().max())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate ``dhidden_pre_bn`` given ``dhidden_pre_activation``, i.e., backpropagate through the batchnorm. You will also need to use some of the variables from the forward pass up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_pre_bn.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i$\n",
    "\n",
    "2. $\\sigma^2 = \\frac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\mu)^2$\n",
    "\n",
    "3. $\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
    "\n",
    "4. $y_i = \\gamma \\hat{x}_i + \\beta$\n",
    "\n",
    "We know that\n",
    "$$\\frac{dL}{dx_i} =  \\sum_{j = 1}^n \\frac{dL}{dy_j} \\cdot \\frac{dy_j}{dx_i}.$$\n",
    "Here we assume that we already know the value of $\\frac{dL}{dy_j}$ from backprop, so we only have to calculate $\\frac{dy_j}{dx_i}$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dy_j}{dx_i} &= \\frac{\\partial y_j}{\\partial \\hat{x}_j} \\cdot \\frac{\\partial \\hat{x}_j}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial y_j}{\\partial \\hat{x}_j} \\cdot \\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} + \\frac{\\partial y_i}{\\partial \\hat{x}_j} \\cdot \\frac{\\partial \\hat{x}_j}{\\partial x_i}\\\\\n",
    "&= \\gamma \\left(\\frac{\\partial \\hat{x}_j}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} + \\frac{\\partial \\hat{x}_j}{\\partial x_i}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "Let's calculate the terms in the paranthesis.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\hat{x}_j}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} &= -\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\frac{1}{n}\\\\\n",
    "\\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} &= -\\frac{1}{2}(x_j - \\mu)(\\sigma^2 + \\epsilon)^{-3/2} \\cdot \\frac{2}{n - 1}\\sum_{j = 1}^n (x_j - \\mu) \\cdot \\delta_{ij}\\\\\n",
    "&= -\\frac{1}{2}(x_j - \\mu)(\\sigma^2 + \\epsilon)^{-3/2} \\cdot \\frac{2}{n - 1}(x_i - \\mu)\\\\\n",
    "\\frac{\\partial \\hat{x}_j}{\\partial x_i} &= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\cdot \\delta_{ij}\n",
    "\\end{align*}\n",
    "$$\n",
    "Substituting in, we get\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dy_j}{dx_i} &= \\gamma\\left(-\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\cdot \\frac{1}{n} - \\frac{1}{2} (x_j - \\mu)(\\sigma^2 + \\epsilon)^{-3/2} \\cdot \\frac{2}{n - 1}(x_i - \\mu) + \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\delta_{ij}\\right)\\\\\n",
    "&= \\gamma\\left(-\\frac{1}{n}\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} - \\frac{1}{n - 1}\\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\delta_{ij}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "Finally, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dL}{dx_i} &= \\sum_{j = 1}^n \\frac{dL}{dy_j} \\cdot \\gamma\\left(-\\frac{1}{n}\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} - \\frac{1}{n - 1}\\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\delta_{ij}\\right)\\\\\n",
    "&= -\\frac{1}{n} \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\left(\\sum_{j = 1}^n \\frac{dL}{dy_j} + \\frac{n}{n - 1} \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\sum_{j = 1}^n \\frac{dL}{dy_j} \\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} - n\\frac{dL}{dy_i}\\right)\\\\\n",
    "&= \\frac{\\gamma (\\sigma^2 + \\epsilon)^{-1/2}}{n} \\left(n\\frac{dL}{dy_i} - \\sum_{j = 1}^n \\frac{dL}{dy_j} - \\frac{n}{n - 1} \\hat{x}_i \\sum_{j = 1}^n \\frac{dL}{dy_j}\\hat{x}_j\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_pre_bn             | exact: False | approximate: True  | max_abs_diff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dhidden_pre_bn = (\n",
    "    bn_gain  # (1, dim_hidden)\n",
    "    * bn_std_inv  # (1, dim_hidden)\n",
    "    / n  # ()\n",
    "    * (\n",
    "        n * dhidden_pre_activation  # (32, dim_hidden)\n",
    "        - dhidden_pre_activation.sum(dim=0)  # (dim_hidden,)\n",
    "        - n\n",
    "        / (n - 1)\n",
    "        * bn_raw\n",
    "        * (dhidden_pre_activation * bn_raw).sum(dim=0)  # (32, dim_hidden)\n",
    "    )\n",
    ")  # (32, dim_hidden)\n",
    "\n",
    "cmp(\"hidden_pre_bn\", dhidden_pre_bn, hidden_pre_bn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4: putting it all together. Train the MLP neural net with your own backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7895\n",
      "  10000/ 200000: 2.1728\n",
      "  20000/ 200000: 2.4086\n",
      "  30000/ 200000: 2.4734\n",
      "  40000/ 200000: 1.9639\n",
      "  50000/ 200000: 2.3418\n",
      "  60000/ 200000: 2.4618\n",
      "  70000/ 200000: 2.0952\n",
      "  80000/ 200000: 2.3559\n",
      "  90000/ 200000: 2.1882\n",
      " 100000/ 200000: 1.8809\n",
      " 110000/ 200000: 2.3082\n",
      " 120000/ 200000: 1.9443\n",
      " 130000/ 200000: 2.4713\n",
      " 140000/ 200000: 2.2350\n",
      " 150000/ 200000: 2.1873\n",
      " 160000/ 200000: 1.9379\n",
      " 170000/ 200000: 1.8704\n",
      " 180000/ 200000: 2.0474\n",
      " 190000/ 200000: 1.9184\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "dim_embedding = 10  # Dimensionality of the character embedding vectors\n",
    "dim_hidden = 200  # Number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn(num_tokens, dim_embedding, generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn(dim_embedding * block_size, dim_hidden, generator=g)\n",
    "    * (5 / 3)\n",
    "    / (dim_embedding * block_size) ** 0.5\n",
    ")\n",
    "b1 = torch.randn(dim_hidden, generator=g) * 0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn(dim_hidden, num_tokens, generator=g) * 0.1\n",
    "b2 = torch.randn(num_tokens, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bn_gain = torch.randn(1, dim_hidden) * 0.1 + 1\n",
    "bn_bias = torch.randn(1, dim_hidden) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "\n",
    "print(\n",
    "    sum(parameter.nelement() for parameter in parameters)\n",
    ")  # Total number of parameters\n",
    "\n",
    "# Same optimization as in the last notebook\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size  # For convenience\n",
    "loss_i = []\n",
    "\n",
    "# Optimization\n",
    "for i in range(max_steps):\n",
    "    # Construct mini-batch\n",
    "    idx = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[idx], Ytr[idx]  # Batch X, Y\n",
    "\n",
    "    # Forward pass\n",
    "    embedding = C[Xb]  # Embed the characters into vectors\n",
    "    embedding_cat = embedding.flatten(start_dim=1)  # Concatenate the contex\n",
    "\n",
    "    # Linear layer\n",
    "    hidden_pre_bn = embedding_cat @ W1 + b1\n",
    "\n",
    "    # BatchNorm layer\n",
    "    bn_mean = hidden_pre_bn.mean(dim=0, keepdim=True)\n",
    "    bn_var = hidden_pre_bn.var(dim=0, keepdim=True, unbiased=True)\n",
    "    bn_std_inv = (bn_var + 1e-5) ** -0.5\n",
    "    bn_raw = (hidden_pre_bn - bn_mean) * bn_std_inv\n",
    "    hidden_pre_activation = bn_gain * bn_raw + bn_bias\n",
    "\n",
    "    # Non-linearity\n",
    "    hidden_activation = torch.tanh(hidden_pre_activation)\n",
    "    logits = hidden_activation @ W2 + b2  # Output layer\n",
    "    loss = F.cross_entropy(logits, Yb)  # Loss function\n",
    "\n",
    "    # Backward pass -- manual backprop!  # swole_doge_meme\n",
    "    dlogits = 1 / n * (F.softmax(logits, dim=1) - F.one_hot(Yb, num_classes=logits.shape[1]))\n",
    "\n",
    "    dhidden_activation = (\n",
    "        dlogits @ W2.T\n",
    "    )  # (32, 27) @ (27, dim_hidden) = (32, dim_hidden)\n",
    "\n",
    "    dW2 = (\n",
    "        hidden_activation.T @ dlogits\n",
    "    )  # (dim_hidden, 32) @ (32, 27) = (dim_hidden, 27)\n",
    "\n",
    "    db2 = dlogits.sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "    dhidden_pre_activation = (\n",
    "        1 - hidden_activation**2\n",
    "    ) * dhidden_activation  # (32, dim_hidden)\n",
    "\n",
    "    dhidden_pre_bn = (\n",
    "        bn_gain  # (1, dim_hidden)\n",
    "        * bn_std_inv  # (1, dim_hidden)\n",
    "        / n  # ()\n",
    "        * (\n",
    "            n * dhidden_pre_activation  # (32, dim_hidden)\n",
    "            - dhidden_pre_activation.sum(dim=0)  # (dim_hidden,)\n",
    "            - n\n",
    "            / (n - 1)\n",
    "            * bn_raw\n",
    "            * (dhidden_pre_activation * bn_raw).sum(dim=0)  # (32, dim_hidden)\n",
    "        )\n",
    "    )  # (32, dim_hidden)\n",
    "\n",
    "    dbn_gain = (bn_raw * dhidden_pre_activation).sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "    dbn_bias = dhidden_pre_activation.sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "    dembedding_cat = dhidden_pre_bn @ W1.T\n",
    "    # (32, dim_hidden) @ (dim_hidden, dim_embedding * block_size)\n",
    "    # = (32, dim_embedding * block_size)\n",
    "\n",
    "    dW1 = embedding_cat.T @ dhidden_pre_bn\n",
    "    # (dim_embedding * block_size, 32) @ (32, dim_hidden)\n",
    "    # = dim_embedding * block_size, dim_hidden)\n",
    "\n",
    "    db1 = dhidden_pre_bn.sum(dim=0)  # (dim_hidden,)\n",
    "\n",
    "    dembedding = dembedding_cat.reshape(\n",
    "        embedding.shape\n",
    "    )  # (32, block_size, dim_embedding)\n",
    "\n",
    "    dC = torch.zeros_like(C)  # (num_tokens, dim_embedding)\n",
    "    for k in range(Xb.shape[0]):  # batch index\n",
    "        for l in range(Xb.shape[1]):  # context index\n",
    "            idx = Xb[k, l]\n",
    "            dC[idx] += dembedding[k, l]  # Accumulate gradients of tokens\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbn_gain, dbn_bias]\n",
    "\n",
    "    # Update\n",
    "    learning_rate = 0.1 if i < 100000 else 0.01  # Step learning rate decay\n",
    "    for parameter, grad in zip(parameters, grads):\n",
    "        parameter.data -= learning_rate * grad  # New way of swole doge\n",
    "    \n",
    "    # Track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    loss_i.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0714423656463623\n",
      "val 2.1101531982421875\n"
     ]
    }
   ],
   "source": [
    "# Calibrate the batch norm at the end of training\n",
    "\n",
    "# Pass the training set through\n",
    "embedding = C[Xtr]\n",
    "embedding_cat = embedding.flatten(start_dim=1)\n",
    "hidden_pre_activation = embedding_cat @ W1 + b1\n",
    "# Measure the mean / std over the entire training set\n",
    "bn_mean = hidden_pre_activation.mean(dim=0, keepdim=True)\n",
    "bn_var = hidden_pre_activation.var(dim=0, keepdim=True, unbiased=True)\n",
    "\n",
    "# Evaluate train and val loss\n",
    "def split_loss(split: str) -> None:\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "    embedding = C[x]  # (N, block_size, dim_embedding)\n",
    "    embedding_cat = embedding.flatten(start_dim=1)  # (N, block_size * dim_embedding)\n",
    "    hidden_pre_activation = embedding_cat @ W1 + b1\n",
    "    hidden_pre_activation = bn_gain * (hidden_pre_activation - bn_mean) * (bn_var + 1e-5)**-0.5 + bn_bias\n",
    "    hidden_activation = torch.tanh(hidden_pre_activation)  # (N, dim_hidden)\n",
    "    logits = hidden_activation @ W2 + b2  # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the same result we obtained previously with ``.backward()``, but now we don't track any gradients automatically. We wrote backpropagation manually, and it works well! Let's sample from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah\n",
      "amille\n",
      "khyimrix\n",
      "taty\n",
      "skaessa\n",
      "jazonte\n",
      "delynn\n",
      "jareeigh\n",
      "kasia\n",
      "chaily\n",
      "kaleigh\n",
      "ham\n",
      "joce\n",
      "quint\n",
      "shoine\n",
      "liven\n",
      "corterra\n",
      "jarynix\n",
      "kaellinsley\n",
      "dae\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483657)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # Initialize with all ...\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        embedding = C[context]  # (block_size, dim_embedding)\n",
    "        embedding_cat = embedding.flatten()  # (block_size * dim_embedding)\n",
    "        hidden_pre_activation = embedding_cat @ W1 + b1\n",
    "        hidden_pre_activation = bn_gain * (hidden_pre_activation - bn_mean) * (bn_var + 1e-5)**-0.5 + bn_bias\n",
    "        hidden_activation = torch.tanh(hidden_pre_activation)  # (dim_hidden,)\n",
    "        logits = hidden_activation @ W2 + b2  # (vocab_size,)\n",
    "\n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [idx]\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "        out.append(idx)\n",
    "    \n",
    "    print(\"\".join(integer_to_string[i] for i in out))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a wrap!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-nn-L00z48Da-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1777287656d261b264802db79277cf28c62daaee414facac31cc4d9617e447f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
